C:\JYN\sd\anaconda\anaconda\envs\paddle-py37\python.exe C:\Users\woait\AppData\Roaming\JetBrains\IntelliJIdea2021.3\plugins\python\helpers\pydev\pydevd.py --cmd-line --multiproc --qt-support=auto --client 127.0.0.1 --port 56556 --file C:/JYN/workspace/DeBERTa_Decoding-enhanced_BERT_with_Disentangled_Attention/tools/test_torch.py
Connected to pydev debugger (build 213.6461.79)
C:\JYN\sd\anaconda\anaconda\envs\paddle-py37\lib\site-packages\paddle\signal.py:307: DeprecationWarning: invalid escape sequence \o
  """
C:\JYN\sd\anaconda\anaconda\envs\paddle-py37\lib\site-packages\paddle\signal.py:472: DeprecationWarning: invalid escape sequence \s
  """
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
01212022 10:28:59|WARNING|datasets.load| Using the latest cached version of the module from C:\Users\woait\.cache\huggingface\modules\datasets_modules\datasets\glue\57a9abc0d18177d2515b44219891e3f51111b2b9662ffe709733c9eefc6705e8 (last modified on Tue Jan  4 16:21:53 2022) since it couldn't be found locally at glue., or remotely on the Hugging Face Hub.
01212022 10:28:59|WARNING|datasets.builder| Reusing dataset glue (C:\Users\woait\.cache\huggingface\datasets\glue\mnli\1.0.0\57a9abc0d18177d2515b44219891e3f51111b2b9662ffe709733c9eefc6705e8)
100%|██████████| 5/5 [00:00<00:00, 29.06it/s]
transform data to ids: 100%|██████████| 392702/392702 [03:27<00:00, 1888.03it/s]
transform data to ids: 100%|██████████| 9815/9815 [00:05<00:00, 1909.25it/s]
epoch 1 training:   0%|          | 99/196351 [00:56<28:29:12,  1.91it/s]global step 100, epoch: 1, batch: 100, loss: 6.12626, acc: 0.27500, time: Fri Jan 21 10:35:35 2022
epoch 1 training:   0%|          | 199/196351 [01:47<27:52:57,  1.95it/s]global step 200, epoch: 1, batch: 200, loss: 6.00401, acc: 0.32750, time: Fri Jan 21 10:36:26 2022
epoch 1 training:   0%|          | 299/196351 [02:39<28:02:42,  1.94it/s]global step 300, epoch: 1, batch: 300, loss: 6.08968, acc: 0.32000, time: Fri Jan 21 10:37:17 2022
epoch 1 training:   0%|          | 399/196351 [03:30<27:36:01,  1.97it/s]global step 400, epoch: 1, batch: 400, loss: 6.09966, acc: 0.32000, time: Fri Jan 21 10:38:08 2022
epoch 1 training:   0%|          | 499/196351 [04:21<28:15:27,  1.93it/s]global step 500, epoch: 1, batch: 500, loss: 6.08636, acc: 0.31400, time: Fri Jan 21 10:39:00 2022
epoch 1 training:   0%|          | 599/196351 [05:12<27:47:58,  1.96it/s]global step 600, epoch: 1, batch: 600, loss: 6.05051, acc: 0.32750, time: Fri Jan 21 10:39:51 2022
epoch 1 training:   0%|          | 699/196351 [06:03<27:27:07,  1.98it/s]global step 700, epoch: 1, batch: 700, loss: 6.09118, acc: 0.32857, time: Fri Jan 21 10:40:42 2022
epoch 1 training:   0%|          | 799/196351 [06:54<27:39:59,  1.96it/s]global step 800, epoch: 1, batch: 800, loss: 6.16696, acc: 0.33000, time: Fri Jan 21 10:41:33 2022
epoch 1 training:   0%|          | 899/196351 [07:45<27:15:10,  1.99it/s]global step 900, epoch: 1, batch: 900, loss: 6.08423, acc: 0.33444, time: Fri Jan 21 10:42:24 2022
epoch 1 training:   1%|          | 999/196351 [08:36<27:35:40,  1.97it/s]global step 1000, epoch: 1, batch: 1000, loss: 5.99692, acc: 0.33350, time: Fri Jan 21 10:43:15 2022
epoch 1 training:   1%|          | 1099/196351 [09:28<27:50:47,  1.95it/s]global step 1100, epoch: 1, batch: 1100, loss: 6.04506, acc: 0.33000, time: Fri Jan 21 10:44:06 2022
epoch 1 training:   1%|          | 1199/196351 [10:19<28:02:28,  1.93it/s]global step 1200, epoch: 1, batch: 1200, loss: 6.05646, acc: 0.33125, time: Fri Jan 21 10:44:57 2022
epoch 1 training:   1%|          | 1299/196351 [11:10<27:25:59,  1.98it/s]global step 1300, epoch: 1, batch: 1300, loss: 6.07094, acc: 0.33154, time: Fri Jan 21 10:45:48 2022
epoch 1 training:   1%|          | 1399/196351 [12:01<27:34:05,  1.96it/s]global step 1400, epoch: 1, batch: 1400, loss: 6.05629, acc: 0.33536, time: Fri Jan 21 10:46:39 2022
epoch 1 training:   1%|          | 1499/196351 [12:52<28:06:07,  1.93it/s]global step 1500, epoch: 1, batch: 1500, loss: 6.06574, acc: 0.33333, time: Fri Jan 21 10:47:31 2022
epoch 1 training:   1%|          | 1599/196351 [13:44<27:44:10,  1.95it/s]global step 1600, epoch: 1, batch: 1600, loss: 6.04337, acc: 0.33594, time: Fri Jan 21 10:48:23 2022
epoch 1 training:   1%|          | 1699/196351 [14:36<27:58:43,  1.93it/s]global step 1700, epoch: 1, batch: 1700, loss: 6.02702, acc: 0.33618, time: Fri Jan 21 10:49:15 2022
epoch 1 training:   1%|          | 1799/196351 [15:27<27:16:10,  1.98it/s]global step 1800, epoch: 1, batch: 1800, loss: 6.01421, acc: 0.33667, time: Fri Jan 21 10:50:06 2022
epoch 1 training:   1%|          | 1899/196351 [16:19<27:35:25,  1.96it/s]global step 1900, epoch: 1, batch: 1900, loss: 6.08029, acc: 0.33816, time: Fri Jan 21 10:50:58 2022
epoch 1 training:   1%|          | 1999/196351 [17:11<27:49:36,  1.94it/s]global step 2000, epoch: 1, batch: 2000, loss: 6.06962, acc: 0.33850, time: Fri Jan 21 10:51:50 2022
epoch 1 training:   1%|          | 2099/196351 [18:03<27:50:04,  1.94it/s]global step 2100, epoch: 1, batch: 2100, loss: 6.09628, acc: 0.33857, time: Fri Jan 21 10:52:42 2022
epoch 1 training:   1%|          | 2199/196351 [18:54<27:31:46,  1.96it/s]global step 2200, epoch: 1, batch: 2200, loss: 6.07541, acc: 0.33795, time: Fri Jan 21 10:53:33 2022
epoch 1 training:   1%|          | 2299/196351 [19:46<28:10:41,  1.91it/s]global step 2300, epoch: 1, batch: 2300, loss: 6.01401, acc: 0.33630, time: Fri Jan 21 10:54:25 2022
epoch 1 training:   1%|          | 2399/196351 [20:37<27:15:10,  1.98it/s]global step 2400, epoch: 1, batch: 2400, loss: 6.11621, acc: 0.33687, time: Fri Jan 21 10:55:16 2022
epoch 1 training:   1%|▏         | 2499/196351 [21:29<28:04:32,  1.92it/s]global step 2500, epoch: 1, batch: 2500, loss: 6.03431, acc: 0.33520, time: Fri Jan 21 10:56:08 2022
epoch 1 training:   1%|▏         | 2599/196351 [22:23<33:50:59,  1.59it/s]global step 2600, epoch: 1, batch: 2600, loss: 6.01460, acc: 0.33269, time: Fri Jan 21 10:57:02 2022
epoch 1 training:   1%|▏         | 2699/196351 [23:15<27:44:20,  1.94it/s]global step 2700, epoch: 1, batch: 2700, loss: 6.05510, acc: 0.33130, time: Fri Jan 21 10:57:54 2022
epoch 1 training:   1%|▏         | 2799/196351 [24:07<28:08:08,  1.91it/s]global step 2800, epoch: 1, batch: 2800, loss: 6.08504, acc: 0.32964, time: Fri Jan 21 10:58:46 2022
epoch 1 training:   1%|▏         | 2899/196351 [24:59<28:25:59,  1.89it/s]global step 2900, epoch: 1, batch: 2900, loss: 6.13079, acc: 0.33052, time: Fri Jan 21 10:59:38 2022
epoch 1 training:   2%|▏         | 2999/196351 [25:52<28:14:28,  1.90it/s]global step 3000, epoch: 1, batch: 3000, loss: 6.02761, acc: 0.33183, time: Fri Jan 21 11:00:30 2022
epoch 1 training:   2%|▏         | 3099/196351 [26:44<27:52:53,  1.93it/s]global step 3100, epoch: 1, batch: 3100, loss: 6.02497, acc: 0.33306, time: Fri Jan 21 11:01:23 2022
epoch 1 training:   2%|▏         | 3199/196351 [27:36<27:44:19,  1.93it/s]global step 3200, epoch: 1, batch: 3200, loss: 5.92951, acc: 0.33344, time: Fri Jan 21 11:02:15 2022
epoch 1 training:   2%|▏         | 3299/196351 [28:28<27:56:25,  1.92it/s]global step 3300, epoch: 1, batch: 3300, loss: 6.09176, acc: 0.33409, time: Fri Jan 21 11:03:07 2022
epoch 1 training:   2%|▏         | 3399/196351 [29:20<27:21:44,  1.96it/s]global step 3400, epoch: 1, batch: 3400, loss: 6.05443, acc: 0.33353, time: Fri Jan 21 11:03:59 2022
epoch 1 training:   2%|▏         | 3499/196351 [30:11<27:20:59,  1.96it/s]global step 3500, epoch: 1, batch: 3500, loss: 6.05645, acc: 0.33371, time: Fri Jan 21 11:04:50 2022
epoch 1 training:   2%|▏         | 3599/196351 [31:03<27:57:28,  1.92it/s]global step 3600, epoch: 1, batch: 3600, loss: 6.05985, acc: 0.33403, time: Fri Jan 21 11:05:41 2022
epoch 1 training:   2%|▏         | 3699/196351 [31:54<27:35:03,  1.94it/s]global step 3700, epoch: 1, batch: 3700, loss: 6.03097, acc: 0.33351, time: Fri Jan 21 11:06:33 2022
epoch 1 training:   2%|▏         | 3799/196351 [32:45<27:33:56,  1.94it/s]global step 3800, epoch: 1, batch: 3800, loss: 6.04497, acc: 0.33158, time: Fri Jan 21 11:07:24 2022
epoch 1 training:   2%|▏         | 3899/196351 [33:37<27:25:06,  1.95it/s]global step 3900, epoch: 1, batch: 3900, loss: 6.00598, acc: 0.33256, time: Fri Jan 21 11:08:16 2022
epoch 1 training:   2%|▏         | 3999/196351 [34:29<27:29:29,  1.94it/s]global step 4000, epoch: 1, batch: 4000, loss: 6.05384, acc: 0.33325, time: Fri Jan 21 11:09:07 2022
epoch 1 training:   2%|▏         | 4099/196351 [35:20<27:01:14,  1.98it/s]global step 4100, epoch: 1, batch: 4100, loss: 6.01310, acc: 0.33268, time: Fri Jan 21 11:09:58 2022
epoch 1 training:   2%|▏         | 4199/196351 [36:13<28:06:19,  1.90it/s]global step 4200, epoch: 1, batch: 4200, loss: 6.01380, acc: 0.33298, time: Fri Jan 21 11:10:52 2022
epoch 1 training:   2%|▏         | 4299/196351 [37:06<27:37:32,  1.93it/s]global step 4300, epoch: 1, batch: 4300, loss: 6.05029, acc: 0.33349, time: Fri Jan 21 11:11:44 2022
epoch 1 training:   2%|▏         | 4399/196351 [37:57<27:30:02,  1.94it/s]global step 4400, epoch: 1, batch: 4400, loss: 6.00423, acc: 0.33352, time: Fri Jan 21 11:12:36 2022
epoch 1 training:   2%|▏         | 4499/196351 [38:48<27:52:31,  1.91it/s]global step 4500, epoch: 1, batch: 4500, loss: 6.02210, acc: 0.33311, time: Fri Jan 21 11:13:27 2022
epoch 1 training:   2%|▏         | 4599/196351 [39:41<27:49:09,  1.91it/s]global step 4600, epoch: 1, batch: 4600, loss: 6.05730, acc: 0.33304, time: Fri Jan 21 11:14:19 2022
epoch 1 training:   2%|▏         | 4699/196351 [40:33<27:42:39,  1.92it/s]global step 4700, epoch: 1, batch: 4700, loss: 5.82841, acc: 0.33394, time: Fri Jan 21 11:15:12 2022
epoch 1 training:   2%|▏         | 4799/196351 [41:26<27:27:18,  1.94it/s]global step 4800, epoch: 1, batch: 4800, loss: 5.99565, acc: 0.33365, time: Fri Jan 21 11:16:05 2022
epoch 1 training:   2%|▏         | 4899/196351 [42:18<27:07:58,  1.96it/s]global step 4900, epoch: 1, batch: 4900, loss: 6.08864, acc: 0.33418, time: Fri Jan 21 11:16:56 2022
epoch 1 training:   3%|▎         | 4999/196351 [43:09<26:52:04,  1.98it/s]global step 5000, epoch: 1, batch: 5000, loss: 6.05491, acc: 0.33340, time: Fri Jan 21 11:17:48 2022
epoch 1 training:   3%|▎         | 5099/196351 [44:00<27:47:34,  1.91it/s]global step 5100, epoch: 1, batch: 5100, loss: 6.02189, acc: 0.33422, time: Fri Jan 21 11:18:39 2022
epoch 1 training:   3%|▎         | 5199/196351 [44:52<27:43:49,  1.91it/s]global step 5200, epoch: 1, batch: 5200, loss: 6.01871, acc: 0.33356, time: Fri Jan 21 11:19:31 2022
epoch 1 training:   3%|▎         | 5299/196351 [45:43<26:50:06,  1.98it/s]global step 5300, epoch: 1, batch: 5300, loss: 5.90175, acc: 0.33321, time: Fri Jan 21 11:20:22 2022
epoch 1 training:   3%|▎         | 5399/196351 [46:35<26:54:52,  1.97it/s]global step 5400, epoch: 1, batch: 5400, loss: 6.06801, acc: 0.33231, time: Fri Jan 21 11:21:13 2022
epoch 1 training:   3%|▎         | 5499/196351 [47:26<27:39:30,  1.92it/s]global step 5500, epoch: 1, batch: 5500, loss: 6.09099, acc: 0.33300, time: Fri Jan 21 11:22:05 2022
epoch 1 training:   3%|▎         | 5599/196351 [48:18<27:15:15,  1.94it/s]global step 5600, epoch: 1, batch: 5600, loss: 5.99851, acc: 0.33366, time: Fri Jan 21 11:22:57 2022
epoch 1 training:   3%|▎         | 5699/196351 [49:09<27:15:48,  1.94it/s]global step 5700, epoch: 1, batch: 5700, loss: 5.93491, acc: 0.33456, time: Fri Jan 21 11:23:48 2022
epoch 1 training:   3%|▎         | 5799/196351 [50:00<26:42:57,  1.98it/s]global step 5800, epoch: 1, batch: 5800, loss: 6.00156, acc: 0.33509, time: Fri Jan 21 11:24:39 2022
epoch 1 training:   3%|▎         | 5899/196351 [50:52<26:59:32,  1.96it/s]global step 5900, epoch: 1, batch: 5900, loss: 6.03474, acc: 0.33517, time: Fri Jan 21 11:25:30 2022
epoch 1 training:   3%|▎         | 5999/196351 [51:43<26:37:27,  1.99it/s]global step 6000, epoch: 1, batch: 6000, loss: 6.10370, acc: 0.33558, time: Fri Jan 21 11:26:22 2022
epoch 1 training:   3%|▎         | 6099/196351 [52:34<26:44:58,  1.98it/s]global step 6100, epoch: 1, batch: 6100, loss: 6.03578, acc: 0.33598, time: Fri Jan 21 11:27:12 2022
epoch 1 training:   3%|▎         | 6199/196351 [53:25<26:37:18,  1.98it/s]global step 6200, epoch: 1, batch: 6200, loss: 5.89342, acc: 0.33798, time: Fri Jan 21 11:28:03 2022
epoch 1 training:   3%|▎         | 6299/196351 [54:16<26:45:21,  1.97it/s]global step 6300, epoch: 1, batch: 6300, loss: 6.07727, acc: 0.33802, time: Fri Jan 21 11:28:55 2022
epoch 1 training:   3%|▎         | 6399/196351 [55:07<26:56:23,  1.96it/s]global step 6400, epoch: 1, batch: 6400, loss: 5.99795, acc: 0.33828, time: Fri Jan 21 11:29:46 2022
epoch 1 training:   3%|▎         | 6499/196351 [55:58<27:10:15,  1.94it/s]global step 6500, epoch: 1, batch: 6500, loss: 5.73695, acc: 0.33985, time: Fri Jan 21 11:30:36 2022
epoch 1 training:   3%|▎         | 6599/196351 [56:49<26:56:52,  1.96it/s]global step 6600, epoch: 1, batch: 6600, loss: 5.96008, acc: 0.34220, time: Fri Jan 21 11:31:28 2022
epoch 1 training:   3%|▎         | 6699/196351 [57:40<26:45:41,  1.97it/s]global step 6700, epoch: 1, batch: 6700, loss: 6.09069, acc: 0.34552, time: Fri Jan 21 11:32:19 2022
epoch 1 training:   3%|▎         | 6799/196351 [58:32<26:45:39,  1.97it/s]global step 6800, epoch: 1, batch: 6800, loss: 5.80344, acc: 0.34838, time: Fri Jan 21 11:33:11 2022
epoch 1 training:   4%|▎         | 6899/196351 [59:23<27:10:04,  1.94it/s]global step 6900, epoch: 1, batch: 6900, loss: 5.46861, acc: 0.35232, time: Fri Jan 21 11:34:02 2022
epoch 1 training:   4%|▎         | 6999/196351 [1:00:14<27:01:39,  1.95it/s]global step 7000, epoch: 1, batch: 7000, loss: 5.39746, acc: 0.35736, time: Fri Jan 21 11:34:53 2022
epoch 1 training:   4%|▎         | 7099/196351 [1:01:05<27:17:58,  1.93it/s]global step 7100, epoch: 1, batch: 7100, loss: 5.07891, acc: 0.36225, time: Fri Jan 21 11:35:44 2022
epoch 1 training:   4%|▎         | 7199/196351 [1:01:57<27:00:59,  1.94it/s]global step 7200, epoch: 1, batch: 7200, loss: 3.96753, acc: 0.36729, time: Fri Jan 21 11:36:35 2022
epoch 1 training:   4%|▎         | 7299/196351 [1:02:49<26:39:33,  1.97it/s]global step 7300, epoch: 1, batch: 7300, loss: 4.61243, acc: 0.37144, time: Fri Jan 21 11:37:28 2022
epoch 1 training:   4%|▍         | 7399/196351 [1:03:41<27:17:59,  1.92it/s]global step 7400, epoch: 1, batch: 7400, loss: 6.05879, acc: 0.37581, time: Fri Jan 21 11:38:20 2022
epoch 1 training:   4%|▍         | 7499/196351 [1:04:33<27:03:45,  1.94it/s]global step 7500, epoch: 1, batch: 7500, loss: 5.96996, acc: 0.37653, time: Fri Jan 21 11:39:12 2022
epoch 1 training:   4%|▍         | 7599/196351 [1:05:24<26:30:48,  1.98it/s]global step 7600, epoch: 1, batch: 7600, loss: 6.01605, acc: 0.37750, time: Fri Jan 21 11:40:03 2022
epoch 1 training:   4%|▍         | 7699/196351 [1:06:15<27:06:08,  1.93it/s]global step 7700, epoch: 1, batch: 7700, loss: 6.02627, acc: 0.37929, time: Fri Jan 21 11:40:54 2022
epoch 1 training:   4%|▍         | 7799/196351 [1:07:06<26:51:26,  1.95it/s]global step 7800, epoch: 1, batch: 7800, loss: 6.06092, acc: 0.38167, time: Fri Jan 21 11:41:45 2022
epoch 1 training:   4%|▍         | 7899/196351 [1:07:57<26:21:27,  1.99it/s]global step 7900, epoch: 1, batch: 7900, loss: 5.24465, acc: 0.38481, time: Fri Jan 21 11:42:36 2022
epoch 1 training:   4%|▍         | 7999/196351 [1:08:48<27:09:53,  1.93it/s]global step 8000, epoch: 1, batch: 8000, loss: 5.31615, acc: 0.38825, time: Fri Jan 21 11:43:27 2022
epoch 1 training:   4%|▍         | 8099/196351 [1:09:39<26:22:15,  1.98it/s]global step 8100, epoch: 1, batch: 8100, loss: 5.36907, acc: 0.39130, time: Fri Jan 21 11:44:18 2022
epoch 1 training:   4%|▍         | 8199/196351 [1:10:30<26:43:27,  1.96it/s]global step 8200, epoch: 1, batch: 8200, loss: 3.50193, acc: 0.39567, time: Fri Jan 21 11:45:09 2022
epoch 1 training:   4%|▍         | 8299/196351 [1:11:22<26:40:24,  1.96it/s]global step 8300, epoch: 1, batch: 8300, loss: 5.06031, acc: 0.39940, time: Fri Jan 21 11:46:00 2022
epoch 1 training:   4%|▍         | 8399/196351 [1:12:13<26:58:33,  1.94it/s]global step 8400, epoch: 1, batch: 8400, loss: 5.85133, acc: 0.40351, time: Fri Jan 21 11:46:51 2022
epoch 1 training:   4%|▍         | 8499/196351 [1:13:04<26:26:14,  1.97it/s]global step 8500, epoch: 1, batch: 8500, loss: 5.49094, acc: 0.40641, time: Fri Jan 21 11:47:42 2022
epoch 1 training:   4%|▍         | 8599/196351 [1:13:55<26:19:56,  1.98it/s]global step 8600, epoch: 1, batch: 8600, loss: 5.38384, acc: 0.41012, time: Fri Jan 21 11:48:33 2022
epoch 1 training:   4%|▍         | 8699/196351 [1:14:46<26:49:09,  1.94it/s]global step 8700, epoch: 1, batch: 8700, loss: 5.74083, acc: 0.41391, time: Fri Jan 21 11:49:25 2022
epoch 1 training:   4%|▍         | 8799/196351 [1:15:37<26:44:11,  1.95it/s]global step 8800, epoch: 1, batch: 8800, loss: 5.15752, acc: 0.41767, time: Fri Jan 21 11:50:16 2022
epoch 1 training:   5%|▍         | 8899/196351 [1:16:28<26:26:59,  1.97it/s]global step 8900, epoch: 1, batch: 8900, loss: 6.10795, acc: 0.42185, time: Fri Jan 21 11:51:07 2022
epoch 1 training:   5%|▍         | 8999/196351 [1:17:19<26:30:12,  1.96it/s]global step 9000, epoch: 1, batch: 9000, loss: 3.66135, acc: 0.42500, time: Fri Jan 21 11:51:58 2022
epoch 1 training:   5%|▍         | 9099/196351 [1:18:10<26:53:05,  1.93it/s]global step 9100, epoch: 1, batch: 9100, loss: 3.16005, acc: 0.42967, time: Fri Jan 21 11:52:49 2022
epoch 1 training:   5%|▍         | 9199/196351 [1:19:01<26:33:27,  1.96it/s]global step 9200, epoch: 1, batch: 9200, loss: 3.10134, acc: 0.43342, time: Fri Jan 21 11:53:40 2022
epoch 1 training:   5%|▍         | 9299/196351 [1:19:52<26:43:49,  1.94it/s]global step 9300, epoch: 1, batch: 9300, loss: 3.39104, acc: 0.43715, time: Fri Jan 21 11:54:31 2022
epoch 1 training:   5%|▍         | 9399/196351 [1:20:43<26:02:25,  1.99it/s]global step 9400, epoch: 1, batch: 9400, loss: 4.75501, acc: 0.44080, time: Fri Jan 21 11:55:22 2022
epoch 1 training:   5%|▍         | 9499/196351 [1:21:34<26:25:57,  1.96it/s]global step 9500, epoch: 1, batch: 9500, loss: 5.01191, acc: 0.44411, time: Fri Jan 21 11:56:13 2022
epoch 1 training:   5%|▍         | 9599/196351 [1:22:25<27:31:36,  1.88it/s]global step 9600, epoch: 1, batch: 9600, loss: 3.90659, acc: 0.44781, time: Fri Jan 21 11:57:04 2022
epoch 1 training:   5%|▍         | 9699/196351 [1:23:16<26:14:18,  1.98it/s]global step 9700, epoch: 1, batch: 9700, loss: 3.13459, acc: 0.45160, time: Fri Jan 21 11:57:55 2022
epoch 1 training:   5%|▍         | 9799/196351 [1:24:08<26:07:16,  1.98it/s]global step 9800, epoch: 1, batch: 9800, loss: 2.95358, acc: 0.45531, time: Fri Jan 21 11:58:46 2022
epoch 1 training:   5%|▌         | 9899/196351 [1:24:59<27:57:20,  1.85it/s]global step 9900, epoch: 1, batch: 9900, loss: 4.29695, acc: 0.45818, time: Fri Jan 21 11:59:38 2022
epoch 1 training:   5%|▌         | 9999/196351 [1:25:51<27:04:43,  1.91it/s]global step 10000, epoch: 1, batch: 10000, loss: 4.48286, acc: 0.46200, time: Fri Jan 21 12:00:30 2022
epoch 1 training:   5%|▌         | 10099/196351 [1:26:42<26:22:59,  1.96it/s]global step 10100, epoch: 1, batch: 10100, loss: 3.27476, acc: 0.46530, time: Fri Jan 21 12:01:21 2022
epoch 1 training:   5%|▌         | 10199/196351 [1:27:35<26:48:33,  1.93it/s]global step 10200, epoch: 1, batch: 10200, loss: 5.19003, acc: 0.46843, time: Fri Jan 21 12:02:13 2022
epoch 1 training:   5%|▌         | 10299/196351 [1:28:26<26:07:06,  1.98it/s]global step 10300, epoch: 1, batch: 10300, loss: 5.27954, acc: 0.47175, time: Fri Jan 21 12:03:05 2022
epoch 1 training:   5%|▌         | 10399/196351 [1:29:16<26:03:22,  1.98it/s]global step 10400, epoch: 1, batch: 10400, loss: 3.64290, acc: 0.47466, time: Fri Jan 21 12:03:55 2022
epoch 1 training:   5%|▌         | 10499/196351 [1:30:07<25:51:58,  2.00it/s]global step 10500, epoch: 1, batch: 10500, loss: 4.81837, acc: 0.47805, time: Fri Jan 21 12:04:46 2022
epoch 1 training:   5%|▌         | 10599/196351 [1:30:59<26:20:40,  1.96it/s]global step 10600, epoch: 1, batch: 10600, loss: 4.91207, acc: 0.48090, time: Fri Jan 21 12:05:37 2022
epoch 1 training:   5%|▌         | 10699/196351 [1:31:49<26:04:53,  1.98it/s]global step 10700, epoch: 1, batch: 10700, loss: 3.09312, acc: 0.48416, time: Fri Jan 21 12:06:28 2022
epoch 1 training:   5%|▌         | 10799/196351 [1:32:39<25:58:54,  1.98it/s]global step 10800, epoch: 1, batch: 10800, loss: 4.24340, acc: 0.48741, time: Fri Jan 21 12:07:18 2022
epoch 1 training:   6%|▌         | 10899/196351 [1:33:30<26:09:07,  1.97it/s]global step 10900, epoch: 1, batch: 10900, loss: 5.04654, acc: 0.49060, time: Fri Jan 21 12:08:09 2022
epoch 1 training:   6%|▌         | 10999/196351 [1:34:21<26:47:43,  1.92it/s]global step 11000, epoch: 1, batch: 11000, loss: 3.62183, acc: 0.49341, time: Fri Jan 21 12:09:00 2022
epoch 1 training:   6%|▌         | 11099/196351 [1:35:12<26:58:34,  1.91it/s]global step 11100, epoch: 1, batch: 11100, loss: 2.02196, acc: 0.49599, time: Fri Jan 21 12:09:51 2022
epoch 1 training:   6%|▌         | 11199/196351 [1:36:04<26:20:43,  1.95it/s]global step 11200, epoch: 1, batch: 11200, loss: 2.61300, acc: 0.49915, time: Fri Jan 21 12:10:43 2022
epoch 1 training:   6%|▌         | 11299/196351 [1:36:55<26:05:10,  1.97it/s]global step 11300, epoch: 1, batch: 11300, loss: 5.11935, acc: 0.50133, time: Fri Jan 21 12:11:34 2022
epoch 1 training:   6%|▌         | 11399/196351 [1:37:46<26:23:21,  1.95it/s]global step 11400, epoch: 1, batch: 11400, loss: 5.14858, acc: 0.50360, time: Fri Jan 21 12:12:25 2022
epoch 1 training:   6%|▌         | 11499/196351 [1:38:37<26:12:47,  1.96it/s]global step 11500, epoch: 1, batch: 11500, loss: 3.22928, acc: 0.50648, time: Fri Jan 21 12:13:16 2022
epoch 1 training:   6%|▌         | 11599/196351 [1:39:28<25:54:30,  1.98it/s]global step 11600, epoch: 1, batch: 11600, loss: 3.57753, acc: 0.50892, time: Fri Jan 21 12:14:07 2022
epoch 1 training:   6%|▌         | 11699/196351 [1:40:19<25:38:03,  2.00it/s]global step 11700, epoch: 1, batch: 11700, loss: 2.51135, acc: 0.51150, time: Fri Jan 21 12:14:58 2022
epoch 1 training:   6%|▌         | 11799/196351 [1:41:10<25:35:01,  2.00it/s]global step 11800, epoch: 1, batch: 11800, loss: 4.08695, acc: 0.51386, time: Fri Jan 21 12:15:49 2022
epoch 1 training:   6%|▌         | 11899/196351 [1:42:02<26:41:03,  1.92it/s]global step 11900, epoch: 1, batch: 11900, loss: 2.80164, acc: 0.51630, time: Fri Jan 21 12:16:40 2022
epoch 1 training:   6%|▌         | 11999/196351 [1:42:54<26:38:26,  1.92it/s]global step 12000, epoch: 1, batch: 12000, loss: 4.78060, acc: 0.51887, time: Fri Jan 21 12:17:32 2022
epoch 1 training:   6%|▌         | 12099/196351 [1:43:45<26:11:24,  1.95it/s]global step 12100, epoch: 1, batch: 12100, loss: 2.51179, acc: 0.52103, time: Fri Jan 21 12:18:23 2022
epoch 1 training:   6%|▌         | 12199/196351 [1:44:35<26:10:26,  1.95it/s]global step 12200, epoch: 1, batch: 12200, loss: 3.84554, acc: 0.52344, time: Fri Jan 21 12:19:14 2022
epoch 1 training:   6%|▋         | 12299/196351 [1:45:26<26:01:45,  1.96it/s]global step 12300, epoch: 1, batch: 12300, loss: 3.01195, acc: 0.52646, time: Fri Jan 21 12:20:05 2022
epoch 1 training:   6%|▋         | 12399/196351 [1:46:16<25:38:38,  1.99it/s]global step 12400, epoch: 1, batch: 12400, loss: 4.29259, acc: 0.52891, time: Fri Jan 21 12:20:55 2022
epoch 1 training:   6%|▋         | 12499/196351 [1:47:07<25:45:22,  1.98it/s]global step 12500, epoch: 1, batch: 12500, loss: 3.96796, acc: 0.53132, time: Fri Jan 21 12:21:46 2022
epoch 1 training:   6%|▋         | 12599/196351 [1:47:58<25:17:26,  2.02it/s]global step 12600, epoch: 1, batch: 12600, loss: 4.61336, acc: 0.53417, time: Fri Jan 21 12:22:37 2022
epoch 1 training:   6%|▋         | 12699/196351 [1:48:49<25:44:02,  1.98it/s]global step 12700, epoch: 1, batch: 12700, loss: 3.60863, acc: 0.53693, time: Fri Jan 21 12:23:28 2022
epoch 1 training:   7%|▋         | 12799/196351 [1:49:40<25:47:38,  1.98it/s]global step 12800, epoch: 1, batch: 12800, loss: 2.25957, acc: 0.53941, time: Fri Jan 21 12:24:19 2022
epoch 1 training:   7%|▋         | 12899/196351 [1:50:31<25:21:02,  2.01it/s]global step 12900, epoch: 1, batch: 12900, loss: 3.32035, acc: 0.54167, time: Fri Jan 21 12:25:09 2022
epoch 1 training:   7%|▋         | 12999/196351 [1:51:21<25:48:14,  1.97it/s]global step 13000, epoch: 1, batch: 13000, loss: 4.59613, acc: 0.54408, time: Fri Jan 21 12:26:00 2022
epoch 1 training:   7%|▋         | 13099/196351 [1:52:12<25:51:20,  1.97it/s]global step 13100, epoch: 1, batch: 13100, loss: 2.52070, acc: 0.54656, time: Fri Jan 21 12:26:50 2022
epoch 1 training:   7%|▋         | 13199/196351 [1:53:02<25:35:41,  1.99it/s]global step 13200, epoch: 1, batch: 13200, loss: 5.42801, acc: 0.54867, time: Fri Jan 21 12:27:41 2022
epoch 1 training:   7%|▋         | 13299/196351 [1:53:53<25:38:27,  1.98it/s]global step 13300, epoch: 1, batch: 13300, loss: 2.29587, acc: 0.55086, time: Fri Jan 21 12:28:31 2022
epoch 1 training:   7%|▋         | 13399/196351 [1:54:43<25:53:41,  1.96it/s]global step 13400, epoch: 1, batch: 13400, loss: 4.17093, acc: 0.55287, time: Fri Jan 21 12:29:22 2022
epoch 1 training:   7%|▋         | 13499/196351 [1:55:34<25:29:41,  1.99it/s]global step 13500, epoch: 1, batch: 13500, loss: 4.96770, acc: 0.55511, time: Fri Jan 21 12:30:13 2022
epoch 1 training:   7%|▋         | 13599/196351 [1:56:25<25:26:37,  2.00it/s]global step 13600, epoch: 1, batch: 13600, loss: 3.99699, acc: 0.55713, time: Fri Jan 21 12:31:04 2022
epoch 1 training:   7%|▋         | 13699/196351 [1:57:15<25:59:11,  1.95it/s]global step 13700, epoch: 1, batch: 13700, loss: 4.73303, acc: 0.55905, time: Fri Jan 21 12:31:54 2022
epoch 1 training:   7%|▋         | 13799/196351 [1:58:06<26:08:37,  1.94it/s]global step 13800, epoch: 1, batch: 13800, loss: 4.72483, acc: 0.56094, time: Fri Jan 21 12:32:45 2022
epoch 1 training:   7%|▋         | 13899/196351 [1:58:57<25:36:23,  1.98it/s]global step 13900, epoch: 1, batch: 13900, loss: 4.78180, acc: 0.56252, time: Fri Jan 21 12:33:35 2022
epoch 1 training:   7%|▋         | 13999/196351 [1:59:47<25:43:56,  1.97it/s]global step 14000, epoch: 1, batch: 14000, loss: 4.02530, acc: 0.56425, time: Fri Jan 21 12:34:26 2022
epoch 1 training:   7%|▋         | 14099/196351 [2:00:38<25:22:14,  2.00it/s]global step 14100, epoch: 1, batch: 14100, loss: 4.69502, acc: 0.56599, time: Fri Jan 21 12:35:17 2022
epoch 1 training:   7%|▋         | 14199/196351 [2:01:29<26:16:36,  1.93it/s]global step 14200, epoch: 1, batch: 14200, loss: 3.27677, acc: 0.56789, time: Fri Jan 21 12:36:08 2022
epoch 1 training:   7%|▋         | 14299/196351 [2:02:21<25:39:27,  1.97it/s]global step 14300, epoch: 1, batch: 14300, loss: 3.78241, acc: 0.56962, time: Fri Jan 21 12:37:00 2022
epoch 1 training:   7%|▋         | 14399/196351 [2:03:12<26:15:29,  1.92it/s]global step 14400, epoch: 1, batch: 14400, loss: 3.83576, acc: 0.57153, time: Fri Jan 21 12:37:51 2022
epoch 1 training:   7%|▋         | 14499/196351 [2:04:03<25:25:57,  1.99it/s]global step 14500, epoch: 1, batch: 14500, loss: 4.34357, acc: 0.57338, time: Fri Jan 21 12:38:42 2022
epoch 1 training:   7%|▋         | 14599/196351 [2:04:55<25:19:58,  1.99it/s]global step 14600, epoch: 1, batch: 14600, loss: 2.18362, acc: 0.57514, time: Fri Jan 21 12:39:33 2022
epoch 1 training:   7%|▋         | 14699/196351 [2:05:46<25:43:48,  1.96it/s]global step 14700, epoch: 1, batch: 14700, loss: 2.97557, acc: 0.57721, time: Fri Jan 21 12:40:24 2022
epoch 1 training:   8%|▊         | 14799/196351 [2:06:38<25:55:14,  1.95it/s]global step 14800, epoch: 1, batch: 14800, loss: 3.78610, acc: 0.57895, time: Fri Jan 21 12:41:17 2022
epoch 1 training:   8%|▊         | 14899/196351 [2:07:29<26:12:34,  1.92it/s]global step 14900, epoch: 1, batch: 14900, loss: 3.25965, acc: 0.58070, time: Fri Jan 21 12:42:08 2022
epoch 1 training:   8%|▊         | 14999/196351 [2:08:20<25:45:50,  1.96it/s]global step 15000, epoch: 1, batch: 15000, loss: 4.84657, acc: 0.58243, time: Fri Jan 21 12:42:59 2022
epoch 1 training:   8%|▊         | 15099/196351 [2:09:11<25:25:24,  1.98it/s]global step 15100, epoch: 1, batch: 15100, loss: 3.27426, acc: 0.58427, time: Fri Jan 21 12:43:50 2022
epoch 1 training:   8%|▊         | 15199/196351 [2:10:02<25:39:26,  1.96it/s]global step 15200, epoch: 1, batch: 15200, loss: 2.03431, acc: 0.58618, time: Fri Jan 21 12:44:41 2022
epoch 1 training:   8%|▊         | 15299/196351 [2:10:54<25:35:31,  1.97it/s]global step 15300, epoch: 1, batch: 15300, loss: 1.18974, acc: 0.58804, time: Fri Jan 21 12:45:33 2022
epoch 1 training:   8%|▊         | 15399/196351 [2:11:45<25:50:23,  1.95it/s]global step 15400, epoch: 1, batch: 15400, loss: 2.62602, acc: 0.58955, time: Fri Jan 21 12:46:24 2022
epoch 1 training:   8%|▊         | 15499/196351 [2:12:36<25:33:33,  1.97it/s]global step 15500, epoch: 1, batch: 15500, loss: 3.78710, acc: 0.59148, time: Fri Jan 21 12:47:15 2022
epoch 1 training:   8%|▊         | 15599/196351 [2:13:27<25:42:10,  1.95it/s]global step 15600, epoch: 1, batch: 15600, loss: 5.21565, acc: 0.59304, time: Fri Jan 21 12:48:06 2022
epoch 1 training:   8%|▊         | 15699/196351 [2:14:18<25:12:00,  1.99it/s]global step 15700, epoch: 1, batch: 15700, loss: 5.46054, acc: 0.59465, time: Fri Jan 21 12:48:57 2022
epoch 1 training:   8%|▊         | 15799/196351 [2:15:09<25:38:15,  1.96it/s]global step 15800, epoch: 1, batch: 15800, loss: 2.50304, acc: 0.59617, time: Fri Jan 21 12:49:48 2022
epoch 1 training:   8%|▊         | 15899/196351 [2:16:00<25:40:38,  1.95it/s]global step 15900, epoch: 1, batch: 15900, loss: 4.61088, acc: 0.59780, time: Fri Jan 21 12:50:39 2022
epoch 1 training:   8%|▊         | 15999/196351 [2:16:51<25:33:55,  1.96it/s]global step 16000, epoch: 1, batch: 16000, loss: 4.10705, acc: 0.59944, time: Fri Jan 21 12:51:30 2022
epoch 1 training:   8%|▊         | 16099/196351 [2:17:42<25:16:55,  1.98it/s]global step 16100, epoch: 1, batch: 16100, loss: 2.69851, acc: 0.60087, time: Fri Jan 21 12:52:21 2022
epoch 1 training:   8%|▊         | 16199/196351 [2:18:34<28:29:29,  1.76it/s]global step 16200, epoch: 1, batch: 16200, loss: 4.17248, acc: 0.60228, time: Fri Jan 21 12:53:13 2022
epoch 1 training:   8%|▊         | 16299/196351 [2:19:25<25:20:25,  1.97it/s]global step 16300, epoch: 1, batch: 16300, loss: 3.05016, acc: 0.60402, time: Fri Jan 21 12:54:04 2022
epoch 1 training:   8%|▊         | 16399/196351 [2:20:17<26:17:33,  1.90it/s]global step 16400, epoch: 1, batch: 16400, loss: 3.65965, acc: 0.60527, time: Fri Jan 21 12:54:56 2022
epoch 1 training:   8%|▊         | 16499/196351 [2:21:09<26:51:54,  1.86it/s]global step 16500, epoch: 1, batch: 16500, loss: 2.96822, acc: 0.60694, time: Fri Jan 21 12:55:47 2022
epoch 1 training:   8%|▊         | 16599/196351 [2:22:01<25:13:59,  1.98it/s]global step 16600, epoch: 1, batch: 16600, loss: 4.66294, acc: 0.60825, time: Fri Jan 21 12:56:40 2022
epoch 1 training:   9%|▊         | 16699/196351 [2:22:52<25:32:34,  1.95it/s]global step 16700, epoch: 1, batch: 16700, loss: 3.98351, acc: 0.60934, time: Fri Jan 21 12:57:31 2022
epoch 1 training:   9%|▊         | 16799/196351 [2:23:44<25:35:21,  1.95it/s]global step 16800, epoch: 1, batch: 16800, loss: 3.35553, acc: 0.61068, time: Fri Jan 21 12:58:23 2022
epoch 1 training:   9%|▊         | 16899/196351 [2:24:35<26:07:33,  1.91it/s]global step 16900, epoch: 1, batch: 16900, loss: 3.45494, acc: 0.61183, time: Fri Jan 21 12:59:14 2022
epoch 1 training:   9%|▊         | 16999/196351 [2:25:26<25:06:33,  1.98it/s]global step 17000, epoch: 1, batch: 17000, loss: 4.98395, acc: 0.61294, time: Fri Jan 21 13:00:05 2022
epoch 1 training:   9%|▊         | 17099/196351 [2:26:17<25:00:13,  1.99it/s]global step 17100, epoch: 1, batch: 17100, loss: 3.30376, acc: 0.61447, time: Fri Jan 21 13:00:56 2022
epoch 1 training:   9%|▉         | 17199/196351 [2:27:08<25:49:03,  1.93it/s]global step 17200, epoch: 1, batch: 17200, loss: 3.98773, acc: 0.61578, time: Fri Jan 21 13:01:47 2022
epoch 1 training:   9%|▉         | 17299/196351 [2:27:59<25:16:46,  1.97it/s]global step 17300, epoch: 1, batch: 17300, loss: 3.68361, acc: 0.61720, time: Fri Jan 21 13:02:38 2022
epoch 1 training:   9%|▉         | 17399/196351 [2:28:50<25:10:43,  1.97it/s]global step 17400, epoch: 1, batch: 17400, loss: 2.46726, acc: 0.61825, time: Fri Jan 21 13:03:29 2022
epoch 1 training:   9%|▉         | 17499/196351 [2:29:42<25:46:50,  1.93it/s]global step 17500, epoch: 1, batch: 17500, loss: 5.26699, acc: 0.61949, time: Fri Jan 21 13:04:20 2022
epoch 1 training:   9%|▉         | 17599/196351 [2:30:34<26:13:25,  1.89it/s]global step 17600, epoch: 1, batch: 17600, loss: 3.80266, acc: 0.62099, time: Fri Jan 21 13:05:12 2022
epoch 1 training:   9%|▉         | 17699/196351 [2:31:25<25:26:54,  1.95it/s]global step 17700, epoch: 1, batch: 17700, loss: 2.44661, acc: 0.62226, time: Fri Jan 21 13:06:04 2022
epoch 1 training:   9%|▉         | 17799/196351 [2:32:16<25:04:48,  1.98it/s]global step 17800, epoch: 1, batch: 17800, loss: 5.52215, acc: 0.62343, time: Fri Jan 21 13:06:55 2022
epoch 1 training:   9%|▉         | 17899/196351 [2:33:07<25:05:26,  1.98it/s]global step 17900, epoch: 1, batch: 17900, loss: 1.53568, acc: 0.62500, time: Fri Jan 21 13:07:46 2022
epoch 1 training:   9%|▉         | 17999/196351 [2:33:58<25:25:01,  1.95it/s]global step 18000, epoch: 1, batch: 18000, loss: 5.10747, acc: 0.62625, time: Fri Jan 21 13:08:37 2022
epoch 1 training:   9%|▉         | 18099/196351 [2:34:49<25:18:06,  1.96it/s]global step 18100, epoch: 1, batch: 18100, loss: 2.85817, acc: 0.62738, time: Fri Jan 21 13:09:27 2022
epoch 1 training:   9%|▉         | 18199/196351 [2:35:40<25:33:24,  1.94it/s]global step 18200, epoch: 1, batch: 18200, loss: 1.78608, acc: 0.62868, time: Fri Jan 21 13:10:19 2022
epoch 1 training:   9%|▉         | 18299/196351 [2:36:31<24:58:04,  1.98it/s]global step 18300, epoch: 1, batch: 18300, loss: 2.90565, acc: 0.62995, time: Fri Jan 21 13:11:09 2022
epoch 1 training:   9%|▉         | 18399/196351 [2:37:22<25:22:21,  1.95it/s]global step 18400, epoch: 1, batch: 18400, loss: 4.50630, acc: 0.63092, time: Fri Jan 21 13:12:00 2022
epoch 1 training:   9%|▉         | 18499/196351 [2:38:12<25:19:51,  1.95it/s]global step 18500, epoch: 1, batch: 18500, loss: 4.22860, acc: 0.63203, time: Fri Jan 21 13:12:51 2022
epoch 1 training:   9%|▉         | 18599/196351 [2:39:03<26:03:26,  1.89it/s]global step 18600, epoch: 1, batch: 18600, loss: 3.88440, acc: 0.63320, time: Fri Jan 21 13:13:42 2022
epoch 1 training:  10%|▉         | 18699/196351 [2:39:54<25:34:47,  1.93it/s]global step 18700, epoch: 1, batch: 18700, loss: 2.60420, acc: 0.63463, time: Fri Jan 21 13:14:33 2022
epoch 1 training:  10%|▉         | 18799/196351 [2:40:46<25:24:04,  1.94it/s]global step 18800, epoch: 1, batch: 18800, loss: 3.51174, acc: 0.63572, time: Fri Jan 21 13:15:25 2022
epoch 1 training:  10%|▉         | 18899/196351 [2:41:37<24:54:37,  1.98it/s]global step 18900, epoch: 1, batch: 18900, loss: 3.30484, acc: 0.63693, time: Fri Jan 21 13:16:16 2022
epoch 1 training:  10%|▉         | 18999/196351 [2:42:28<26:11:10,  1.88it/s]global step 19000, epoch: 1, batch: 19000, loss: 4.64623, acc: 0.63813, time: Fri Jan 21 13:17:07 2022
epoch 1 training:  10%|▉         | 19099/196351 [2:43:21<26:14:13,  1.88it/s]global step 19100, epoch: 1, batch: 19100, loss: 1.00188, acc: 0.63924, time: Fri Jan 21 13:18:00 2022
epoch 1 training:  10%|▉         | 19199/196351 [2:44:13<25:30:17,  1.93it/s]global step 19200, epoch: 1, batch: 19200, loss: 1.65664, acc: 0.64044, time: Fri Jan 21 13:18:52 2022
epoch 1 training:  10%|▉         | 19299/196351 [2:45:05<25:14:37,  1.95it/s]global step 19300, epoch: 1, batch: 19300, loss: 2.26979, acc: 0.64158, time: Fri Jan 21 13:19:44 2022
epoch 1 training:  10%|▉         | 19399/196351 [2:45:56<25:31:57,  1.93it/s]global step 19400, epoch: 1, batch: 19400, loss: 3.51313, acc: 0.64278, time: Fri Jan 21 13:20:35 2022
epoch 1 training:  10%|▉         | 19499/196351 [2:46:47<24:55:30,  1.97it/s]global step 19500, epoch: 1, batch: 19500, loss: 2.06044, acc: 0.64392, time: Fri Jan 21 13:21:26 2022
epoch 1 training:  10%|▉         | 19599/196351 [2:47:38<24:58:09,  1.97it/s]global step 19600, epoch: 1, batch: 19600, loss: 1.93993, acc: 0.64503, time: Fri Jan 21 13:22:17 2022
epoch 1 training:  10%|█         | 19699/196351 [2:48:29<25:11:46,  1.95it/s]global step 19700, epoch: 1, batch: 19700, loss: 2.46619, acc: 0.64619, time: Fri Jan 21 13:23:08 2022
epoch 1 training:  10%|█         | 19799/196351 [2:49:20<25:12:12,  1.95it/s]global step 19800, epoch: 1, batch: 19800, loss: 2.80821, acc: 0.64750, time: Fri Jan 21 13:23:59 2022
epoch 1 training:  10%|█         | 19899/196351 [2:50:11<25:18:21,  1.94it/s]global step 19900, epoch: 1, batch: 19900, loss: 2.99393, acc: 0.64864, time: Fri Jan 21 13:24:50 2022
epoch 1 training:  10%|█         | 19999/196351 [2:51:03<25:40:46,  1.91it/s]global step 20000, epoch: 1, batch: 20000, loss: 4.85074, acc: 0.64982, time: Fri Jan 21 13:25:41 2022
epoch 1 training:  10%|█         | 20099/196351 [2:51:54<24:44:00,  1.98it/s]global step 20100, epoch: 1, batch: 20100, loss: 5.18275, acc: 0.65075, time: Fri Jan 21 13:26:33 2022
epoch 1 training:  10%|█         | 20199/196351 [2:52:46<25:31:17,  1.92it/s]global step 20200, epoch: 1, batch: 20200, loss: 3.03538, acc: 0.65171, time: Fri Jan 21 13:27:24 2022
epoch 1 training:  10%|█         | 20299/196351 [2:53:37<24:29:41,  2.00it/s]global step 20300, epoch: 1, batch: 20300, loss: 2.92908, acc: 0.65256, time: Fri Jan 21 13:28:15 2022
epoch 1 training:  10%|█         | 20399/196351 [2:54:27<24:51:15,  1.97it/s]global step 20400, epoch: 1, batch: 20400, loss: 3.79345, acc: 0.65341, time: Fri Jan 21 13:29:06 2022
epoch 1 training:  10%|█         | 20499/196351 [2:55:19<24:53:29,  1.96it/s]global step 20500, epoch: 1, batch: 20500, loss: 5.00167, acc: 0.65427, time: Fri Jan 21 13:29:57 2022
epoch 1 training:  10%|█         | 20599/196351 [2:56:10<24:42:14,  1.98it/s]global step 20600, epoch: 1, batch: 20600, loss: 4.83057, acc: 0.65515, time: Fri Jan 21 13:30:49 2022
epoch 1 training:  11%|█         | 20699/196351 [2:57:02<24:43:43,  1.97it/s]global step 20700, epoch: 1, batch: 20700, loss: 5.09860, acc: 0.65611, time: Fri Jan 21 13:31:40 2022
epoch 1 training:  11%|█         | 20799/196351 [2:57:53<24:58:32,  1.95it/s]global step 20800, epoch: 1, batch: 20800, loss: 3.99964, acc: 0.65731, time: Fri Jan 21 13:32:32 2022
epoch 1 training:  11%|█         | 20899/196351 [2:58:45<25:00:40,  1.95it/s]global step 20900, epoch: 1, batch: 20900, loss: 3.89805, acc: 0.65833, time: Fri Jan 21 13:33:24 2022
epoch 1 training:  11%|█         | 20999/196351 [2:59:37<25:02:50,  1.94it/s]global step 21000, epoch: 1, batch: 21000, loss: 3.99965, acc: 0.65902, time: Fri Jan 21 13:34:15 2022
epoch 1 training:  11%|█         | 21099/196351 [3:00:29<25:44:44,  1.89it/s]global step 21100, epoch: 1, batch: 21100, loss: 5.19330, acc: 0.65995, time: Fri Jan 21 13:35:08 2022
epoch 1 training:  11%|█         | 21199/196351 [3:01:29<31:04:20,  1.57it/s]global step 21200, epoch: 1, batch: 21200, loss: 1.51871, acc: 0.66106, time: Fri Jan 21 13:36:08 2022
epoch 1 training:  11%|█         | 21299/196351 [3:02:21<24:39:50,  1.97it/s]global step 21300, epoch: 1, batch: 21300, loss: 4.38406, acc: 0.66183, time: Fri Jan 21 13:37:00 2022
epoch 1 training:  11%|█         | 21399/196351 [3:03:13<24:44:08,  1.96it/s]global step 21400, epoch: 1, batch: 21400, loss: 3.54240, acc: 0.66259, time: Fri Jan 21 13:37:52 2022
epoch 1 training:  11%|█         | 21499/196351 [3:04:04<24:50:33,  1.96it/s]global step 21500, epoch: 1, batch: 21500, loss: 4.70529, acc: 0.66353, time: Fri Jan 21 13:38:43 2022
epoch 1 training:  11%|█         | 21599/196351 [3:04:57<27:39:19,  1.76it/s]global step 21600, epoch: 1, batch: 21600, loss: 3.88580, acc: 0.66431, time: Fri Jan 21 13:39:36 2022
epoch 1 training:  11%|█         | 21699/196351 [3:05:49<24:52:45,  1.95it/s]global step 21700, epoch: 1, batch: 21700, loss: 4.66785, acc: 0.66528, time: Fri Jan 21 13:40:28 2022
epoch 1 training:  11%|█         | 21799/196351 [3:06:41<24:51:03,  1.95it/s]global step 21800, epoch: 1, batch: 21800, loss: 3.19475, acc: 0.66619, time: Fri Jan 21 13:41:20 2022
epoch 1 training:  11%|█         | 21899/196351 [3:07:32<24:47:39,  1.95it/s]global step 21900, epoch: 1, batch: 21900, loss: 3.99163, acc: 0.66710, time: Fri Jan 21 13:42:11 2022
epoch 1 training:  11%|█         | 21999/196351 [3:08:23<24:53:09,  1.95it/s]global step 22000, epoch: 1, batch: 22000, loss: 3.73957, acc: 0.66802, time: Fri Jan 21 13:43:02 2022
epoch 1 training:  11%|█▏        | 22099/196351 [3:09:14<25:15:04,  1.92it/s]global step 22100, epoch: 1, batch: 22100, loss: 2.96456, acc: 0.66880, time: Fri Jan 21 13:43:53 2022
epoch 1 training:  11%|█▏        | 22199/196351 [3:10:06<25:09:22,  1.92it/s]global step 22200, epoch: 1, batch: 22200, loss: 2.48720, acc: 0.66962, time: Fri Jan 21 13:44:44 2022
epoch 1 training:  11%|█▏        | 22299/196351 [3:10:57<24:24:23,  1.98it/s]global step 22300, epoch: 1, batch: 22300, loss: 2.89993, acc: 0.67054, time: Fri Jan 21 13:45:36 2022
epoch 1 training:  11%|█▏        | 22399/196351 [3:11:48<24:20:58,  1.98it/s]global step 22400, epoch: 1, batch: 22400, loss: 3.47790, acc: 0.67132, time: Fri Jan 21 13:46:27 2022
epoch 1 training:  11%|█▏        | 22499/196351 [3:12:39<24:47:40,  1.95it/s]global step 22500, epoch: 1, batch: 22500, loss: 4.51315, acc: 0.67213, time: Fri Jan 21 13:47:18 2022
epoch 1 training:  12%|█▏        | 22599/196351 [3:13:31<24:41:15,  1.96it/s]global step 22600, epoch: 1, batch: 22600, loss: 1.80789, acc: 0.67303, time: Fri Jan 21 13:48:09 2022
epoch 1 training:  12%|█▏        | 22699/196351 [3:14:22<24:48:45,  1.94it/s]global step 22700, epoch: 1, batch: 22700, loss: 2.17201, acc: 0.67374, time: Fri Jan 21 13:49:01 2022
epoch 1 training:  12%|█▏        | 22799/196351 [3:15:13<24:38:50,  1.96it/s]global step 22800, epoch: 1, batch: 22800, loss: 3.55121, acc: 0.67469, time: Fri Jan 21 13:49:52 2022
epoch 1 training:  12%|█▏        | 22899/196351 [3:16:04<24:26:00,  1.97it/s]global step 22900, epoch: 1, batch: 22900, loss: 3.33441, acc: 0.67546, time: Fri Jan 21 13:50:43 2022
epoch 1 training:  12%|█▏        | 22999/196351 [3:16:56<25:31:40,  1.89it/s]global step 23000, epoch: 1, batch: 23000, loss: 5.26377, acc: 0.67622, time: Fri Jan 21 13:51:34 2022
epoch 1 training:  12%|█▏        | 23099/196351 [3:17:47<24:12:17,  1.99it/s]global step 23100, epoch: 1, batch: 23100, loss: 2.33577, acc: 0.67697, time: Fri Jan 21 13:52:26 2022
epoch 1 training:  12%|█▏        | 23199/196351 [3:18:38<24:31:44,  1.96it/s]global step 23200, epoch: 1, batch: 23200, loss: 2.47450, acc: 0.67795, time: Fri Jan 21 13:53:17 2022
epoch 1 training:  12%|█▏        | 23299/196351 [3:19:29<24:34:17,  1.96it/s]global step 23300, epoch: 1, batch: 23300, loss: 3.89009, acc: 0.67878, time: Fri Jan 21 13:54:08 2022
epoch 1 training:  12%|█▏        | 23399/196351 [3:20:21<24:08:42,  1.99it/s]global step 23400, epoch: 1, batch: 23400, loss: 4.43004, acc: 0.67953, time: Fri Jan 21 13:54:59 2022
epoch 1 training:  12%|█▏        | 23499/196351 [3:21:12<24:53:44,  1.93it/s]global step 23500, epoch: 1, batch: 23500, loss: 4.39288, acc: 0.68030, time: Fri Jan 21 13:55:51 2022
epoch 1 training:  12%|█▏        | 23599/196351 [3:22:04<24:28:07,  1.96it/s]global step 23600, epoch: 1, batch: 23600, loss: 3.80056, acc: 0.68091, time: Fri Jan 21 13:56:43 2022
epoch 1 training:  12%|█▏        | 23699/196351 [3:22:56<24:48:44,  1.93it/s]global step 23700, epoch: 1, batch: 23700, loss: 4.34175, acc: 0.68171, time: Fri Jan 21 13:57:35 2022
epoch 1 training:  12%|█▏        | 23799/196351 [3:23:48<24:50:08,  1.93it/s]global step 23800, epoch: 1, batch: 23800, loss: 2.26640, acc: 0.68252, time: Fri Jan 21 13:58:27 2022
epoch 1 training:  12%|█▏        | 23899/196351 [3:24:40<25:22:45,  1.89it/s]global step 23900, epoch: 1, batch: 23900, loss: 6.13739, acc: 0.68318, time: Fri Jan 21 13:59:19 2022
epoch 1 training:  12%|█▏        | 23999/196351 [3:25:32<24:45:31,  1.93it/s]global step 24000, epoch: 1, batch: 24000, loss: 4.76507, acc: 0.68404, time: Fri Jan 21 14:00:10 2022
epoch 1 training:  12%|█▏        | 24099/196351 [3:26:24<24:24:00,  1.96it/s]global step 24100, epoch: 1, batch: 24100, loss: 2.07462, acc: 0.68461, time: Fri Jan 21 14:01:03 2022
epoch 1 training:  12%|█▏        | 24199/196351 [3:27:15<24:24:54,  1.96it/s]global step 24200, epoch: 1, batch: 24200, loss: 3.90867, acc: 0.68525, time: Fri Jan 21 14:01:54 2022
epoch 1 training:  12%|█▏        | 24299/196351 [3:28:07<24:33:59,  1.95it/s]global step 24300, epoch: 1, batch: 24300, loss: 3.46963, acc: 0.68591, time: Fri Jan 21 14:02:45 2022
epoch 1 training:  12%|█▏        | 24399/196351 [3:28:58<24:11:17,  1.97it/s]global step 24400, epoch: 1, batch: 24400, loss: 3.37024, acc: 0.68664, time: Fri Jan 21 14:03:37 2022
epoch 1 training:  12%|█▏        | 24499/196351 [3:29:49<24:17:23,  1.97it/s]global step 24500, epoch: 1, batch: 24500, loss: 2.63355, acc: 0.68741, time: Fri Jan 21 14:04:28 2022
epoch 1 training:  13%|█▎        | 24599/196351 [3:30:39<24:01:06,  1.99it/s]global step 24600, epoch: 1, batch: 24600, loss: 2.48719, acc: 0.68821, time: Fri Jan 21 14:05:18 2022
epoch 1 training:  13%|█▎        | 24699/196351 [3:31:30<24:16:17,  1.96it/s]global step 24700, epoch: 1, batch: 24700, loss: 3.04673, acc: 0.68885, time: Fri Jan 21 14:06:09 2022
epoch 1 training:  13%|█▎        | 24799/196351 [3:32:21<24:19:08,  1.96it/s]global step 24800, epoch: 1, batch: 24800, loss: 1.42435, acc: 0.68956, time: Fri Jan 21 14:07:00 2022
epoch 1 training:  13%|█▎        | 24899/196351 [3:33:12<23:48:03,  2.00it/s]global step 24900, epoch: 1, batch: 24900, loss: 3.01857, acc: 0.69030, time: Fri Jan 21 14:07:51 2022
epoch 1 training:  13%|█▎        | 24999/196351 [3:34:03<23:46:43,  2.00it/s]global step 25000, epoch: 1, batch: 25000, loss: 5.38580, acc: 0.69096, time: Fri Jan 21 14:08:42 2022
epoch 1 training:  13%|█▎        | 25099/196351 [3:34:54<24:51:55,  1.91it/s]global step 25100, epoch: 1, batch: 25100, loss: 1.94100, acc: 0.69171, time: Fri Jan 21 14:09:33 2022
epoch 1 training:  13%|█▎        | 25199/196351 [3:35:45<24:13:49,  1.96it/s]global step 25200, epoch: 1, batch: 25200, loss: 3.20677, acc: 0.69244, time: Fri Jan 21 14:10:24 2022
epoch 1 training:  13%|█▎        | 25299/196351 [3:36:36<24:14:07,  1.96it/s]global step 25300, epoch: 1, batch: 25300, loss: 3.47177, acc: 0.69294, time: Fri Jan 21 14:11:15 2022
epoch 1 training:  13%|█▎        | 25399/196351 [3:37:27<24:22:16,  1.95it/s]global step 25400, epoch: 1, batch: 25400, loss: 3.55783, acc: 0.69374, time: Fri Jan 21 14:12:06 2022
epoch 1 training:  13%|█▎        | 25499/196351 [3:38:19<25:11:22,  1.88it/s]global step 25500, epoch: 1, batch: 25500, loss: 3.52544, acc: 0.69447, time: Fri Jan 21 14:12:58 2022
epoch 1 training:  13%|█▎        | 25599/196351 [3:39:11<24:38:29,  1.92it/s]global step 25600, epoch: 1, batch: 25600, loss: 1.65724, acc: 0.69512, time: Fri Jan 21 14:13:49 2022
epoch 1 training:  13%|█▎        | 25699/196351 [3:40:02<23:58:12,  1.98it/s]global step 25700, epoch: 1, batch: 25700, loss: 2.63903, acc: 0.69570, time: Fri Jan 21 14:14:41 2022
epoch 1 training:  13%|█▎        | 25799/196351 [3:40:53<24:13:06,  1.96it/s]global step 25800, epoch: 1, batch: 25800, loss: 2.59147, acc: 0.69643, time: Fri Jan 21 14:15:32 2022
epoch 1 training:  13%|█▎        | 25899/196351 [3:41:44<24:29:34,  1.93it/s]global step 25900, epoch: 1, batch: 25900, loss: 2.51148, acc: 0.69714, time: Fri Jan 21 14:16:23 2022
epoch 1 training:  13%|█▎        | 25999/196351 [3:42:35<24:04:40,  1.97it/s]global step 26000, epoch: 1, batch: 26000, loss: 1.47085, acc: 0.69788, time: Fri Jan 21 14:17:14 2022
epoch 1 training:  13%|█▎        | 26099/196351 [3:43:26<23:43:59,  1.99it/s]global step 26100, epoch: 1, batch: 26100, loss: 5.50617, acc: 0.69858, time: Fri Jan 21 14:18:05 2022
epoch 1 training:  13%|█▎        | 26199/196351 [3:44:17<23:53:33,  1.98it/s]global step 26200, epoch: 1, batch: 26200, loss: 2.20409, acc: 0.69927, time: Fri Jan 21 14:18:56 2022
epoch 1 training:  13%|█▎        | 26299/196351 [3:45:08<24:20:46,  1.94it/s]global step 26300, epoch: 1, batch: 26300, loss: 4.06687, acc: 0.69994, time: Fri Jan 21 14:19:47 2022
epoch 1 training:  13%|█▎        | 26399/196351 [3:46:00<24:10:30,  1.95it/s]global step 26400, epoch: 1, batch: 26400, loss: 2.64137, acc: 0.70051, time: Fri Jan 21 14:20:38 2022
epoch 1 training:  13%|█▎        | 26499/196351 [3:46:51<24:11:38,  1.95it/s]global step 26500, epoch: 1, batch: 26500, loss: 2.78489, acc: 0.70115, time: Fri Jan 21 14:21:29 2022
epoch 1 training:  14%|█▎        | 26599/196351 [3:47:42<23:57:22,  1.97it/s]global step 26600, epoch: 1, batch: 26600, loss: 2.33209, acc: 0.70179, time: Fri Jan 21 14:22:20 2022
epoch 1 training:  14%|█▎        | 26699/196351 [3:48:33<24:05:07,  1.96it/s]global step 26700, epoch: 1, batch: 26700, loss: 1.79446, acc: 0.70232, time: Fri Jan 21 14:23:11 2022
epoch 1 training:  14%|█▎        | 26799/196351 [3:49:23<24:02:19,  1.96it/s]global step 26800, epoch: 1, batch: 26800, loss: 5.48799, acc: 0.70289, time: Fri Jan 21 14:24:02 2022
epoch 1 training:  14%|█▎        | 26899/196351 [3:50:15<24:02:31,  1.96it/s]global step 26900, epoch: 1, batch: 26900, loss: 2.72074, acc: 0.70351, time: Fri Jan 21 14:24:54 2022
epoch 1 training:  14%|█▍        | 26999/196351 [3:51:06<24:03:20,  1.96it/s]global step 27000, epoch: 1, batch: 27000, loss: 3.50352, acc: 0.70411, time: Fri Jan 21 14:25:45 2022
epoch 1 training:  14%|█▍        | 27099/196351 [3:51:57<24:00:06,  1.96it/s]global step 27100, epoch: 1, batch: 27100, loss: 2.96518, acc: 0.70482, time: Fri Jan 21 14:26:36 2022
epoch 1 training:  14%|█▍        | 27199/196351 [3:52:49<24:06:21,  1.95it/s]global step 27200, epoch: 1, batch: 27200, loss: 2.84494, acc: 0.70539, time: Fri Jan 21 14:27:28 2022
epoch 1 training:  14%|█▍        | 27299/196351 [3:53:40<24:02:03,  1.95it/s]global step 27300, epoch: 1, batch: 27300, loss: 5.32352, acc: 0.70582, time: Fri Jan 21 14:28:19 2022
epoch 1 training:  14%|█▍        | 27399/196351 [3:54:31<23:43:51,  1.98it/s]global step 27400, epoch: 1, batch: 27400, loss: 2.59639, acc: 0.70633, time: Fri Jan 21 14:29:10 2022
epoch 1 training:  14%|█▍        | 27499/196351 [3:55:22<24:26:36,  1.92it/s]global step 27500, epoch: 1, batch: 27500, loss: 3.99079, acc: 0.70689, time: Fri Jan 21 14:30:01 2022
epoch 1 training:  14%|█▍        | 27599/196351 [3:56:13<23:45:17,  1.97it/s]global step 27600, epoch: 1, batch: 27600, loss: 1.40383, acc: 0.70754, time: Fri Jan 21 14:30:52 2022
epoch 1 training:  14%|█▍        | 27699/196351 [3:57:04<24:05:33,  1.94it/s]global step 27700, epoch: 1, batch: 27700, loss: 2.67616, acc: 0.70814, time: Fri Jan 21 14:31:43 2022
epoch 1 training:  14%|█▍        | 27799/196351 [3:57:55<23:35:41,  1.98it/s]global step 27800, epoch: 1, batch: 27800, loss: 3.05329, acc: 0.70878, time: Fri Jan 21 14:32:33 2022
epoch 1 training:  14%|█▍        | 27899/196351 [3:58:46<23:44:08,  1.97it/s]global step 27900, epoch: 1, batch: 27900, loss: 4.53342, acc: 0.70941, time: Fri Jan 21 14:33:24 2022
epoch 1 training:  14%|█▍        | 27999/196351 [3:59:37<24:12:12,  1.93it/s]global step 28000, epoch: 1, batch: 28000, loss: 4.36117, acc: 0.71002, time: Fri Jan 21 14:34:16 2022
epoch 1 training:  14%|█▍        | 28099/196351 [4:00:29<23:46:37,  1.97it/s]global step 28100, epoch: 1, batch: 28100, loss: 0.78010, acc: 0.71050, time: Fri Jan 21 14:35:08 2022
epoch 1 training:  14%|█▍        | 28199/196351 [4:01:20<23:52:06,  1.96it/s]global step 28200, epoch: 1, batch: 28200, loss: 3.87130, acc: 0.71106, time: Fri Jan 21 14:35:59 2022
epoch 1 training:  14%|█▍        | 28299/196351 [4:02:12<24:14:47,  1.93it/s]global step 28300, epoch: 1, batch: 28300, loss: 3.67096, acc: 0.71141, time: Fri Jan 21 14:36:51 2022
epoch 1 training:  14%|█▍        | 28399/196351 [4:03:05<24:48:55,  1.88it/s]global step 28400, epoch: 1, batch: 28400, loss: 3.23599, acc: 0.71192, time: Fri Jan 21 14:37:43 2022
epoch 1 training:  15%|█▍        | 28499/196351 [4:03:56<24:05:06,  1.94it/s]global step 28500, epoch: 1, batch: 28500, loss: 0.92226, acc: 0.71246, time: Fri Jan 21 14:38:35 2022
epoch 1 training:  15%|█▍        | 28599/196351 [4:04:48<23:56:02,  1.95it/s]global step 28600, epoch: 1, batch: 28600, loss: 3.66684, acc: 0.71302, time: Fri Jan 21 14:39:27 2022
epoch 1 training:  15%|█▍        | 28699/196351 [4:05:40<23:50:23,  1.95it/s]global step 28700, epoch: 1, batch: 28700, loss: 4.92353, acc: 0.71348, time: Fri Jan 21 14:40:18 2022
epoch 1 training:  15%|█▍        | 28799/196351 [4:06:31<23:28:57,  1.98it/s]global step 28800, epoch: 1, batch: 28800, loss: 3.06764, acc: 0.71396, time: Fri Jan 21 14:41:09 2022
epoch 1 training:  15%|█▍        | 28899/196351 [4:07:22<23:29:49,  1.98it/s]global step 28900, epoch: 1, batch: 28900, loss: 2.76834, acc: 0.71441, time: Fri Jan 21 14:42:00 2022
epoch 1 training:  15%|█▍        | 28999/196351 [4:08:13<23:59:36,  1.94it/s]global step 29000, epoch: 1, batch: 29000, loss: 3.24585, acc: 0.71510, time: Fri Jan 21 14:42:52 2022
epoch 1 training:  15%|█▍        | 29099/196351 [4:09:04<23:58:09,  1.94it/s]global step 29100, epoch: 1, batch: 29100, loss: 2.44229, acc: 0.71569, time: Fri Jan 21 14:43:43 2022
epoch 1 training:  15%|█▍        | 29199/196351 [4:09:55<23:36:50,  1.97it/s]global step 29200, epoch: 1, batch: 29200, loss: 4.00186, acc: 0.71623, time: Fri Jan 21 14:44:34 2022
epoch 1 training:  15%|█▍        | 29299/196351 [4:10:47<23:55:21,  1.94it/s]global step 29300, epoch: 1, batch: 29300, loss: 3.19198, acc: 0.71679, time: Fri Jan 21 14:45:26 2022
epoch 1 training:  15%|█▍        | 29399/196351 [4:11:38<23:33:08,  1.97it/s]global step 29400, epoch: 1, batch: 29400, loss: 3.58014, acc: 0.71740, time: Fri Jan 21 14:46:17 2022
epoch 1 training:  15%|█▌        | 29499/196351 [4:12:29<23:18:25,  1.99it/s]global step 29500, epoch: 1, batch: 29500, loss: 1.74727, acc: 0.71790, time: Fri Jan 21 14:47:08 2022
epoch 1 training:  15%|█▌        | 29599/196351 [4:13:20<23:58:20,  1.93it/s]global step 29600, epoch: 1, batch: 29600, loss: 2.65239, acc: 0.71848, time: Fri Jan 21 14:47:58 2022
epoch 1 training:  15%|█▌        | 29699/196351 [4:14:11<23:16:07,  1.99it/s]global step 29700, epoch: 1, batch: 29700, loss: 2.54457, acc: 0.71911, time: Fri Jan 21 14:48:49 2022
epoch 1 training:  15%|█▌        | 29799/196351 [4:15:02<23:59:12,  1.93it/s]global step 29800, epoch: 1, batch: 29800, loss: 4.04243, acc: 0.71965, time: Fri Jan 21 14:49:41 2022
epoch 1 training:  15%|█▌        | 29899/196351 [4:15:53<23:27:03,  1.97it/s]global step 29900, epoch: 1, batch: 29900, loss: 3.61172, acc: 0.72012, time: Fri Jan 21 14:50:32 2022
epoch 1 training:  15%|█▌        | 29999/196351 [4:16:44<23:17:13,  1.98it/s]global step 30000, epoch: 1, batch: 30000, loss: 0.86993, acc: 0.72067, time: Fri Jan 21 14:51:23 2022
epoch 1 training:  15%|█▌        | 30099/196351 [4:17:36<24:04:55,  1.92it/s]global step 30100, epoch: 1, batch: 30100, loss: 2.18116, acc: 0.72110, time: Fri Jan 21 14:52:15 2022
epoch 1 training:  15%|█▌        | 30199/196351 [4:18:27<23:57:43,  1.93it/s]global step 30200, epoch: 1, batch: 30200, loss: 4.34536, acc: 0.72151, time: Fri Jan 21 14:53:06 2022
epoch 1 training:  15%|█▌        | 30299/196351 [4:19:19<24:00:55,  1.92it/s]global step 30300, epoch: 1, batch: 30300, loss: 5.13282, acc: 0.72198, time: Fri Jan 21 14:53:58 2022
epoch 1 training:  15%|█▌        | 30399/196351 [4:20:11<24:17:12,  1.90it/s]global step 30400, epoch: 1, batch: 30400, loss: 4.17985, acc: 0.72260, time: Fri Jan 21 14:54:50 2022
epoch 1 training:  16%|█▌        | 30499/196351 [4:21:02<23:34:52,  1.95it/s]global step 30500, epoch: 1, batch: 30500, loss: 2.75290, acc: 0.72307, time: Fri Jan 21 14:55:41 2022
epoch 1 training:  16%|█▌        | 30599/196351 [4:21:54<23:56:17,  1.92it/s]global step 30600, epoch: 1, batch: 30600, loss: 4.56624, acc: 0.72355, time: Fri Jan 21 14:56:33 2022
epoch 1 training:  16%|█▌        | 30699/196351 [4:22:45<23:32:10,  1.96it/s]global step 30700, epoch: 1, batch: 30700, loss: 0.70739, acc: 0.72402, time: Fri Jan 21 14:57:24 2022
epoch 1 training:  16%|█▌        | 30799/196351 [4:23:37<23:32:52,  1.95it/s]global step 30800, epoch: 1, batch: 30800, loss: 2.17813, acc: 0.72466, time: Fri Jan 21 14:58:16 2022
epoch 1 training:  16%|█▌        | 30899/196351 [4:24:28<23:35:58,  1.95it/s]global step 30900, epoch: 1, batch: 30900, loss: 1.58741, acc: 0.72515, time: Fri Jan 21 14:59:07 2022
epoch 1 training:  16%|█▌        | 30999/196351 [4:25:19<23:11:37,  1.98it/s]global step 31000, epoch: 1, batch: 31000, loss: 2.76209, acc: 0.72556, time: Fri Jan 21 14:59:58 2022
epoch 1 training:  16%|█▌        | 31099/196351 [4:26:11<23:38:15,  1.94it/s]global step 31100, epoch: 1, batch: 31100, loss: 2.25564, acc: 0.72588, time: Fri Jan 21 15:00:50 2022
epoch 1 training:  16%|█▌        | 31199/196351 [4:27:03<24:14:21,  1.89it/s]global step 31200, epoch: 1, batch: 31200, loss: 2.52290, acc: 0.72638, time: Fri Jan 21 15:01:42 2022
epoch 1 training:  16%|█▌        | 31299/196351 [4:27:56<23:54:07,  1.92it/s]global step 31300, epoch: 1, batch: 31300, loss: 1.92695, acc: 0.72679, time: Fri Jan 21 15:02:35 2022
epoch 1 training:  16%|█▌        | 31399/196351 [4:28:48<23:51:02,  1.92it/s]global step 31400, epoch: 1, batch: 31400, loss: 5.32576, acc: 0.72725, time: Fri Jan 21 15:03:27 2022
epoch 1 training:  16%|█▌        | 31499/196351 [4:29:41<24:35:01,  1.86it/s]global step 31500, epoch: 1, batch: 31500, loss: 3.24214, acc: 0.72775, time: Fri Jan 21 15:04:19 2022
epoch 1 training:  16%|█▌        | 31599/196351 [4:30:34<23:34:27,  1.94it/s]global step 31600, epoch: 1, batch: 31600, loss: 0.86645, acc: 0.72828, time: Fri Jan 21 15:05:13 2022
epoch 1 training:  16%|█▌        | 31699/196351 [4:31:25<23:31:29,  1.94it/s]global step 31700, epoch: 1, batch: 31700, loss: 2.08779, acc: 0.72866, time: Fri Jan 21 15:06:04 2022
epoch 1 training:  16%|█▌        | 31799/196351 [4:32:18<24:11:43,  1.89it/s]global step 31800, epoch: 1, batch: 31800, loss: 0.92557, acc: 0.72910, time: Fri Jan 21 15:06:57 2022
epoch 1 training:  16%|█▌        | 31899/196351 [4:33:09<23:09:58,  1.97it/s]global step 31900, epoch: 1, batch: 31900, loss: 3.89617, acc: 0.72947, time: Fri Jan 21 15:07:48 2022
epoch 1 training:  16%|█▋        | 31999/196351 [4:34:00<23:18:03,  1.96it/s]global step 32000, epoch: 1, batch: 32000, loss: 1.96935, acc: 0.72992, time: Fri Jan 21 15:08:39 2022
epoch 1 training:  16%|█▋        | 32099/196351 [4:34:52<23:46:02,  1.92it/s]global step 32100, epoch: 1, batch: 32100, loss: 4.65136, acc: 0.73030, time: Fri Jan 21 15:09:31 2022
epoch 1 training:  16%|█▋        | 32199/196351 [4:35:44<23:45:47,  1.92it/s]global step 32200, epoch: 1, batch: 32200, loss: 1.38421, acc: 0.73068, time: Fri Jan 21 15:10:22 2022
epoch 1 training:  16%|█▋        | 32299/196351 [4:36:36<23:20:43,  1.95it/s]global step 32300, epoch: 1, batch: 32300, loss: 0.96605, acc: 0.73127, time: Fri Jan 21 15:11:14 2022
epoch 1 training:  17%|█▋        | 32399/196351 [4:37:28<23:30:17,  1.94it/s]global step 32400, epoch: 1, batch: 32400, loss: 3.15501, acc: 0.73165, time: Fri Jan 21 15:12:06 2022
epoch 1 training:  17%|█▋        | 32499/196351 [4:38:20<23:34:31,  1.93it/s]global step 32500, epoch: 1, batch: 32500, loss: 2.99042, acc: 0.73197, time: Fri Jan 21 15:12:58 2022
epoch 1 training:  17%|█▋        | 32599/196351 [4:39:12<23:43:10,  1.92it/s]global step 32600, epoch: 1, batch: 32600, loss: 1.86853, acc: 0.73242, time: Fri Jan 21 15:13:50 2022
epoch 1 training:  17%|█▋        | 32699/196351 [4:40:04<23:21:00,  1.95it/s]global step 32700, epoch: 1, batch: 32700, loss: 2.83301, acc: 0.73294, time: Fri Jan 21 15:14:42 2022
epoch 1 training:  17%|█▋        | 32799/196351 [4:40:57<24:10:16,  1.88it/s]global step 32800, epoch: 1, batch: 32800, loss: 3.95564, acc: 0.73328, time: Fri Jan 21 15:15:35 2022
epoch 1 training:  17%|█▋        | 32899/196351 [4:41:49<23:23:43,  1.94it/s]global step 32900, epoch: 1, batch: 32900, loss: 4.27770, acc: 0.73359, time: Fri Jan 21 15:16:27 2022
epoch 1 training:  17%|█▋        | 32999/196351 [4:42:40<23:06:57,  1.96it/s]global step 33000, epoch: 1, batch: 33000, loss: 3.08168, acc: 0.73418, time: Fri Jan 21 15:17:19 2022
epoch 1 training:  17%|█▋        | 33099/196351 [4:43:32<23:33:45,  1.92it/s]global step 33100, epoch: 1, batch: 33100, loss: 4.17168, acc: 0.73471, time: Fri Jan 21 15:18:11 2022
epoch 1 training:  17%|█▋        | 33199/196351 [4:44:23<22:50:52,  1.98it/s]global step 33200, epoch: 1, batch: 33200, loss: 2.05615, acc: 0.73515, time: Fri Jan 21 15:19:02 2022
epoch 1 training:  17%|█▋        | 33299/196351 [4:45:14<23:03:26,  1.96it/s]global step 33300, epoch: 1, batch: 33300, loss: 2.25088, acc: 0.73569, time: Fri Jan 21 15:19:53 2022
epoch 1 training:  17%|█▋        | 33399/196351 [4:46:06<22:55:46,  1.97it/s]global step 33400, epoch: 1, batch: 33400, loss: 3.97957, acc: 0.73602, time: Fri Jan 21 15:20:45 2022
epoch 1 training:  17%|█▋        | 33499/196351 [4:46:57<22:37:54,  2.00it/s]global step 33500, epoch: 1, batch: 33500, loss: 1.45858, acc: 0.73633, time: Fri Jan 21 15:21:36 2022
epoch 1 training:  17%|█▋        | 33599/196351 [4:47:48<23:02:34,  1.96it/s]global step 33600, epoch: 1, batch: 33600, loss: 3.06712, acc: 0.73676, time: Fri Jan 21 15:22:26 2022
epoch 1 training:  17%|█▋        | 33699/196351 [4:48:39<23:05:50,  1.96it/s]global step 33700, epoch: 1, batch: 33700, loss: 2.51558, acc: 0.73721, time: Fri Jan 21 15:23:18 2022
epoch 1 training:  17%|█▋        | 33799/196351 [4:49:30<23:25:44,  1.93it/s]global step 33800, epoch: 1, batch: 33800, loss: 4.57151, acc: 0.73759, time: Fri Jan 21 15:24:09 2022
epoch 1 training:  17%|█▋        | 33899/196351 [4:50:21<22:55:09,  1.97it/s]global step 33900, epoch: 1, batch: 33900, loss: 1.13313, acc: 0.73795, time: Fri Jan 21 15:25:00 2022
epoch 1 training:  17%|█▋        | 33999/196351 [4:51:12<22:37:37,  1.99it/s]global step 34000, epoch: 1, batch: 34000, loss: 2.31583, acc: 0.73838, time: Fri Jan 21 15:25:51 2022
epoch 1 training:  17%|█▋        | 34099/196351 [4:52:02<22:38:39,  1.99it/s]global step 34100, epoch: 1, batch: 34100, loss: 3.82375, acc: 0.73867, time: Fri Jan 21 15:26:41 2022
epoch 1 training:  17%|█▋        | 34199/196351 [4:52:54<23:06:25,  1.95it/s]global step 34200, epoch: 1, batch: 34200, loss: 2.25790, acc: 0.73918, time: Fri Jan 21 15:27:33 2022
epoch 1 training:  17%|█▋        | 34299/196351 [4:53:44<22:48:32,  1.97it/s]global step 34300, epoch: 1, batch: 34300, loss: 2.40411, acc: 0.73965, time: Fri Jan 21 15:28:23 2022
epoch 1 training:  18%|█▊        | 34399/196351 [4:54:35<22:17:20,  2.02it/s]global step 34400, epoch: 1, batch: 34400, loss: 1.44368, acc: 0.74003, time: Fri Jan 21 15:29:14 2022
epoch 1 training:  18%|█▊        | 34499/196351 [4:55:26<22:45:32,  1.98it/s]global step 34500, epoch: 1, batch: 34500, loss: 1.79135, acc: 0.74036, time: Fri Jan 21 15:30:05 2022
epoch 1 training:  18%|█▊        | 34599/196351 [4:56:17<22:48:26,  1.97it/s]global step 34600, epoch: 1, batch: 34600, loss: 1.82456, acc: 0.74077, time: Fri Jan 21 15:30:56 2022
epoch 1 training:  18%|█▊        | 34699/196351 [4:57:08<22:37:24,  1.98it/s]global step 34700, epoch: 1, batch: 34700, loss: 1.13480, acc: 0.74115, time: Fri Jan 21 15:31:47 2022
epoch 1 training:  18%|█▊        | 34799/196351 [4:57:59<22:23:52,  2.00it/s]global step 34800, epoch: 1, batch: 34800, loss: 1.70157, acc: 0.74154, time: Fri Jan 21 15:32:37 2022
epoch 1 training:  18%|█▊        | 34899/196351 [4:58:49<22:35:34,  1.99it/s]global step 34900, epoch: 1, batch: 34900, loss: 3.04454, acc: 0.74189, time: Fri Jan 21 15:33:28 2022
epoch 1 training:  18%|█▊        | 34999/196351 [4:59:40<22:29:04,  1.99it/s]global step 35000, epoch: 1, batch: 35000, loss: 2.46257, acc: 0.74233, time: Fri Jan 21 15:34:18 2022
epoch 1 training:  18%|█▊        | 35099/196351 [5:00:30<22:23:54,  2.00it/s]global step 35100, epoch: 1, batch: 35100, loss: 2.71161, acc: 0.74279, time: Fri Jan 21 15:35:09 2022
epoch 1 training:  18%|█▊        | 35199/196351 [5:01:20<22:48:43,  1.96it/s]global step 35200, epoch: 1, batch: 35200, loss: 1.35491, acc: 0.74327, time: Fri Jan 21 15:35:59 2022
epoch 1 training:  18%|█▊        | 35299/196351 [5:02:11<22:38:59,  1.98it/s]global step 35300, epoch: 1, batch: 35300, loss: 4.35881, acc: 0.74358, time: Fri Jan 21 15:36:50 2022
epoch 1 training:  18%|█▊        | 35399/196351 [5:03:02<22:26:58,  1.99it/s]global step 35400, epoch: 1, batch: 35400, loss: 2.49217, acc: 0.74397, time: Fri Jan 21 15:37:40 2022
epoch 1 training:  18%|█▊        | 35499/196351 [5:03:52<22:30:18,  1.99it/s]global step 35500, epoch: 1, batch: 35500, loss: 3.71048, acc: 0.74417, time: Fri Jan 21 15:38:31 2022
epoch 1 training:  18%|█▊        | 35599/196351 [5:04:43<23:07:05,  1.93it/s]global step 35600, epoch: 1, batch: 35600, loss: 3.24525, acc: 0.74454, time: Fri Jan 21 15:39:22 2022
epoch 1 training:  18%|█▊        | 35699/196351 [5:05:34<22:36:26,  1.97it/s]global step 35700, epoch: 1, batch: 35700, loss: 3.62128, acc: 0.74497, time: Fri Jan 21 15:40:13 2022
epoch 1 training:  18%|█▊        | 35799/196351 [5:06:25<22:28:02,  1.99it/s]global step 35800, epoch: 1, batch: 35800, loss: 4.14715, acc: 0.74541, time: Fri Jan 21 15:41:04 2022
epoch 1 training:  18%|█▊        | 35899/196351 [5:07:15<22:44:27,  1.96it/s]global step 35900, epoch: 1, batch: 35900, loss: 4.60340, acc: 0.74574, time: Fri Jan 21 15:41:54 2022
epoch 1 training:  18%|█▊        | 35999/196351 [5:08:06<22:20:00,  1.99it/s]global step 36000, epoch: 1, batch: 36000, loss: 2.32835, acc: 0.74603, time: Fri Jan 21 15:42:45 2022
epoch 1 training:  18%|█▊        | 36099/196351 [5:08:56<22:28:04,  1.98it/s]global step 36100, epoch: 1, batch: 36100, loss: 1.24195, acc: 0.74630, time: Fri Jan 21 15:43:35 2022
epoch 1 training:  18%|█▊        | 36199/196351 [5:09:48<24:40:15,  1.80it/s]global step 36200, epoch: 1, batch: 36200, loss: 1.97316, acc: 0.74663, time: Fri Jan 21 15:44:27 2022
epoch 1 training:  18%|█▊        | 36299/196351 [5:10:40<22:09:46,  2.01it/s]global step 36300, epoch: 1, batch: 36300, loss: 1.04270, acc: 0.74696, time: Fri Jan 21 15:45:19 2022
epoch 1 training:  19%|█▊        | 36399/196351 [5:11:31<22:39:09,  1.96it/s]global step 36400, epoch: 1, batch: 36400, loss: 2.92193, acc: 0.74727, time: Fri Jan 21 15:46:10 2022
epoch 1 training:  19%|█▊        | 36499/196351 [5:12:22<23:01:06,  1.93it/s]global step 36500, epoch: 1, batch: 36500, loss: 2.64954, acc: 0.74762, time: Fri Jan 21 15:47:01 2022
epoch 1 training:  19%|█▊        | 36599/196351 [5:13:13<22:52:21,  1.94it/s]global step 36600, epoch: 1, batch: 36600, loss: 2.10437, acc: 0.74803, time: Fri Jan 21 15:47:52 2022
epoch 1 training:  19%|█▊        | 36699/196351 [5:14:05<22:26:37,  1.98it/s]global step 36700, epoch: 1, batch: 36700, loss: 1.68087, acc: 0.74838, time: Fri Jan 21 15:48:44 2022
epoch 1 training:  19%|█▊        | 36799/196351 [5:14:56<22:40:28,  1.95it/s]global step 36800, epoch: 1, batch: 36800, loss: 0.89562, acc: 0.74872, time: Fri Jan 21 15:49:35 2022
epoch 1 training:  19%|█▉        | 36899/196351 [5:15:47<22:32:15,  1.97it/s]global step 36900, epoch: 1, batch: 36900, loss: 3.62164, acc: 0.74912, time: Fri Jan 21 15:50:26 2022
epoch 1 training:  19%|█▉        | 36999/196351 [5:16:38<22:33:14,  1.96it/s]global step 37000, epoch: 1, batch: 37000, loss: 1.46633, acc: 0.74946, time: Fri Jan 21 15:51:16 2022
epoch 1 training:  19%|█▉        | 37099/196351 [5:17:29<22:36:33,  1.96it/s]global step 37100, epoch: 1, batch: 37100, loss: 2.20196, acc: 0.74984, time: Fri Jan 21 15:52:08 2022
epoch 1 training:  19%|█▉        | 37199/196351 [5:18:21<23:14:35,  1.90it/s]global step 37200, epoch: 1, batch: 37200, loss: 2.10699, acc: 0.75020, time: Fri Jan 21 15:52:59 2022
epoch 1 training:  19%|█▉        | 37299/196351 [5:19:12<23:00:09,  1.92it/s]global step 37300, epoch: 1, batch: 37300, loss: 3.02398, acc: 0.75063, time: Fri Jan 21 15:53:51 2022
epoch 1 training:  19%|█▉        | 37399/196351 [5:20:04<22:55:40,  1.93it/s]global step 37400, epoch: 1, batch: 37400, loss: 2.41714, acc: 0.75088, time: Fri Jan 21 15:54:42 2022
epoch 1 training:  19%|█▉        | 37499/196351 [5:20:55<22:22:26,  1.97it/s]global step 37500, epoch: 1, batch: 37500, loss: 2.95470, acc: 0.75124, time: Fri Jan 21 15:55:34 2022
epoch 1 training:  19%|█▉        | 37599/196351 [5:21:46<22:28:15,  1.96it/s]global step 37600, epoch: 1, batch: 37600, loss: 2.05071, acc: 0.75160, time: Fri Jan 21 15:56:25 2022
epoch 1 training:  19%|█▉        | 37699/196351 [5:22:38<22:24:49,  1.97it/s]global step 37700, epoch: 1, batch: 37700, loss: 2.90891, acc: 0.75191, time: Fri Jan 21 15:57:17 2022
epoch 1 training:  19%|█▉        | 37799/196351 [5:23:29<22:37:46,  1.95it/s]global step 37800, epoch: 1, batch: 37800, loss: 3.54601, acc: 0.75218, time: Fri Jan 21 15:58:07 2022
epoch 1 training:  19%|█▉        | 37899/196351 [5:24:19<22:04:07,  1.99it/s]global step 37900, epoch: 1, batch: 37900, loss: 1.56773, acc: 0.75249, time: Fri Jan 21 15:58:58 2022
epoch 1 training:  19%|█▉        | 37999/196351 [5:25:10<22:17:59,  1.97it/s]global step 38000, epoch: 1, batch: 38000, loss: 2.96622, acc: 0.75280, time: Fri Jan 21 15:59:49 2022
epoch 1 training:  19%|█▉        | 38099/196351 [5:26:01<22:23:37,  1.96it/s]global step 38100, epoch: 1, batch: 38100, loss: 2.55281, acc: 0.75312, time: Fri Jan 21 16:00:40 2022
epoch 1 training:  19%|█▉        | 38199/196351 [5:26:52<23:02:03,  1.91it/s]global step 38200, epoch: 1, batch: 38200, loss: 3.06577, acc: 0.75343, time: Fri Jan 21 16:01:31 2022
epoch 1 training:  20%|█▉        | 38299/196351 [5:27:43<22:17:50,  1.97it/s]global step 38300, epoch: 1, batch: 38300, loss: 1.51170, acc: 0.75380, time: Fri Jan 21 16:02:22 2022
epoch 1 training:  20%|█▉        | 38399/196351 [5:28:34<21:48:56,  2.01it/s]global step 38400, epoch: 1, batch: 38400, loss: 4.92430, acc: 0.75401, time: Fri Jan 21 16:03:12 2022
epoch 1 training:  20%|█▉        | 38499/196351 [5:29:25<22:22:45,  1.96it/s]global step 38500, epoch: 1, batch: 38500, loss: 4.29860, acc: 0.75432, time: Fri Jan 21 16:04:04 2022
epoch 1 training:  20%|█▉        | 38599/196351 [5:30:16<22:35:41,  1.94it/s]global step 38600, epoch: 1, batch: 38600, loss: 2.21590, acc: 0.75465, time: Fri Jan 21 16:04:55 2022
epoch 1 training:  20%|█▉        | 38699/196351 [5:31:08<22:23:25,  1.96it/s]global step 38700, epoch: 1, batch: 38700, loss: 2.02199, acc: 0.75497, time: Fri Jan 21 16:05:46 2022
epoch 1 training:  20%|█▉        | 38799/196351 [5:31:59<22:26:45,  1.95it/s]global step 38800, epoch: 1, batch: 38800, loss: 3.04354, acc: 0.75527, time: Fri Jan 21 16:06:38 2022
epoch 1 training:  20%|█▉        | 38899/196351 [5:32:50<22:29:32,  1.94it/s]global step 38900, epoch: 1, batch: 38900, loss: 2.56311, acc: 0.75563, time: Fri Jan 21 16:07:29 2022
epoch 1 training:  20%|█▉        | 38999/196351 [5:33:42<22:24:00,  1.95it/s]global step 39000, epoch: 1, batch: 39000, loss: 1.48334, acc: 0.75601, time: Fri Jan 21 16:08:21 2022
epoch 1 training:  20%|█▉        | 39099/196351 [5:34:33<22:12:59,  1.97it/s]global step 39100, epoch: 1, batch: 39100, loss: 2.46963, acc: 0.75625, time: Fri Jan 21 16:09:12 2022
epoch 1 training:  20%|█▉        | 39199/196351 [5:35:25<22:44:07,  1.92it/s]global step 39200, epoch: 1, batch: 39200, loss: 3.34104, acc: 0.75662, time: Fri Jan 21 16:10:04 2022
epoch 1 training:  20%|██        | 39299/196351 [5:36:16<21:55:09,  1.99it/s]global step 39300, epoch: 1, batch: 39300, loss: 2.64277, acc: 0.75700, time: Fri Jan 21 16:10:55 2022
epoch 1 training:  20%|██        | 39399/196351 [5:37:07<23:14:20,  1.88it/s]global step 39400, epoch: 1, batch: 39400, loss: 1.48940, acc: 0.75725, time: Fri Jan 21 16:11:46 2022
epoch 1 training:  20%|██        | 39499/196351 [5:37:58<22:00:02,  1.98it/s]global step 39500, epoch: 1, batch: 39500, loss: 2.01750, acc: 0.75765, time: Fri Jan 21 16:12:37 2022
epoch 1 training:  20%|██        | 39599/196351 [5:38:50<22:19:13,  1.95it/s]global step 39600, epoch: 1, batch: 39600, loss: 1.80666, acc: 0.75799, time: Fri Jan 21 16:13:29 2022
epoch 1 training:  20%|██        | 39699/196351 [5:39:41<22:38:09,  1.92it/s]global step 39700, epoch: 1, batch: 39700, loss: 2.62231, acc: 0.75824, time: Fri Jan 21 16:14:20 2022
epoch 1 training:  20%|██        | 39799/196351 [5:40:33<21:57:22,  1.98it/s]global step 39800, epoch: 1, batch: 39800, loss: 1.52424, acc: 0.75858, time: Fri Jan 21 16:15:12 2022
epoch 1 training:  20%|██        | 39899/196351 [5:41:24<21:50:31,  1.99it/s]global step 39900, epoch: 1, batch: 39900, loss: 2.64407, acc: 0.75887, time: Fri Jan 21 16:16:03 2022
epoch 1 training:  20%|██        | 39999/196351 [5:42:16<22:21:34,  1.94it/s]global step 40000, epoch: 1, batch: 40000, loss: 4.86889, acc: 0.75918, time: Fri Jan 21 16:16:55 2022
epoch 1 training:  20%|██        | 40099/196351 [5:43:08<22:28:00,  1.93it/s]global step 40100, epoch: 1, batch: 40100, loss: 1.17172, acc: 0.75941, time: Fri Jan 21 16:17:47 2022
epoch 1 training:  20%|██        | 40199/196351 [5:43:59<22:11:40,  1.95it/s]global step 40200, epoch: 1, batch: 40200, loss: 2.16249, acc: 0.75975, time: Fri Jan 21 16:18:38 2022
epoch 1 training:  21%|██        | 40299/196351 [5:44:51<22:10:15,  1.96it/s]global step 40300, epoch: 1, batch: 40300, loss: 1.10810, acc: 0.76007, time: Fri Jan 21 16:19:30 2022
epoch 1 training:  21%|██        | 40399/196351 [5:45:43<21:58:10,  1.97it/s]global step 40400, epoch: 1, batch: 40400, loss: 1.29463, acc: 0.76041, time: Fri Jan 21 16:20:22 2022
epoch 1 training:  21%|██        | 40499/196351 [5:46:34<21:48:27,  1.99it/s]global step 40500, epoch: 1, batch: 40500, loss: 3.54259, acc: 0.76067, time: Fri Jan 21 16:21:13 2022
epoch 1 training:  21%|██        | 40599/196351 [5:47:26<21:44:59,  1.99it/s]global step 40600, epoch: 1, batch: 40600, loss: 2.52662, acc: 0.76086, time: Fri Jan 21 16:22:04 2022
epoch 1 training:  21%|██        | 40699/196351 [5:48:16<21:46:39,  1.99it/s]global step 40700, epoch: 1, batch: 40700, loss: 1.03962, acc: 0.76128, time: Fri Jan 21 16:22:55 2022
epoch 1 training:  21%|██        | 40799/196351 [5:49:07<22:10:20,  1.95it/s]global step 40800, epoch: 1, batch: 40800, loss: 2.34060, acc: 0.76150, time: Fri Jan 21 16:23:46 2022
epoch 1 training:  21%|██        | 40899/196351 [5:49:58<21:49:34,  1.98it/s]global step 40900, epoch: 1, batch: 40900, loss: 1.97505, acc: 0.76169, time: Fri Jan 21 16:24:37 2022
epoch 1 training:  21%|██        | 40999/196351 [5:50:49<22:04:05,  1.96it/s]global step 41000, epoch: 1, batch: 41000, loss: 2.27048, acc: 0.76198, time: Fri Jan 21 16:25:28 2022
epoch 1 training:  21%|██        | 41099/196351 [5:51:41<22:47:04,  1.89it/s]global step 41100, epoch: 1, batch: 41100, loss: 2.23311, acc: 0.76227, time: Fri Jan 21 16:26:20 2022
epoch 1 training:  21%|██        | 41199/196351 [5:52:33<22:37:50,  1.90it/s]global step 41200, epoch: 1, batch: 41200, loss: 2.98912, acc: 0.76254, time: Fri Jan 21 16:27:12 2022
epoch 1 training:  21%|██        | 41299/196351 [5:53:24<21:56:22,  1.96it/s]global step 41300, epoch: 1, batch: 41300, loss: 1.00787, acc: 0.76282, time: Fri Jan 21 16:28:03 2022
epoch 1 training:  21%|██        | 41399/196351 [5:54:15<21:54:16,  1.96it/s]global step 41400, epoch: 1, batch: 41400, loss: 1.91050, acc: 0.76306, time: Fri Jan 21 16:28:54 2022
epoch 1 training:  21%|██        | 41499/196351 [5:55:07<22:17:07,  1.93it/s]global step 41500, epoch: 1, batch: 41500, loss: 1.81327, acc: 0.76325, time: Fri Jan 21 16:29:45 2022
epoch 1 training:  21%|██        | 41599/196351 [5:55:58<21:53:16,  1.96it/s]global step 41600, epoch: 1, batch: 41600, loss: 4.92431, acc: 0.76358, time: Fri Jan 21 16:30:36 2022
epoch 1 training:  21%|██        | 41699/196351 [5:56:49<22:00:39,  1.95it/s]global step 41700, epoch: 1, batch: 41700, loss: 1.89874, acc: 0.76376, time: Fri Jan 21 16:31:27 2022
epoch 1 training:  21%|██▏       | 41799/196351 [5:57:40<22:13:00,  1.93it/s]global step 41800, epoch: 1, batch: 41800, loss: 1.37002, acc: 0.76413, time: Fri Jan 21 16:32:19 2022
epoch 1 training:  21%|██▏       | 41899/196351 [5:58:31<21:48:09,  1.97it/s]global step 41900, epoch: 1, batch: 41900, loss: 3.70231, acc: 0.76433, time: Fri Jan 21 16:33:10 2022
epoch 1 training:  21%|██▏       | 41999/196351 [5:59:23<22:09:41,  1.93it/s]global step 42000, epoch: 1, batch: 42000, loss: 3.22121, acc: 0.76460, time: Fri Jan 21 16:34:02 2022
epoch 1 training:  21%|██▏       | 42099/196351 [6:00:14<22:22:49,  1.91it/s]global step 42100, epoch: 1, batch: 42100, loss: 1.51572, acc: 0.76481, time: Fri Jan 21 16:34:53 2022
epoch 1 training:  21%|██▏       | 42199/196351 [6:01:06<22:09:02,  1.93it/s]global step 42200, epoch: 1, batch: 42200, loss: 4.19972, acc: 0.76511, time: Fri Jan 21 16:35:45 2022
epoch 1 training:  22%|██▏       | 42299/196351 [6:01:58<22:10:25,  1.93it/s]global step 42300, epoch: 1, batch: 42300, loss: 1.51210, acc: 0.76538, time: Fri Jan 21 16:36:37 2022
epoch 1 training:  22%|██▏       | 42399/196351 [6:02:50<22:24:05,  1.91it/s]global step 42400, epoch: 1, batch: 42400, loss: 2.78901, acc: 0.76567, time: Fri Jan 21 16:37:29 2022
epoch 1 training:  22%|██▏       | 42499/196351 [6:03:41<21:41:43,  1.97it/s]global step 42500, epoch: 1, batch: 42500, loss: 3.29851, acc: 0.76595, time: Fri Jan 21 16:38:20 2022
epoch 1 training:  22%|██▏       | 42599/196351 [6:04:34<22:06:22,  1.93it/s]global step 42600, epoch: 1, batch: 42600, loss: 1.11495, acc: 0.76621, time: Fri Jan 21 16:39:12 2022
epoch 1 training:  22%|██▏       | 42699/196351 [6:05:25<21:51:30,  1.95it/s]global step 42700, epoch: 1, batch: 42700, loss: 3.37942, acc: 0.76641, time: Fri Jan 21 16:40:04 2022
epoch 1 training:  22%|██▏       | 42799/196351 [6:06:17<22:13:15,  1.92it/s]global step 42800, epoch: 1, batch: 42800, loss: 0.61682, acc: 0.76664, time: Fri Jan 21 16:40:55 2022
epoch 1 training:  22%|██▏       | 42899/196351 [6:07:08<22:06:05,  1.93it/s]global step 42900, epoch: 1, batch: 42900, loss: 2.40741, acc: 0.76688, time: Fri Jan 21 16:41:47 2022
epoch 1 training:  22%|██▏       | 42999/196351 [6:07:59<21:33:35,  1.98it/s]global step 43000, epoch: 1, batch: 43000, loss: 2.46302, acc: 0.76721, time: Fri Jan 21 16:42:38 2022
epoch 1 training:  22%|██▏       | 43099/196351 [6:08:50<21:30:43,  1.98it/s]global step 43100, epoch: 1, batch: 43100, loss: 3.11724, acc: 0.76746, time: Fri Jan 21 16:43:29 2022
epoch 1 training:  22%|██▏       | 43199/196351 [6:09:41<21:32:55,  1.97it/s]global step 43200, epoch: 1, batch: 43200, loss: 3.40128, acc: 0.76773, time: Fri Jan 21 16:44:20 2022
epoch 1 training:  22%|██▏       | 43299/196351 [6:10:32<21:53:09,  1.94it/s]global step 43300, epoch: 1, batch: 43300, loss: 3.40813, acc: 0.76789, time: Fri Jan 21 16:45:11 2022
epoch 1 training:  22%|██▏       | 43399/196351 [6:11:23<21:39:50,  1.96it/s]global step 43400, epoch: 1, batch: 43400, loss: 2.87418, acc: 0.76816, time: Fri Jan 21 16:46:02 2022
epoch 1 training:  22%|██▏       | 43499/196351 [6:12:14<21:33:54,  1.97it/s]global step 43500, epoch: 1, batch: 43500, loss: 1.48884, acc: 0.76841, time: Fri Jan 21 16:46:53 2022
epoch 1 training:  22%|██▏       | 43599/196351 [6:13:06<21:53:50,  1.94it/s]global step 43600, epoch: 1, batch: 43600, loss: 2.07852, acc: 0.76876, time: Fri Jan 21 16:47:44 2022
epoch 1 training:  22%|██▏       | 43699/196351 [6:13:57<22:13:35,  1.91it/s]global step 43700, epoch: 1, batch: 43700, loss: 3.34456, acc: 0.76902, time: Fri Jan 21 16:48:36 2022
epoch 1 training:  22%|██▏       | 43799/196351 [6:14:48<21:47:20,  1.94it/s]global step 43800, epoch: 1, batch: 43800, loss: 3.57708, acc: 0.76930, time: Fri Jan 21 16:49:27 2022
epoch 1 training:  22%|██▏       | 43899/196351 [6:15:39<21:25:55,  1.98it/s]global step 43900, epoch: 1, batch: 43900, loss: 1.22887, acc: 0.76951, time: Fri Jan 21 16:50:18 2022
epoch 1 training:  22%|██▏       | 43999/196351 [6:16:30<21:41:00,  1.95it/s]global step 44000, epoch: 1, batch: 44000, loss: 1.97965, acc: 0.76978, time: Fri Jan 21 16:51:09 2022
epoch 1 training:  22%|██▏       | 44099/196351 [6:17:22<21:39:38,  1.95it/s]global step 44100, epoch: 1, batch: 44100, loss: 1.54919, acc: 0.77001, time: Fri Jan 21 16:52:00 2022
epoch 1 training:  23%|██▎       | 44199/196351 [6:18:13<21:38:17,  1.95it/s]global step 44200, epoch: 1, batch: 44200, loss: 4.15864, acc: 0.77023, time: Fri Jan 21 16:52:51 2022
epoch 1 training:  23%|██▎       | 44299/196351 [6:19:05<21:58:49,  1.92it/s]global step 44300, epoch: 1, batch: 44300, loss: 4.27411, acc: 0.77044, time: Fri Jan 21 16:53:43 2022
epoch 1 training:  23%|██▎       | 44399/196351 [6:19:57<21:22:20,  1.97it/s]global step 44400, epoch: 1, batch: 44400, loss: 4.24610, acc: 0.77069, time: Fri Jan 21 16:54:35 2022
epoch 1 training:  23%|██▎       | 44499/196351 [6:20:48<21:34:15,  1.96it/s]global step 44500, epoch: 1, batch: 44500, loss: 2.63439, acc: 0.77087, time: Fri Jan 21 16:55:27 2022
epoch 1 training:  23%|██▎       | 44599/196351 [6:21:39<21:28:59,  1.96it/s]global step 44600, epoch: 1, batch: 44600, loss: 0.41355, acc: 0.77118, time: Fri Jan 21 16:56:18 2022
epoch 1 training:  23%|██▎       | 44699/196351 [6:22:31<22:08:29,  1.90it/s]global step 44700, epoch: 1, batch: 44700, loss: 2.13580, acc: 0.77144, time: Fri Jan 21 16:57:09 2022
epoch 1 training:  23%|██▎       | 44799/196351 [6:23:23<21:33:07,  1.95it/s]global step 44800, epoch: 1, batch: 44800, loss: 0.96505, acc: 0.77166, time: Fri Jan 21 16:58:02 2022
epoch 1 training:  23%|██▎       | 44899/196351 [6:24:14<21:55:47,  1.92it/s]global step 44900, epoch: 1, batch: 44900, loss: 1.80973, acc: 0.77188, time: Fri Jan 21 16:58:53 2022
epoch 1 training:  23%|██▎       | 44999/196351 [6:25:05<21:14:10,  1.98it/s]global step 45000, epoch: 1, batch: 45000, loss: 3.97748, acc: 0.77204, time: Fri Jan 21 16:59:44 2022
epoch 1 training:  23%|██▎       | 45099/196351 [6:25:56<21:41:31,  1.94it/s]global step 45100, epoch: 1, batch: 45100, loss: 3.09796, acc: 0.77218, time: Fri Jan 21 17:00:34 2022
epoch 1 training:  23%|██▎       | 45199/196351 [6:26:47<21:54:16,  1.92it/s]global step 45200, epoch: 1, batch: 45200, loss: 1.57461, acc: 0.77240, time: Fri Jan 21 17:01:26 2022
epoch 1 training:  23%|██▎       | 45299/196351 [6:27:38<21:27:20,  1.96it/s]global step 45300, epoch: 1, batch: 45300, loss: 2.18571, acc: 0.77263, time: Fri Jan 21 17:02:17 2022
epoch 1 training:  23%|██▎       | 45399/196351 [6:28:30<21:13:08,  1.98it/s]global step 45400, epoch: 1, batch: 45400, loss: 2.86843, acc: 0.77292, time: Fri Jan 21 17:03:08 2022
epoch 1 training:  23%|██▎       | 45499/196351 [6:29:21<21:02:12,  1.99it/s]global step 45500, epoch: 1, batch: 45500, loss: 1.50070, acc: 0.77315, time: Fri Jan 21 17:04:00 2022
epoch 1 training:  23%|██▎       | 45599/196351 [6:30:12<21:17:31,  1.97it/s]global step 45600, epoch: 1, batch: 45600, loss: 1.55038, acc: 0.77345, time: Fri Jan 21 17:04:51 2022
epoch 1 training:  23%|██▎       | 45699/196351 [6:31:04<21:46:23,  1.92it/s]global step 45700, epoch: 1, batch: 45700, loss: 3.32649, acc: 0.77364, time: Fri Jan 21 17:05:42 2022
epoch 1 training:  23%|██▎       | 45799/196351 [6:31:56<21:14:20,  1.97it/s]global step 45800, epoch: 1, batch: 45800, loss: 3.24834, acc: 0.77395, time: Fri Jan 21 17:06:35 2022
epoch 1 training:  23%|██▎       | 45899/196351 [6:32:47<21:07:17,  1.98it/s]global step 45900, epoch: 1, batch: 45900, loss: 1.57896, acc: 0.77417, time: Fri Jan 21 17:07:26 2022
epoch 1 training:  23%|██▎       | 45999/196351 [6:33:39<21:14:35,  1.97it/s]global step 46000, epoch: 1, batch: 46000, loss: 1.25300, acc: 0.77445, time: Fri Jan 21 17:08:17 2022
epoch 1 training:  23%|██▎       | 46099/196351 [6:34:30<21:19:55,  1.96it/s]global step 46100, epoch: 1, batch: 46100, loss: 2.31106, acc: 0.77474, time: Fri Jan 21 17:09:09 2022
epoch 1 training:  24%|██▎       | 46199/196351 [6:35:21<21:02:32,  1.98it/s]global step 46200, epoch: 1, batch: 46200, loss: 1.67698, acc: 0.77482, time: Fri Jan 21 17:10:00 2022
epoch 1 training:  24%|██▎       | 46299/196351 [6:36:13<21:08:52,  1.97it/s]global step 46300, epoch: 1, batch: 46300, loss: 2.17929, acc: 0.77504, time: Fri Jan 21 17:10:51 2022
epoch 1 training:  24%|██▎       | 46399/196351 [6:37:03<21:11:36,  1.97it/s]global step 46400, epoch: 1, batch: 46400, loss: 2.65282, acc: 0.77523, time: Fri Jan 21 17:11:42 2022
epoch 1 training:  24%|██▎       | 46499/196351 [6:37:54<21:07:22,  1.97it/s]global step 46500, epoch: 1, batch: 46500, loss: 2.49568, acc: 0.77555, time: Fri Jan 21 17:12:33 2022
epoch 1 training:  24%|██▎       | 46599/196351 [6:38:45<20:59:31,  1.98it/s]global step 46600, epoch: 1, batch: 46600, loss: 2.42978, acc: 0.77574, time: Fri Jan 21 17:13:24 2022
epoch 1 training:  24%|██▍       | 46699/196351 [6:39:36<21:16:07,  1.95it/s]global step 46700, epoch: 1, batch: 46700, loss: 2.49213, acc: 0.77597, time: Fri Jan 21 17:14:15 2022
epoch 1 training:  24%|██▍       | 46799/196351 [6:40:26<21:22:20,  1.94it/s]global step 46800, epoch: 1, batch: 46800, loss: 4.91695, acc: 0.77622, time: Fri Jan 21 17:15:05 2022
epoch 1 training:  24%|██▍       | 46899/196351 [6:41:17<21:10:33,  1.96it/s]global step 46900, epoch: 1, batch: 46900, loss: 3.74027, acc: 0.77645, time: Fri Jan 21 17:15:56 2022
epoch 1 training:  24%|██▍       | 46999/196351 [6:42:09<21:20:44,  1.94it/s]global step 47000, epoch: 1, batch: 47000, loss: 2.20800, acc: 0.77662, time: Fri Jan 21 17:16:48 2022
epoch 1 training:  24%|██▍       | 47099/196351 [6:43:00<21:06:22,  1.96it/s]global step 47100, epoch: 1, batch: 47100, loss: 3.17357, acc: 0.77686, time: Fri Jan 21 17:17:39 2022
epoch 1 training:  24%|██▍       | 47199/196351 [6:43:51<20:48:59,  1.99it/s]global step 47200, epoch: 1, batch: 47200, loss: 0.81915, acc: 0.77706, time: Fri Jan 21 17:18:30 2022
epoch 1 training:  24%|██▍       | 47299/196351 [6:44:43<21:19:12,  1.94it/s]global step 47300, epoch: 1, batch: 47300, loss: 3.67078, acc: 0.77729, time: Fri Jan 21 17:19:22 2022
epoch 1 training:  24%|██▍       | 47399/196351 [6:45:34<21:10:11,  1.95it/s]global step 47400, epoch: 1, batch: 47400, loss: 3.28370, acc: 0.77755, time: Fri Jan 21 17:20:12 2022
epoch 1 training:  24%|██▍       | 47499/196351 [6:46:25<21:18:42,  1.94it/s]global step 47500, epoch: 1, batch: 47500, loss: 3.08115, acc: 0.77779, time: Fri Jan 21 17:21:04 2022
epoch 1 training:  24%|██▍       | 47599/196351 [6:47:16<21:00:48,  1.97it/s]global step 47600, epoch: 1, batch: 47600, loss: 3.65652, acc: 0.77804, time: Fri Jan 21 17:21:55 2022
epoch 1 training:  24%|██▍       | 47699/196351 [6:48:09<22:45:45,  1.81it/s]global step 47700, epoch: 1, batch: 47700, loss: 2.06499, acc: 0.77822, time: Fri Jan 21 17:22:47 2022
epoch 1 training:  24%|██▍       | 47799/196351 [6:48:59<20:33:42,  2.01it/s]global step 47800, epoch: 1, batch: 47800, loss: 2.28856, acc: 0.77838, time: Fri Jan 21 17:23:38 2022
epoch 1 training:  24%|██▍       | 47899/196351 [6:49:50<20:41:04,  1.99it/s]global step 47900, epoch: 1, batch: 47900, loss: 4.18513, acc: 0.77860, time: Fri Jan 21 17:24:29 2022
epoch 1 training:  24%|██▍       | 47999/196351 [6:50:42<21:47:14,  1.89it/s]global step 48000, epoch: 1, batch: 48000, loss: 1.42528, acc: 0.77877, time: Fri Jan 21 17:25:21 2022
epoch 1 training:  24%|██▍       | 48099/196351 [6:51:34<20:35:25,  2.00it/s]global step 48100, epoch: 1, batch: 48100, loss: 4.77320, acc: 0.77897, time: Fri Jan 21 17:26:13 2022
epoch 1 training:  25%|██▍       | 48199/196351 [6:52:25<21:55:45,  1.88it/s]global step 48200, epoch: 1, batch: 48200, loss: 4.03677, acc: 0.77914, time: Fri Jan 21 17:27:03 2022
epoch 1 training:  25%|██▍       | 48299/196351 [6:53:17<21:05:44,  1.95it/s]global step 48300, epoch: 1, batch: 48300, loss: 1.90888, acc: 0.77929, time: Fri Jan 21 17:27:56 2022
epoch 1 training:  25%|██▍       | 48399/196351 [6:54:08<20:48:02,  1.98it/s]global step 48400, epoch: 1, batch: 48400, loss: 3.03792, acc: 0.77947, time: Fri Jan 21 17:28:47 2022
epoch 1 training:  25%|██▍       | 48499/196351 [6:54:59<20:45:26,  1.98it/s]global step 48500, epoch: 1, batch: 48500, loss: 1.73675, acc: 0.77964, time: Fri Jan 21 17:29:38 2022
epoch 1 training:  25%|██▍       | 48599/196351 [6:55:51<20:52:09,  1.97it/s]global step 48600, epoch: 1, batch: 48600, loss: 1.35863, acc: 0.77980, time: Fri Jan 21 17:30:29 2022
epoch 1 training:  25%|██▍       | 48699/196351 [6:56:43<21:13:06,  1.93it/s]global step 48700, epoch: 1, batch: 48700, loss: 2.42748, acc: 0.78002, time: Fri Jan 21 17:31:22 2022
epoch 1 training:  25%|██▍       | 48799/196351 [6:57:34<20:50:10,  1.97it/s]global step 48800, epoch: 1, batch: 48800, loss: 2.72848, acc: 0.78022, time: Fri Jan 21 17:32:13 2022
epoch 1 training:  25%|██▍       | 48899/196351 [6:58:25<20:47:44,  1.97it/s]global step 48900, epoch: 1, batch: 48900, loss: 2.34259, acc: 0.78042, time: Fri Jan 21 17:33:04 2022
epoch 1 training:  25%|██▍       | 48999/196351 [6:59:16<21:45:55,  1.88it/s]global step 49000, epoch: 1, batch: 49000, loss: 4.25888, acc: 0.78065, time: Fri Jan 21 17:33:55 2022
epoch 1 training:  25%|██▌       | 49099/196351 [7:00:08<21:09:52,  1.93it/s]global step 49100, epoch: 1, batch: 49100, loss: 2.58612, acc: 0.78077, time: Fri Jan 21 17:34:47 2022
epoch 1 training:  25%|██▌       | 49199/196351 [7:01:00<20:56:11,  1.95it/s]global step 49200, epoch: 1, batch: 49200, loss: 2.88429, acc: 0.78089, time: Fri Jan 21 17:35:38 2022
epoch 1 training:  25%|██▌       | 49299/196351 [7:01:51<21:14:42,  1.92it/s]global step 49300, epoch: 1, batch: 49300, loss: 1.67195, acc: 0.78106, time: Fri Jan 21 17:36:30 2022
epoch 1 training:  25%|██▌       | 49399/196351 [7:02:43<20:54:03,  1.95it/s]global step 49400, epoch: 1, batch: 49400, loss: 3.57845, acc: 0.78121, time: Fri Jan 21 17:37:22 2022
epoch 1 training:  25%|██▌       | 49499/196351 [7:03:35<20:22:21,  2.00it/s]global step 49500, epoch: 1, batch: 49500, loss: 3.14681, acc: 0.78142, time: Fri Jan 21 17:38:14 2022
epoch 1 training:  25%|██▌       | 49599/196351 [7:04:25<20:47:35,  1.96it/s]global step 49600, epoch: 1, batch: 49600, loss: 3.23265, acc: 0.78157, time: Fri Jan 21 17:39:04 2022
epoch 1 training:  25%|██▌       | 49699/196351 [7:05:17<20:32:17,  1.98it/s]global step 49700, epoch: 1, batch: 49700, loss: 2.27946, acc: 0.78177, time: Fri Jan 21 17:39:55 2022
epoch 1 training:  25%|██▌       | 49799/196351 [7:06:08<21:30:14,  1.89it/s]global step 49800, epoch: 1, batch: 49800, loss: 2.96764, acc: 0.78201, time: Fri Jan 21 17:40:47 2022
epoch 1 training:  25%|██▌       | 49899/196351 [7:07:00<20:55:37,  1.94it/s]global step 49900, epoch: 1, batch: 49900, loss: 1.22644, acc: 0.78220, time: Fri Jan 21 17:41:38 2022
epoch 1 training:  25%|██▌       | 49999/196351 [7:07:51<21:04:15,  1.93it/s]global step 50000, epoch: 1, batch: 50000, loss: 1.82600, acc: 0.78241, time: Fri Jan 21 17:42:30 2022
epoch 1 training:  26%|██▌       | 50099/196351 [7:08:45<21:02:01,  1.93it/s]global step 50100, epoch: 1, batch: 50100, loss: 4.35319, acc: 0.78260, time: Fri Jan 21 17:43:24 2022
epoch 1 training:  26%|██▌       | 50199/196351 [7:09:36<20:50:30,  1.95it/s]global step 50200, epoch: 1, batch: 50200, loss: 2.26434, acc: 0.78286, time: Fri Jan 21 17:44:15 2022
epoch 1 training:  26%|██▌       | 50299/196351 [7:10:28<20:36:31,  1.97it/s]global step 50300, epoch: 1, batch: 50300, loss: 2.45027, acc: 0.78307, time: Fri Jan 21 17:45:06 2022
epoch 1 training:  26%|██▌       | 50399/196351 [7:11:19<20:22:57,  1.99it/s]global step 50400, epoch: 1, batch: 50400, loss: 0.61722, acc: 0.78328, time: Fri Jan 21 17:45:58 2022
epoch 1 training:  26%|██▌       | 50499/196351 [7:12:10<21:03:24,  1.92it/s]global step 50500, epoch: 1, batch: 50500, loss: 3.47034, acc: 0.78346, time: Fri Jan 21 17:46:49 2022
epoch 1 training:  26%|██▌       | 50599/196351 [7:13:02<20:52:31,  1.94it/s]global step 50600, epoch: 1, batch: 50600, loss: 1.51961, acc: 0.78369, time: Fri Jan 21 17:47:40 2022
epoch 1 training:  26%|██▌       | 50699/196351 [7:13:52<20:59:14,  1.93it/s]global step 50700, epoch: 1, batch: 50700, loss: 4.13218, acc: 0.78392, time: Fri Jan 21 17:48:31 2022
epoch 1 training:  26%|██▌       | 50799/196351 [7:14:43<20:34:31,  1.97it/s]global step 50800, epoch: 1, batch: 50800, loss: 3.05401, acc: 0.78414, time: Fri Jan 21 17:49:22 2022
epoch 1 training:  26%|██▌       | 50899/196351 [7:15:34<20:34:02,  1.96it/s]global step 50900, epoch: 1, batch: 50900, loss: 0.97970, acc: 0.78439, time: Fri Jan 21 17:50:13 2022
epoch 1 training:  26%|██▌       | 50999/196351 [7:16:25<20:32:15,  1.97it/s]global step 51000, epoch: 1, batch: 51000, loss: 0.55535, acc: 0.78457, time: Fri Jan 21 17:51:04 2022
epoch 1 training:  26%|██▌       | 51099/196351 [7:17:17<20:51:19,  1.93it/s]global step 51100, epoch: 1, batch: 51100, loss: 1.08015, acc: 0.78483, time: Fri Jan 21 17:51:55 2022
epoch 1 training:  26%|██▌       | 51199/196351 [7:18:09<20:52:33,  1.93it/s]global step 51200, epoch: 1, batch: 51200, loss: 1.69352, acc: 0.78506, time: Fri Jan 21 17:52:48 2022
epoch 1 training:  26%|██▌       | 51299/196351 [7:19:01<20:44:26,  1.94it/s]global step 51300, epoch: 1, batch: 51300, loss: 2.27783, acc: 0.78515, time: Fri Jan 21 17:53:40 2022
epoch 1 training:  26%|██▌       | 51399/196351 [7:19:52<20:26:31,  1.97it/s]global step 51400, epoch: 1, batch: 51400, loss: 2.03774, acc: 0.78542, time: Fri Jan 21 17:54:31 2022
epoch 1 training:  26%|██▌       | 51499/196351 [7:20:43<20:25:13,  1.97it/s]global step 51500, epoch: 1, batch: 51500, loss: 1.43480, acc: 0.78568, time: Fri Jan 21 17:55:21 2022
epoch 1 training:  26%|██▋       | 51599/196351 [7:21:34<20:45:34,  1.94it/s]global step 51600, epoch: 1, batch: 51600, loss: 1.32678, acc: 0.78588, time: Fri Jan 21 17:56:13 2022
epoch 1 training:  26%|██▋       | 51699/196351 [7:22:25<21:11:13,  1.90it/s]global step 51700, epoch: 1, batch: 51700, loss: 2.95253, acc: 0.78608, time: Fri Jan 21 17:57:04 2022
epoch 1 training:  26%|██▋       | 51799/196351 [7:23:16<20:51:32,  1.92it/s]global step 51800, epoch: 1, batch: 51800, loss: 2.68206, acc: 0.78624, time: Fri Jan 21 17:57:55 2022
epoch 1 training:  26%|██▋       | 51899/196351 [7:24:07<20:57:51,  1.91it/s]global step 51900, epoch: 1, batch: 51900, loss: 2.44731, acc: 0.78639, time: Fri Jan 21 17:58:46 2022
epoch 1 training:  26%|██▋       | 51999/196351 [7:24:59<20:39:51,  1.94it/s]global step 52000, epoch: 1, batch: 52000, loss: 2.89590, acc: 0.78660, time: Fri Jan 21 17:59:38 2022
epoch 1 training:  27%|██▋       | 52099/196351 [7:25:50<23:50:40,  1.68it/s]global step 52100, epoch: 1, batch: 52100, loss: 1.64504, acc: 0.78679, time: Fri Jan 21 18:00:29 2022
epoch 1 training:  27%|██▋       | 52199/196351 [7:26:42<20:17:13,  1.97it/s]global step 52200, epoch: 1, batch: 52200, loss: 2.52126, acc: 0.78697, time: Fri Jan 21 18:01:20 2022
epoch 1 training:  27%|██▋       | 52299/196351 [7:27:33<20:37:59,  1.94it/s]global step 52300, epoch: 1, batch: 52300, loss: 3.75513, acc: 0.78709, time: Fri Jan 21 18:02:12 2022
epoch 1 training:  27%|██▋       | 52399/196351 [7:28:24<20:40:29,  1.93it/s]global step 52400, epoch: 1, batch: 52400, loss: 1.39074, acc: 0.78720, time: Fri Jan 21 18:03:03 2022
epoch 1 training:  27%|██▋       | 52499/196351 [7:29:16<20:46:15,  1.92it/s]global step 52500, epoch: 1, batch: 52500, loss: 1.73054, acc: 0.78741, time: Fri Jan 21 18:03:54 2022
epoch 1 training:  27%|██▋       | 52599/196351 [7:30:07<20:04:58,  1.99it/s]global step 52600, epoch: 1, batch: 52600, loss: 2.76727, acc: 0.78752, time: Fri Jan 21 18:04:46 2022
epoch 1 training:  27%|██▋       | 52699/196351 [7:30:59<20:18:02,  1.97it/s]global step 52700, epoch: 1, batch: 52700, loss: 2.99675, acc: 0.78770, time: Fri Jan 21 18:05:38 2022
epoch 1 training:  27%|██▋       | 52799/196351 [7:31:51<20:11:51,  1.97it/s]global step 52800, epoch: 1, batch: 52800, loss: 2.62943, acc: 0.78784, time: Fri Jan 21 18:06:30 2022
epoch 1 training:  27%|██▋       | 52899/196351 [7:32:43<20:17:53,  1.96it/s]global step 52900, epoch: 1, batch: 52900, loss: 2.87062, acc: 0.78799, time: Fri Jan 21 18:07:22 2022
epoch 1 training:  27%|██▋       | 52999/196351 [7:33:36<20:35:16,  1.93it/s]global step 53000, epoch: 1, batch: 53000, loss: 2.41563, acc: 0.78814, time: Fri Jan 21 18:08:15 2022
epoch 1 training:  27%|██▋       | 53099/196351 [7:34:27<20:27:04,  1.95it/s]global step 53100, epoch: 1, batch: 53100, loss: 5.33811, acc: 0.78831, time: Fri Jan 21 18:09:06 2022
epoch 1 training:  27%|██▋       | 53199/196351 [7:35:19<20:31:13,  1.94it/s]global step 53200, epoch: 1, batch: 53200, loss: 3.22056, acc: 0.78846, time: Fri Jan 21 18:09:57 2022
epoch 1 training:  27%|██▋       | 53299/196351 [7:36:10<20:22:16,  1.95it/s]global step 53300, epoch: 1, batch: 53300, loss: 2.15436, acc: 0.78864, time: Fri Jan 21 18:10:49 2022
epoch 1 training:  27%|██▋       | 53399/196351 [7:37:02<20:19:27,  1.95it/s]global step 53400, epoch: 1, batch: 53400, loss: 2.97034, acc: 0.78876, time: Fri Jan 21 18:11:41 2022
epoch 1 training:  27%|██▋       | 53499/196351 [7:37:53<20:06:34,  1.97it/s]global step 53500, epoch: 1, batch: 53500, loss: 1.12678, acc: 0.78893, time: Fri Jan 21 18:12:32 2022
epoch 1 training:  27%|██▋       | 53599/196351 [7:38:44<20:14:13,  1.96it/s]global step 53600, epoch: 1, batch: 53600, loss: 2.43317, acc: 0.78904, time: Fri Jan 21 18:13:23 2022
epoch 1 training:  27%|██▋       | 53699/196351 [7:39:35<20:12:13,  1.96it/s]global step 53700, epoch: 1, batch: 53700, loss: 2.60916, acc: 0.78924, time: Fri Jan 21 18:14:14 2022
epoch 1 training:  27%|██▋       | 53799/196351 [7:40:27<20:27:12,  1.94it/s]global step 53800, epoch: 1, batch: 53800, loss: 0.99369, acc: 0.78942, time: Fri Jan 21 18:15:05 2022
epoch 1 training:  27%|██▋       | 53899/196351 [7:41:18<20:08:05,  1.97it/s]global step 53900, epoch: 1, batch: 53900, loss: 2.36797, acc: 0.78961, time: Fri Jan 21 18:15:57 2022
epoch 1 training:  28%|██▊       | 53999/196351 [7:42:10<21:28:41,  1.84it/s]global step 54000, epoch: 1, batch: 54000, loss: 2.77201, acc: 0.78980, time: Fri Jan 21 18:16:49 2022
epoch 1 training:  28%|██▊       | 54099/196351 [7:43:02<20:28:33,  1.93it/s]global step 54100, epoch: 1, batch: 54100, loss: 2.15441, acc: 0.78996, time: Fri Jan 21 18:17:41 2022
epoch 1 training:  28%|██▊       | 54199/196351 [7:43:53<20:14:12,  1.95it/s]global step 54200, epoch: 1, batch: 54200, loss: 3.09048, acc: 0.79017, time: Fri Jan 21 18:18:32 2022
epoch 1 training:  28%|██▊       | 54299/196351 [7:44:45<21:02:17,  1.88it/s]global step 54300, epoch: 1, batch: 54300, loss: 3.50621, acc: 0.79023, time: Fri Jan 21 18:19:24 2022
epoch 1 training:  28%|██▊       | 54399/196351 [7:45:37<19:53:44,  1.98it/s]global step 54400, epoch: 1, batch: 54400, loss: 0.75825, acc: 0.79039, time: Fri Jan 21 18:20:16 2022
epoch 1 training:  28%|██▊       | 54499/196351 [7:46:30<21:14:14,  1.86it/s]global step 54500, epoch: 1, batch: 54500, loss: 1.71175, acc: 0.79053, time: Fri Jan 21 18:21:08 2022
epoch 1 training:  28%|██▊       | 54599/196351 [7:47:23<20:18:05,  1.94it/s]global step 54600, epoch: 1, batch: 54600, loss: 0.80521, acc: 0.79069, time: Fri Jan 21 18:22:01 2022
epoch 1 training:  28%|██▊       | 54699/196351 [7:48:13<19:45:08,  1.99it/s]global step 54700, epoch: 1, batch: 54700, loss: 1.58175, acc: 0.79090, time: Fri Jan 21 18:22:52 2022
epoch 1 training:  28%|██▊       | 54799/196351 [7:49:05<20:37:00,  1.91it/s]global step 54800, epoch: 1, batch: 54800, loss: 4.89415, acc: 0.79105, time: Fri Jan 21 18:23:44 2022
epoch 1 training:  28%|██▊       | 54899/196351 [7:49:56<19:46:49,  1.99it/s]global step 54900, epoch: 1, batch: 54900, loss: 4.35607, acc: 0.79121, time: Fri Jan 21 18:24:35 2022
epoch 1 training:  28%|██▊       | 54999/196351 [7:50:48<20:03:35,  1.96it/s]global step 55000, epoch: 1, batch: 55000, loss: 2.61787, acc: 0.79140, time: Fri Jan 21 18:25:26 2022
epoch 1 training:  28%|██▊       | 55099/196351 [7:51:39<19:57:13,  1.97it/s]global step 55100, epoch: 1, batch: 55100, loss: 3.96791, acc: 0.79164, time: Fri Jan 21 18:26:17 2022
epoch 1 training:  28%|██▊       | 55199/196351 [7:52:29<20:02:54,  1.96it/s]global step 55200, epoch: 1, batch: 55200, loss: 3.78467, acc: 0.79184, time: Fri Jan 21 18:27:08 2022
epoch 1 training:  28%|██▊       | 55299/196351 [7:53:20<19:56:53,  1.96it/s]global step 55300, epoch: 1, batch: 55300, loss: 3.10687, acc: 0.79202, time: Fri Jan 21 18:27:59 2022
epoch 1 training:  28%|██▊       | 55399/196351 [7:54:11<20:16:03,  1.93it/s]global step 55400, epoch: 1, batch: 55400, loss: 2.68617, acc: 0.79210, time: Fri Jan 21 18:28:50 2022
epoch 1 training:  28%|██▊       | 55499/196351 [7:55:02<19:54:05,  1.97it/s]global step 55500, epoch: 1, batch: 55500, loss: 0.57327, acc: 0.79228, time: Fri Jan 21 18:29:41 2022
epoch 1 training:  28%|██▊       | 55599/196351 [7:55:53<20:11:48,  1.94it/s]global step 55600, epoch: 1, batch: 55600, loss: 3.69037, acc: 0.79249, time: Fri Jan 21 18:30:31 2022
epoch 1 training:  28%|██▊       | 55699/196351 [7:56:44<19:35:40,  1.99it/s]global step 55700, epoch: 1, batch: 55700, loss: 5.54690, acc: 0.79268, time: Fri Jan 21 18:31:22 2022
epoch 1 training:  28%|██▊       | 55799/196351 [7:57:34<19:37:22,  1.99it/s]global step 55800, epoch: 1, batch: 55800, loss: 1.47677, acc: 0.79285, time: Fri Jan 21 18:32:13 2022
epoch 1 training:  28%|██▊       | 55899/196351 [7:58:25<19:48:46,  1.97it/s]global step 55900, epoch: 1, batch: 55900, loss: 3.01743, acc: 0.79307, time: Fri Jan 21 18:33:04 2022
epoch 1 training:  29%|██▊       | 55999/196351 [7:59:16<19:48:30,  1.97it/s]global step 56000, epoch: 1, batch: 56000, loss: 1.66023, acc: 0.79323, time: Fri Jan 21 18:33:55 2022
epoch 1 training:  29%|██▊       | 56099/196351 [8:00:07<20:05:46,  1.94it/s]global step 56100, epoch: 1, batch: 56100, loss: 2.41069, acc: 0.79344, time: Fri Jan 21 18:34:46 2022
epoch 1 training:  29%|██▊       | 56199/196351 [8:00:57<19:53:37,  1.96it/s]global step 56200, epoch: 1, batch: 56200, loss: 2.26539, acc: 0.79355, time: Fri Jan 21 18:35:36 2022
epoch 1 training:  29%|██▊       | 56299/196351 [8:01:48<20:02:29,  1.94it/s]global step 56300, epoch: 1, batch: 56300, loss: 2.22345, acc: 0.79368, time: Fri Jan 21 18:36:27 2022
epoch 1 training:  29%|██▊       | 56399/196351 [8:02:39<19:43:28,  1.97it/s]global step 56400, epoch: 1, batch: 56400, loss: 1.42136, acc: 0.79384, time: Fri Jan 21 18:37:18 2022
epoch 1 training:  29%|██▉       | 56499/196351 [8:03:31<19:38:19,  1.98it/s]global step 56500, epoch: 1, batch: 56500, loss: 1.60061, acc: 0.79393, time: Fri Jan 21 18:38:09 2022
epoch 1 training:  29%|██▉       | 56599/196351 [8:04:22<19:39:05,  1.98it/s]global step 56600, epoch: 1, batch: 56600, loss: 1.46402, acc: 0.79408, time: Fri Jan 21 18:39:00 2022
epoch 1 training:  29%|██▉       | 56699/196351 [8:05:13<19:23:32,  2.00it/s]global step 56700, epoch: 1, batch: 56700, loss: 1.77705, acc: 0.79419, time: Fri Jan 21 18:39:51 2022
epoch 1 training:  29%|██▉       | 56799/196351 [8:06:03<19:28:19,  1.99it/s]global step 56800, epoch: 1, batch: 56800, loss: 3.53266, acc: 0.79430, time: Fri Jan 21 18:40:42 2022
epoch 1 training:  29%|██▉       | 56899/196351 [8:06:54<19:40:47,  1.97it/s]global step 56900, epoch: 1, batch: 56900, loss: 5.13752, acc: 0.79438, time: Fri Jan 21 18:41:33 2022
epoch 1 training:  29%|██▉       | 56999/196351 [8:07:45<19:35:56,  1.98it/s]global step 57000, epoch: 1, batch: 57000, loss: 3.92433, acc: 0.79450, time: Fri Jan 21 18:42:24 2022
epoch 1 training:  29%|██▉       | 57099/196351 [8:08:36<19:35:43,  1.97it/s]global step 57100, epoch: 1, batch: 57100, loss: 0.98757, acc: 0.79468, time: Fri Jan 21 18:43:14 2022
epoch 1 training:  29%|██▉       | 57199/196351 [8:09:26<19:28:47,  1.98it/s]global step 57200, epoch: 1, batch: 57200, loss: 4.20830, acc: 0.79483, time: Fri Jan 21 18:44:05 2022
epoch 1 training:  29%|██▉       | 57299/196351 [8:10:17<19:51:34,  1.94it/s]global step 57300, epoch: 1, batch: 57300, loss: 3.19252, acc: 0.79490, time: Fri Jan 21 18:44:56 2022
epoch 1 training:  29%|██▉       | 57399/196351 [8:11:08<19:41:37,  1.96it/s]global step 57400, epoch: 1, batch: 57400, loss: 3.68202, acc: 0.79503, time: Fri Jan 21 18:45:47 2022
epoch 1 training:  29%|██▉       | 57499/196351 [8:11:59<19:44:58,  1.95it/s]global step 57500, epoch: 1, batch: 57500, loss: 3.70261, acc: 0.79517, time: Fri Jan 21 18:46:38 2022
epoch 1 training:  29%|██▉       | 57599/196351 [8:12:50<19:34:52,  1.97it/s]global step 57600, epoch: 1, batch: 57600, loss: 2.95100, acc: 0.79536, time: Fri Jan 21 18:47:29 2022
epoch 1 training:  29%|██▉       | 57699/196351 [8:13:41<19:44:51,  1.95it/s]global step 57700, epoch: 1, batch: 57700, loss: 3.54263, acc: 0.79543, time: Fri Jan 21 18:48:20 2022
epoch 1 training:  29%|██▉       | 57799/196351 [8:14:32<19:23:50,  1.98it/s]global step 57800, epoch: 1, batch: 57800, loss: 2.52590, acc: 0.79565, time: Fri Jan 21 18:49:11 2022
epoch 1 training:  29%|██▉       | 57899/196351 [8:15:23<19:38:10,  1.96it/s]global step 57900, epoch: 1, batch: 57900, loss: 2.66705, acc: 0.79587, time: Fri Jan 21 18:50:02 2022
epoch 1 training:  30%|██▉       | 57999/196351 [8:16:14<19:37:21,  1.96it/s]global step 58000, epoch: 1, batch: 58000, loss: 4.34513, acc: 0.79603, time: Fri Jan 21 18:50:53 2022
epoch 1 training:  30%|██▉       | 58099/196351 [8:17:05<19:23:57,  1.98it/s]global step 58100, epoch: 1, batch: 58100, loss: 4.35450, acc: 0.79614, time: Fri Jan 21 18:51:44 2022
epoch 1 training:  30%|██▉       | 58199/196351 [8:17:56<19:18:52,  1.99it/s]global step 58200, epoch: 1, batch: 58200, loss: 1.05147, acc: 0.79630, time: Fri Jan 21 18:52:34 2022
epoch 1 training:  30%|██▉       | 58299/196351 [8:18:46<19:35:53,  1.96it/s]global step 58300, epoch: 1, batch: 58300, loss: 1.64422, acc: 0.79636, time: Fri Jan 21 18:53:25 2022
epoch 1 training:  30%|██▉       | 58399/196351 [8:19:37<19:27:16,  1.97it/s]global step 58400, epoch: 1, batch: 58400, loss: 3.32209, acc: 0.79650, time: Fri Jan 21 18:54:16 2022
epoch 1 training:  30%|██▉       | 58499/196351 [8:20:28<19:27:13,  1.97it/s]global step 58500, epoch: 1, batch: 58500, loss: 1.45074, acc: 0.79664, time: Fri Jan 21 18:55:07 2022
epoch 1 training:  30%|██▉       | 58599/196351 [8:21:19<19:21:41,  1.98it/s]global step 58600, epoch: 1, batch: 58600, loss: 1.58484, acc: 0.79683, time: Fri Jan 21 18:55:58 2022
epoch 1 training:  30%|██▉       | 58699/196351 [8:22:10<19:28:07,  1.96it/s]global step 58700, epoch: 1, batch: 58700, loss: 3.03565, acc: 0.79700, time: Fri Jan 21 18:56:49 2022
epoch 1 training:  30%|██▉       | 58799/196351 [8:23:01<19:22:52,  1.97it/s]global step 58800, epoch: 1, batch: 58800, loss: 3.03883, acc: 0.79714, time: Fri Jan 21 18:57:40 2022
epoch 1 training:  30%|██▉       | 58899/196351 [8:23:53<19:13:10,  1.99it/s]global step 58900, epoch: 1, batch: 58900, loss: 3.57448, acc: 0.79728, time: Fri Jan 21 18:58:32 2022
epoch 1 training:  30%|███       | 58999/196351 [8:24:44<19:34:28,  1.95it/s]global step 59000, epoch: 1, batch: 59000, loss: 1.45089, acc: 0.79747, time: Fri Jan 21 18:59:23 2022
epoch 1 training:  30%|███       | 59099/196351 [8:25:35<19:30:13,  1.95it/s]global step 59100, epoch: 1, batch: 59100, loss: 0.95811, acc: 0.79763, time: Fri Jan 21 19:00:14 2022
epoch 1 training:  30%|███       | 59199/196351 [8:26:26<19:29:34,  1.95it/s]global step 59200, epoch: 1, batch: 59200, loss: 0.66671, acc: 0.79780, time: Fri Jan 21 19:01:05 2022
epoch 1 training:  30%|███       | 59299/196351 [8:27:17<19:03:19,  2.00it/s]global step 59300, epoch: 1, batch: 59300, loss: 3.28526, acc: 0.79790, time: Fri Jan 21 19:01:56 2022
epoch 1 training:  30%|███       | 59399/196351 [8:28:08<19:29:48,  1.95it/s]global step 59400, epoch: 1, batch: 59400, loss: 3.76225, acc: 0.79805, time: Fri Jan 21 19:02:46 2022
epoch 1 training:  30%|███       | 59499/196351 [8:28:59<19:26:26,  1.96it/s]global step 59500, epoch: 1, batch: 59500, loss: 1.22045, acc: 0.79814, time: Fri Jan 21 19:03:38 2022
epoch 1 training:  30%|███       | 59599/196351 [8:29:50<19:18:45,  1.97it/s]global step 59600, epoch: 1, batch: 59600, loss: 2.31387, acc: 0.79831, time: Fri Jan 21 19:04:28 2022
epoch 1 training:  30%|███       | 59699/196351 [8:30:40<19:43:04,  1.93it/s]global step 59700, epoch: 1, batch: 59700, loss: 0.92905, acc: 0.79845, time: Fri Jan 21 19:05:19 2022
epoch 1 training:  30%|███       | 59799/196351 [8:31:31<19:24:44,  1.95it/s]global step 59800, epoch: 1, batch: 59800, loss: 2.50184, acc: 0.79860, time: Fri Jan 21 19:06:10 2022
epoch 1 training:  31%|███       | 59899/196351 [8:32:23<19:07:07,  1.98it/s]global step 59900, epoch: 1, batch: 59900, loss: 2.16322, acc: 0.79880, time: Fri Jan 21 19:07:02 2022
epoch 1 training:  31%|███       | 59999/196351 [8:33:14<19:14:29,  1.97it/s]global step 60000, epoch: 1, batch: 60000, loss: 1.85068, acc: 0.79892, time: Fri Jan 21 19:07:52 2022
epoch 1 training:  31%|███       | 60099/196351 [8:34:05<19:12:46,  1.97it/s]global step 60100, epoch: 1, batch: 60100, loss: 1.32414, acc: 0.79903, time: Fri Jan 21 19:08:43 2022
epoch 1 training:  31%|███       | 60199/196351 [8:34:56<19:15:31,  1.96it/s]global step 60200, epoch: 1, batch: 60200, loss: 1.67929, acc: 0.79914, time: Fri Jan 21 19:09:34 2022
epoch 1 training:  31%|███       | 60299/196351 [8:35:47<18:56:09,  2.00it/s]global step 60300, epoch: 1, batch: 60300, loss: 2.81651, acc: 0.79931, time: Fri Jan 21 19:10:25 2022
epoch 1 training:  31%|███       | 60399/196351 [8:36:38<19:06:40,  1.98it/s]global step 60400, epoch: 1, batch: 60400, loss: 4.02384, acc: 0.79943, time: Fri Jan 21 19:11:17 2022
epoch 1 training:  31%|███       | 60499/196351 [8:37:29<19:04:05,  1.98it/s]global step 60500, epoch: 1, batch: 60500, loss: 2.74167, acc: 0.79951, time: Fri Jan 21 19:12:08 2022
epoch 1 training:  31%|███       | 60599/196351 [8:38:20<19:15:42,  1.96it/s]global step 60600, epoch: 1, batch: 60600, loss: 2.89323, acc: 0.79970, time: Fri Jan 21 19:12:59 2022
epoch 1 training:  31%|███       | 60699/196351 [8:39:11<19:29:27,  1.93it/s]global step 60700, epoch: 1, batch: 60700, loss: 1.24300, acc: 0.79986, time: Fri Jan 21 19:13:50 2022
epoch 1 training:  31%|███       | 60799/196351 [8:40:02<19:13:23,  1.96it/s]global step 60800, epoch: 1, batch: 60800, loss: 3.05405, acc: 0.80001, time: Fri Jan 21 19:14:41 2022
epoch 1 training:  31%|███       | 60899/196351 [8:40:53<19:22:44,  1.94it/s]global step 60900, epoch: 1, batch: 60900, loss: 2.90289, acc: 0.80016, time: Fri Jan 21 19:15:32 2022
epoch 1 training:  31%|███       | 60999/196351 [8:41:44<18:53:32,  1.99it/s]global step 61000, epoch: 1, batch: 61000, loss: 1.17544, acc: 0.80036, time: Fri Jan 21 19:16:23 2022
epoch 1 training:  31%|███       | 61099/196351 [8:42:35<19:07:59,  1.96it/s]global step 61100, epoch: 1, batch: 61100, loss: 3.78058, acc: 0.80044, time: Fri Jan 21 19:17:13 2022
epoch 1 training:  31%|███       | 61199/196351 [8:43:25<19:14:35,  1.95it/s]global step 61200, epoch: 1, batch: 61200, loss: 2.04836, acc: 0.80061, time: Fri Jan 21 19:18:04 2022
epoch 1 training:  31%|███       | 61299/196351 [8:44:16<19:04:01,  1.97it/s]global step 61300, epoch: 1, batch: 61300, loss: 2.37116, acc: 0.80072, time: Fri Jan 21 19:18:55 2022
epoch 1 training:  31%|███▏      | 61399/196351 [8:45:07<19:20:37,  1.94it/s]global step 61400, epoch: 1, batch: 61400, loss: 2.06426, acc: 0.80085, time: Fri Jan 21 19:19:46 2022
epoch 1 training:  31%|███▏      | 61499/196351 [8:45:58<19:02:16,  1.97it/s]global step 61500, epoch: 1, batch: 61500, loss: 3.97552, acc: 0.80093, time: Fri Jan 21 19:20:36 2022
epoch 1 training:  31%|███▏      | 61599/196351 [8:46:49<18:55:38,  1.98it/s]global step 61600, epoch: 1, batch: 61600, loss: 1.75997, acc: 0.80104, time: Fri Jan 21 19:21:27 2022
epoch 1 training:  31%|███▏      | 61699/196351 [8:47:40<19:10:29,  1.95it/s]global step 61700, epoch: 1, batch: 61700, loss: 0.17442, acc: 0.80118, time: Fri Jan 21 19:22:19 2022
epoch 1 training:  31%|███▏      | 61799/196351 [8:48:31<19:02:12,  1.96it/s]global step 61800, epoch: 1, batch: 61800, loss: 1.70750, acc: 0.80137, time: Fri Jan 21 19:23:10 2022
epoch 1 training:  32%|███▏      | 61899/196351 [8:49:22<18:57:48,  1.97it/s]global step 61900, epoch: 1, batch: 61900, loss: 2.90716, acc: 0.80151, time: Fri Jan 21 19:24:01 2022
epoch 1 training:  32%|███▏      | 61999/196351 [8:50:13<18:54:03,  1.97it/s]global step 62000, epoch: 1, batch: 62000, loss: 4.09971, acc: 0.80164, time: Fri Jan 21 19:24:52 2022
epoch 1 training:  32%|███▏      | 62099/196351 [8:51:03<18:53:50,  1.97it/s]global step 62100, epoch: 1, batch: 62100, loss: 1.42452, acc: 0.80178, time: Fri Jan 21 19:25:42 2022
epoch 1 training:  32%|███▏      | 62199/196351 [8:51:54<18:35:48,  2.00it/s]global step 62200, epoch: 1, batch: 62200, loss: 3.45520, acc: 0.80189, time: Fri Jan 21 19:26:33 2022
epoch 1 training:  32%|███▏      | 62299/196351 [8:52:45<19:05:19,  1.95it/s]global step 62300, epoch: 1, batch: 62300, loss: 1.69269, acc: 0.80204, time: Fri Jan 21 19:27:23 2022
epoch 1 training:  32%|███▏      | 62399/196351 [8:53:35<19:12:10,  1.94it/s]global step 62400, epoch: 1, batch: 62400, loss: 1.52964, acc: 0.80210, time: Fri Jan 21 19:28:14 2022
epoch 1 training:  32%|███▏      | 62499/196351 [8:54:26<19:39:23,  1.89it/s]global step 62500, epoch: 1, batch: 62500, loss: 3.02067, acc: 0.80220, time: Fri Jan 21 19:29:05 2022
epoch 1 training:  32%|███▏      | 62599/196351 [8:55:18<18:39:40,  1.99it/s]global step 62600, epoch: 1, batch: 62600, loss: 2.46905, acc: 0.80238, time: Fri Jan 21 19:29:56 2022
epoch 1 training:  32%|███▏      | 62699/196351 [8:56:09<18:50:24,  1.97it/s]global step 62700, epoch: 1, batch: 62700, loss: 2.53102, acc: 0.80255, time: Fri Jan 21 19:30:47 2022
epoch 1 training:  32%|███▏      | 62799/196351 [8:57:00<19:02:42,  1.95it/s]global step 62800, epoch: 1, batch: 62800, loss: 4.57741, acc: 0.80271, time: Fri Jan 21 19:31:39 2022
epoch 1 training:  32%|███▏      | 62899/196351 [8:57:52<18:54:37,  1.96it/s]global step 62900, epoch: 1, batch: 62900, loss: 1.90375, acc: 0.80278, time: Fri Jan 21 19:32:30 2022
epoch 1 training:  32%|███▏      | 62999/196351 [8:58:43<19:00:11,  1.95it/s]global step 63000, epoch: 1, batch: 63000, loss: 2.24444, acc: 0.80296, time: Fri Jan 21 19:33:22 2022
epoch 1 training:  32%|███▏      | 63099/196351 [8:59:34<18:47:05,  1.97it/s]global step 63100, epoch: 1, batch: 63100, loss: 1.31237, acc: 0.80311, time: Fri Jan 21 19:34:13 2022
epoch 1 training:  32%|███▏      | 63199/196351 [9:00:25<18:42:02,  1.98it/s]global step 63200, epoch: 1, batch: 63200, loss: 2.85717, acc: 0.80318, time: Fri Jan 21 19:35:03 2022
epoch 1 training:  32%|███▏      | 63299/196351 [9:01:16<19:12:33,  1.92it/s]global step 63300, epoch: 1, batch: 63300, loss: 1.79152, acc: 0.80329, time: Fri Jan 21 19:35:55 2022
epoch 1 training:  32%|███▏      | 63399/196351 [9:02:07<18:33:57,  1.99it/s]global step 63400, epoch: 1, batch: 63400, loss: 2.05452, acc: 0.80334, time: Fri Jan 21 19:36:46 2022
epoch 1 training:  32%|███▏      | 63499/196351 [9:02:58<18:50:45,  1.96it/s]global step 63500, epoch: 1, batch: 63500, loss: 3.02732, acc: 0.80349, time: Fri Jan 21 19:37:37 2022
epoch 1 training:  32%|███▏      | 63599/196351 [9:03:48<18:47:27,  1.96it/s]global step 63600, epoch: 1, batch: 63600, loss: 4.44912, acc: 0.80356, time: Fri Jan 21 19:38:27 2022
epoch 1 training:  32%|███▏      | 63699/196351 [9:04:39<19:27:15,  1.89it/s]global step 63700, epoch: 1, batch: 63700, loss: 3.04905, acc: 0.80365, time: Fri Jan 21 19:39:18 2022
epoch 1 training:  32%|███▏      | 63799/196351 [9:05:30<18:42:48,  1.97it/s]global step 63800, epoch: 1, batch: 63800, loss: 1.11489, acc: 0.80375, time: Fri Jan 21 19:40:09 2022
epoch 1 training:  33%|███▎      | 63899/196351 [9:06:21<18:42:31,  1.97it/s]global step 63900, epoch: 1, batch: 63900, loss: 2.49494, acc: 0.80391, time: Fri Jan 21 19:41:00 2022
epoch 1 training:  33%|███▎      | 63999/196351 [9:07:12<18:24:22,  2.00it/s]global step 64000, epoch: 1, batch: 64000, loss: 2.22789, acc: 0.80407, time: Fri Jan 21 19:41:51 2022
epoch 1 training:  33%|███▎      | 64099/196351 [9:08:03<18:48:02,  1.95it/s]global step 64100, epoch: 1, batch: 64100, loss: 3.26111, acc: 0.80417, time: Fri Jan 21 19:42:42 2022
epoch 1 training:  33%|███▎      | 64199/196351 [9:08:54<18:39:50,  1.97it/s]global step 64200, epoch: 1, batch: 64200, loss: 3.05178, acc: 0.80433, time: Fri Jan 21 19:43:33 2022
epoch 1 training:  33%|███▎      | 64299/196351 [9:09:45<18:52:32,  1.94it/s]global step 64300, epoch: 1, batch: 64300, loss: 0.10458, acc: 0.80451, time: Fri Jan 21 19:44:24 2022
epoch 1 training:  33%|███▎      | 64399/196351 [9:10:36<18:53:20,  1.94it/s]global step 64400, epoch: 1, batch: 64400, loss: 3.51566, acc: 0.80465, time: Fri Jan 21 19:45:15 2022
epoch 1 training:  33%|███▎      | 64499/196351 [9:11:27<18:35:40,  1.97it/s]global step 64500, epoch: 1, batch: 64500, loss: 1.45471, acc: 0.80475, time: Fri Jan 21 19:46:06 2022
epoch 1 training:  33%|███▎      | 64599/196351 [9:12:18<18:20:06,  2.00it/s]global step 64600, epoch: 1, batch: 64600, loss: 1.34050, acc: 0.80478, time: Fri Jan 21 19:46:57 2022
epoch 1 training:  33%|███▎      | 64699/196351 [9:13:10<18:26:39,  1.98it/s]global step 64700, epoch: 1, batch: 64700, loss: 1.54839, acc: 0.80486, time: Fri Jan 21 19:47:48 2022
epoch 1 training:  33%|███▎      | 64799/196351 [9:14:01<19:00:01,  1.92it/s]global step 64800, epoch: 1, batch: 64800, loss: 3.15354, acc: 0.80496, time: Fri Jan 21 19:48:39 2022
epoch 1 training:  33%|███▎      | 64899/196351 [9:14:52<18:22:48,  1.99it/s]global step 64900, epoch: 1, batch: 64900, loss: 3.94193, acc: 0.80507, time: Fri Jan 21 19:49:30 2022
epoch 1 training:  33%|███▎      | 64999/196351 [9:15:42<18:32:24,  1.97it/s]global step 65000, epoch: 1, batch: 65000, loss: 3.05871, acc: 0.80524, time: Fri Jan 21 19:50:21 2022
epoch 1 training:  33%|███▎      | 65099/196351 [9:16:33<18:24:41,  1.98it/s]global step 65100, epoch: 1, batch: 65100, loss: 2.78846, acc: 0.80536, time: Fri Jan 21 19:51:12 2022
epoch 1 training:  33%|███▎      | 65199/196351 [9:17:24<18:27:48,  1.97it/s]global step 65200, epoch: 1, batch: 65200, loss: 3.71311, acc: 0.80541, time: Fri Jan 21 19:52:03 2022
epoch 1 training:  33%|███▎      | 65299/196351 [9:18:15<18:30:58,  1.97it/s]global step 65300, epoch: 1, batch: 65300, loss: 4.47804, acc: 0.80559, time: Fri Jan 21 19:52:53 2022
epoch 1 training:  33%|███▎      | 65399/196351 [9:19:06<18:30:11,  1.97it/s]global step 65400, epoch: 1, batch: 65400, loss: 1.46341, acc: 0.80570, time: Fri Jan 21 19:53:44 2022
epoch 1 training:  33%|███▎      | 65499/196351 [9:19:56<18:33:41,  1.96it/s]global step 65500, epoch: 1, batch: 65500, loss: 3.44163, acc: 0.80582, time: Fri Jan 21 19:54:35 2022
epoch 1 training:  33%|███▎      | 65599/196351 [9:20:48<18:16:13,  1.99it/s]global step 65600, epoch: 1, batch: 65600, loss: 1.47534, acc: 0.80595, time: Fri Jan 21 19:55:26 2022
epoch 1 training:  33%|███▎      | 65699/196351 [9:21:39<18:29:38,  1.96it/s]global step 65700, epoch: 1, batch: 65700, loss: 1.62481, acc: 0.80609, time: Fri Jan 21 19:56:18 2022
epoch 1 training:  34%|███▎      | 65799/196351 [9:22:30<18:29:58,  1.96it/s]global step 65800, epoch: 1, batch: 65800, loss: 4.60126, acc: 0.80622, time: Fri Jan 21 19:57:09 2022
epoch 1 training:  34%|███▎      | 65899/196351 [9:23:21<18:31:14,  1.96it/s]global step 65900, epoch: 1, batch: 65900, loss: 2.47410, acc: 0.80628, time: Fri Jan 21 19:58:00 2022
epoch 1 training:  34%|███▎      | 65999/196351 [9:24:12<18:15:45,  1.98it/s]global step 66000, epoch: 1, batch: 66000, loss: 4.71538, acc: 0.80640, time: Fri Jan 21 19:58:51 2022
epoch 1 training:  34%|███▎      | 66099/196351 [9:25:03<18:30:20,  1.96it/s]global step 66100, epoch: 1, batch: 66100, loss: 1.13193, acc: 0.80657, time: Fri Jan 21 19:59:42 2022
epoch 1 training:  34%|███▎      | 66199/196351 [9:25:54<18:22:14,  1.97it/s]global step 66200, epoch: 1, batch: 66200, loss: 4.13120, acc: 0.80672, time: Fri Jan 21 20:00:33 2022
epoch 1 training:  34%|███▍      | 66299/196351 [9:26:45<18:28:07,  1.96it/s]global step 66300, epoch: 1, batch: 66300, loss: 3.66596, acc: 0.80676, time: Fri Jan 21 20:01:24 2022
epoch 1 training:  34%|███▍      | 66399/196351 [9:27:36<18:26:35,  1.96it/s]global step 66400, epoch: 1, batch: 66400, loss: 3.39446, acc: 0.80691, time: Fri Jan 21 20:02:15 2022
epoch 1 training:  34%|███▍      | 66499/196351 [9:28:27<18:08:33,  1.99it/s]global step 66500, epoch: 1, batch: 66500, loss: 1.70737, acc: 0.80709, time: Fri Jan 21 20:03:05 2022
epoch 1 training:  34%|███▍      | 66599/196351 [9:29:17<18:07:56,  1.99it/s]global step 66600, epoch: 1, batch: 66600, loss: 2.68413, acc: 0.80724, time: Fri Jan 21 20:03:56 2022
epoch 1 training:  34%|███▍      | 66699/196351 [9:30:08<18:20:19,  1.96it/s]global step 66700, epoch: 1, batch: 66700, loss: 3.07566, acc: 0.80730, time: Fri Jan 21 20:04:47 2022
epoch 1 training:  34%|███▍      | 66799/196351 [9:30:59<18:24:38,  1.95it/s]global step 66800, epoch: 1, batch: 66800, loss: 3.59298, acc: 0.80737, time: Fri Jan 21 20:05:38 2022
epoch 1 training:  34%|███▍      | 66899/196351 [9:31:50<18:15:15,  1.97it/s]global step 66900, epoch: 1, batch: 66900, loss: 2.41916, acc: 0.80744, time: Fri Jan 21 20:06:29 2022
epoch 1 training:  34%|███▍      | 66999/196351 [9:32:40<18:00:43,  1.99it/s]global step 67000, epoch: 1, batch: 67000, loss: 1.57245, acc: 0.80756, time: Fri Jan 21 20:07:19 2022
epoch 1 training:  34%|███▍      | 67099/196351 [9:33:31<18:06:36,  1.98it/s]global step 67100, epoch: 1, batch: 67100, loss: 2.96724, acc: 0.80768, time: Fri Jan 21 20:08:10 2022
epoch 1 training:  34%|███▍      | 67199/196351 [9:34:22<18:24:58,  1.95it/s]global step 67200, epoch: 1, batch: 67200, loss: 4.69553, acc: 0.80774, time: Fri Jan 21 20:09:01 2022
epoch 1 training:  34%|███▍      | 67299/196351 [9:35:13<18:12:05,  1.97it/s]global step 67300, epoch: 1, batch: 67300, loss: 1.01782, acc: 0.80785, time: Fri Jan 21 20:09:52 2022
epoch 1 training:  34%|███▍      | 67399/196351 [9:36:04<18:13:58,  1.96it/s]global step 67400, epoch: 1, batch: 67400, loss: 1.05505, acc: 0.80797, time: Fri Jan 21 20:10:43 2022
epoch 1 training:  34%|███▍      | 67499/196351 [9:36:55<18:17:07,  1.96it/s]global step 67500, epoch: 1, batch: 67500, loss: 1.99109, acc: 0.80808, time: Fri Jan 21 20:11:34 2022
epoch 1 training:  34%|███▍      | 67599/196351 [9:37:46<18:17:25,  1.96it/s]global step 67600, epoch: 1, batch: 67600, loss: 2.55770, acc: 0.80819, time: Fri Jan 21 20:12:25 2022
epoch 1 training:  34%|███▍      | 67699/196351 [9:38:37<18:10:50,  1.97it/s]global step 67700, epoch: 1, batch: 67700, loss: 1.93080, acc: 0.80832, time: Fri Jan 21 20:13:16 2022
epoch 1 training:  35%|███▍      | 67799/196351 [9:39:28<18:02:59,  1.98it/s]global step 67800, epoch: 1, batch: 67800, loss: 1.08298, acc: 0.80843, time: Fri Jan 21 20:14:07 2022
epoch 1 training:  35%|███▍      | 67899/196351 [9:40:19<18:09:28,  1.97it/s]global step 67900, epoch: 1, batch: 67900, loss: 0.60650, acc: 0.80857, time: Fri Jan 21 20:14:57 2022
epoch 1 training:  35%|███▍      | 67999/196351 [9:41:09<17:49:37,  2.00it/s]global step 68000, epoch: 1, batch: 68000, loss: 1.08845, acc: 0.80871, time: Fri Jan 21 20:15:48 2022
epoch 1 training:  35%|███▍      | 68099/196351 [9:42:00<17:47:21,  2.00it/s]global step 68100, epoch: 1, batch: 68100, loss: 0.66584, acc: 0.80881, time: Fri Jan 21 20:16:38 2022
epoch 1 training:  35%|███▍      | 68199/196351 [9:42:50<18:08:42,  1.96it/s]global step 68200, epoch: 1, batch: 68200, loss: 1.58268, acc: 0.80894, time: Fri Jan 21 20:17:29 2022
epoch 1 training:  35%|███▍      | 68299/196351 [9:43:41<17:53:49,  1.99it/s]global step 68300, epoch: 1, batch: 68300, loss: 2.85663, acc: 0.80903, time: Fri Jan 21 20:18:20 2022
epoch 1 training:  35%|███▍      | 68399/196351 [9:44:32<17:49:28,  1.99it/s]global step 68400, epoch: 1, batch: 68400, loss: 2.60682, acc: 0.80910, time: Fri Jan 21 20:19:11 2022
epoch 1 training:  35%|███▍      | 68499/196351 [9:45:23<17:58:45,  1.98it/s]global step 68500, epoch: 1, batch: 68500, loss: 1.65473, acc: 0.80924, time: Fri Jan 21 20:20:01 2022
epoch 1 training:  35%|███▍      | 68599/196351 [9:46:13<18:00:55,  1.97it/s]global step 68600, epoch: 1, batch: 68600, loss: 1.58967, acc: 0.80937, time: Fri Jan 21 20:20:52 2022
epoch 1 training:  35%|███▍      | 68699/196351 [9:47:04<17:58:46,  1.97it/s]global step 68700, epoch: 1, batch: 68700, loss: 3.32191, acc: 0.80946, time: Fri Jan 21 20:21:43 2022
epoch 1 training:  35%|███▌      | 68799/196351 [9:47:55<17:44:25,  2.00it/s]global step 68800, epoch: 1, batch: 68800, loss: 2.52200, acc: 0.80951, time: Fri Jan 21 20:22:34 2022
epoch 1 training:  35%|███▌      | 68899/196351 [9:48:46<18:05:16,  1.96it/s]global step 68900, epoch: 1, batch: 68900, loss: 2.49620, acc: 0.80962, time: Fri Jan 21 20:23:25 2022
epoch 1 training:  35%|███▌      | 68999/196351 [9:49:37<18:00:57,  1.96it/s]global step 69000, epoch: 1, batch: 69000, loss: 4.02412, acc: 0.80969, time: Fri Jan 21 20:24:16 2022
epoch 1 training:  35%|███▌      | 69099/196351 [9:50:28<17:52:55,  1.98it/s]global step 69100, epoch: 1, batch: 69100, loss: 0.50880, acc: 0.80980, time: Fri Jan 21 20:25:06 2022
epoch 1 training:  35%|███▌      | 69199/196351 [9:51:19<18:01:09,  1.96it/s]global step 69200, epoch: 1, batch: 69200, loss: 0.77595, acc: 0.80991, time: Fri Jan 21 20:25:57 2022
epoch 1 training:  35%|███▌      | 69299/196351 [9:52:09<17:58:01,  1.96it/s]global step 69300, epoch: 1, batch: 69300, loss: 2.10094, acc: 0.81006, time: Fri Jan 21 20:26:48 2022
epoch 1 training:  35%|███▌      | 69399/196351 [9:53:00<17:44:11,  1.99it/s]global step 69400, epoch: 1, batch: 69400, loss: 3.18620, acc: 0.81020, time: Fri Jan 21 20:27:39 2022
epoch 1 training:  35%|███▌      | 69499/196351 [9:53:51<17:40:40,  1.99it/s]global step 69500, epoch: 1, batch: 69500, loss: 1.08727, acc: 0.81036, time: Fri Jan 21 20:28:29 2022
epoch 1 training:  35%|███▌      | 69599/196351 [9:54:41<17:54:24,  1.97it/s]global step 69600, epoch: 1, batch: 69600, loss: 2.76857, acc: 0.81055, time: Fri Jan 21 20:29:20 2022
epoch 1 training:  35%|███▌      | 69699/196351 [9:55:32<17:46:23,  1.98it/s]global step 69700, epoch: 1, batch: 69700, loss: 2.15052, acc: 0.81066, time: Fri Jan 21 20:30:11 2022
epoch 1 training:  36%|███▌      | 69799/196351 [9:56:22<17:41:28,  1.99it/s]global step 69800, epoch: 1, batch: 69800, loss: 0.72167, acc: 0.81076, time: Fri Jan 21 20:31:01 2022
epoch 1 training:  36%|███▌      | 69899/196351 [9:57:13<17:36:44,  1.99it/s]global step 69900, epoch: 1, batch: 69900, loss: 0.70944, acc: 0.81089, time: Fri Jan 21 20:31:52 2022
epoch 1 training:  36%|███▌      | 69999/196351 [9:58:04<18:26:07,  1.90it/s]global step 70000, epoch: 1, batch: 70000, loss: 1.20624, acc: 0.81100, time: Fri Jan 21 20:32:43 2022
epoch 1 training:  36%|███▌      | 70099/196351 [9:58:55<17:39:39,  1.99it/s]global step 70100, epoch: 1, batch: 70100, loss: 3.71335, acc: 0.81109, time: Fri Jan 21 20:33:34 2022
epoch 1 training:  36%|███▌      | 70199/196351 [9:59:46<17:47:59,  1.97it/s]global step 70200, epoch: 1, batch: 70200, loss: 0.97911, acc: 0.81115, time: Fri Jan 21 20:34:25 2022
epoch 1 training:  36%|███▌      | 70299/196351 [10:00:37<17:56:27,  1.95it/s]global step 70300, epoch: 1, batch: 70300, loss: 4.05652, acc: 0.81124, time: Fri Jan 21 20:35:16 2022
epoch 1 training:  36%|███▌      | 70399/196351 [10:01:28<17:55:56,  1.95it/s]global step 70400, epoch: 1, batch: 70400, loss: 0.54945, acc: 0.81134, time: Fri Jan 21 20:36:07 2022
epoch 1 training:  36%|███▌      | 70499/196351 [10:02:19<17:50:47,  1.96it/s]global step 70500, epoch: 1, batch: 70500, loss: 2.97010, acc: 0.81147, time: Fri Jan 21 20:36:58 2022
epoch 1 training:  36%|███▌      | 70599/196351 [10:03:10<17:51:56,  1.96it/s]global step 70600, epoch: 1, batch: 70600, loss: 1.03242, acc: 0.81153, time: Fri Jan 21 20:37:49 2022
epoch 1 training:  36%|███▌      | 70699/196351 [10:04:01<17:46:23,  1.96it/s]global step 70700, epoch: 1, batch: 70700, loss: 1.66662, acc: 0.81161, time: Fri Jan 21 20:38:40 2022
epoch 1 training:  36%|███▌      | 70799/196351 [10:04:52<17:39:03,  1.98it/s]global step 70800, epoch: 1, batch: 70800, loss: 0.92450, acc: 0.81176, time: Fri Jan 21 20:39:31 2022
epoch 1 training:  36%|███▌      | 70899/196351 [10:05:43<17:35:28,  1.98it/s]global step 70900, epoch: 1, batch: 70900, loss: 4.81141, acc: 0.81185, time: Fri Jan 21 20:40:21 2022
epoch 1 training:  36%|███▌      | 70999/196351 [10:06:34<17:22:38,  2.00it/s]global step 71000, epoch: 1, batch: 71000, loss: 2.32984, acc: 0.81195, time: Fri Jan 21 20:41:12 2022
epoch 1 training:  36%|███▌      | 71099/196351 [10:07:24<17:48:10,  1.95it/s]global step 71100, epoch: 1, batch: 71100, loss: 1.40445, acc: 0.81205, time: Fri Jan 21 20:42:03 2022
epoch 1 training:  36%|███▋      | 71199/196351 [10:08:15<17:30:00,  1.99it/s]global step 71200, epoch: 1, batch: 71200, loss: 4.75845, acc: 0.81221, time: Fri Jan 21 20:42:53 2022
epoch 1 training:  36%|███▋      | 71299/196351 [10:09:05<17:40:05,  1.97it/s]global step 71300, epoch: 1, batch: 71300, loss: 1.61952, acc: 0.81231, time: Fri Jan 21 20:43:44 2022
epoch 1 training:  36%|███▋      | 71399/196351 [10:09:56<17:33:46,  1.98it/s]global step 71400, epoch: 1, batch: 71400, loss: 3.73905, acc: 0.81244, time: Fri Jan 21 20:44:35 2022
epoch 1 training:  36%|███▋      | 71499/196351 [10:10:47<17:37:05,  1.97it/s]global step 71500, epoch: 1, batch: 71500, loss: 2.43424, acc: 0.81257, time: Fri Jan 21 20:45:26 2022
epoch 1 training:  36%|███▋      | 71599/196351 [10:11:38<17:23:08,  1.99it/s]global step 71600, epoch: 1, batch: 71600, loss: 2.00199, acc: 0.81266, time: Fri Jan 21 20:46:17 2022
epoch 1 training:  37%|███▋      | 71699/196351 [10:12:29<17:55:35,  1.93it/s]global step 71700, epoch: 1, batch: 71700, loss: 4.77261, acc: 0.81276, time: Fri Jan 21 20:47:08 2022
epoch 1 training:  37%|███▋      | 71799/196351 [10:13:20<17:35:08,  1.97it/s]global step 71800, epoch: 1, batch: 71800, loss: 0.78692, acc: 0.81288, time: Fri Jan 21 20:47:59 2022
epoch 1 training:  37%|███▋      | 71899/196351 [10:14:11<17:25:16,  1.98it/s]global step 71900, epoch: 1, batch: 71900, loss: 3.48172, acc: 0.81303, time: Fri Jan 21 20:48:50 2022
epoch 1 training:  37%|███▋      | 71999/196351 [10:15:02<17:39:23,  1.96it/s]global step 72000, epoch: 1, batch: 72000, loss: 3.73314, acc: 0.81314, time: Fri Jan 21 20:49:41 2022
epoch 1 training:  37%|███▋      | 72099/196351 [10:15:53<17:19:08,  1.99it/s]global step 72100, epoch: 1, batch: 72100, loss: 2.36061, acc: 0.81326, time: Fri Jan 21 20:50:32 2022
epoch 1 training:  37%|███▋      | 72199/196351 [10:16:44<17:35:06,  1.96it/s]global step 72200, epoch: 1, batch: 72200, loss: 3.77189, acc: 0.81334, time: Fri Jan 21 20:51:22 2022
epoch 1 training:  37%|███▋      | 72299/196351 [10:17:34<17:29:22,  1.97it/s]global step 72300, epoch: 1, batch: 72300, loss: 1.69385, acc: 0.81347, time: Fri Jan 21 20:52:13 2022
epoch 1 training:  37%|███▋      | 72399/196351 [10:18:25<17:24:42,  1.98it/s]global step 72400, epoch: 1, batch: 72400, loss: 1.19159, acc: 0.81356, time: Fri Jan 21 20:53:03 2022
epoch 1 training:  37%|███▋      | 72499/196351 [10:19:15<17:20:17,  1.98it/s]global step 72500, epoch: 1, batch: 72500, loss: 2.22496, acc: 0.81371, time: Fri Jan 21 20:53:54 2022
epoch 1 training:  37%|███▋      | 72599/196351 [10:20:06<17:21:44,  1.98it/s]global step 72600, epoch: 1, batch: 72600, loss: 1.56341, acc: 0.81377, time: Fri Jan 21 20:54:45 2022
epoch 1 training:  37%|███▋      | 72699/196351 [10:20:57<17:40:43,  1.94it/s]global step 72700, epoch: 1, batch: 72700, loss: 2.40192, acc: 0.81389, time: Fri Jan 21 20:55:36 2022
epoch 1 training:  37%|███▋      | 72799/196351 [10:21:48<17:25:53,  1.97it/s]global step 72800, epoch: 1, batch: 72800, loss: 1.35273, acc: 0.81399, time: Fri Jan 21 20:56:26 2022
epoch 1 training:  37%|███▋      | 72899/196351 [10:22:38<17:15:45,  1.99it/s]global step 72900, epoch: 1, batch: 72900, loss: 4.38470, acc: 0.81409, time: Fri Jan 21 20:57:17 2022
epoch 1 training:  37%|███▋      | 72999/196351 [10:23:29<17:34:36,  1.95it/s]global step 73000, epoch: 1, batch: 73000, loss: 4.30312, acc: 0.81422, time: Fri Jan 21 20:58:08 2022
epoch 1 training:  37%|███▋      | 73099/196351 [10:24:19<17:01:56,  2.01it/s]global step 73100, epoch: 1, batch: 73100, loss: 1.98849, acc: 0.81434, time: Fri Jan 21 20:58:58 2022
epoch 1 training:  37%|███▋      | 73199/196351 [10:25:10<17:32:35,  1.95it/s]global step 73200, epoch: 1, batch: 73200, loss: 3.81161, acc: 0.81445, time: Fri Jan 21 20:59:49 2022
epoch 1 training:  37%|███▋      | 73299/196351 [10:26:01<17:11:18,  1.99it/s]global step 73300, epoch: 1, batch: 73300, loss: 1.47502, acc: 0.81456, time: Fri Jan 21 21:00:40 2022
epoch 1 training:  37%|███▋      | 73399/196351 [10:26:52<17:24:21,  1.96it/s]global step 73400, epoch: 1, batch: 73400, loss: 0.61810, acc: 0.81469, time: Fri Jan 21 21:01:31 2022
epoch 1 training:  37%|███▋      | 73499/196351 [10:27:43<17:12:29,  1.98it/s]global step 73500, epoch: 1, batch: 73500, loss: 3.35158, acc: 0.81471, time: Fri Jan 21 21:02:22 2022
epoch 1 training:  37%|███▋      | 73599/196351 [10:28:34<17:24:06,  1.96it/s]global step 73600, epoch: 1, batch: 73600, loss: 2.08849, acc: 0.81475, time: Fri Jan 21 21:03:13 2022
epoch 1 training:  38%|███▊      | 73699/196351 [10:29:25<17:28:59,  1.95it/s]global step 73700, epoch: 1, batch: 73700, loss: 2.81677, acc: 0.81487, time: Fri Jan 21 21:04:04 2022
epoch 1 training:  38%|███▊      | 73799/196351 [10:30:16<16:58:17,  2.01it/s]global step 73800, epoch: 1, batch: 73800, loss: 3.66393, acc: 0.81494, time: Fri Jan 21 21:04:55 2022
epoch 1 training:  38%|███▊      | 73899/196351 [10:31:07<17:09:17,  1.98it/s]global step 73900, epoch: 1, batch: 73900, loss: 2.39936, acc: 0.81506, time: Fri Jan 21 21:05:45 2022
epoch 1 training:  38%|███▊      | 73999/196351 [10:31:58<17:08:30,  1.98it/s]global step 74000, epoch: 1, batch: 74000, loss: 2.31247, acc: 0.81518, time: Fri Jan 21 21:06:36 2022
epoch 1 training:  38%|███▊      | 74099/196351 [10:32:49<17:25:52,  1.95it/s]global step 74100, epoch: 1, batch: 74100, loss: 3.76774, acc: 0.81528, time: Fri Jan 21 21:07:27 2022
epoch 1 training:  38%|███▊      | 74199/196351 [10:33:40<17:26:47,  1.94it/s]global step 74200, epoch: 1, batch: 74200, loss: 2.52574, acc: 0.81538, time: Fri Jan 21 21:08:18 2022
epoch 1 training:  38%|███▊      | 74299/196351 [10:34:31<20:24:52,  1.66it/s]global step 74300, epoch: 1, batch: 74300, loss: 2.19115, acc: 0.81545, time: Fri Jan 21 21:09:10 2022
epoch 1 training:  38%|███▊      | 74399/196351 [10:35:22<17:00:19,  1.99it/s]global step 74400, epoch: 1, batch: 74400, loss: 2.37901, acc: 0.81555, time: Fri Jan 21 21:10:00 2022
epoch 1 training:  38%|███▊      | 74499/196351 [10:36:12<17:22:22,  1.95it/s]global step 74500, epoch: 1, batch: 74500, loss: 2.84119, acc: 0.81562, time: Fri Jan 21 21:10:51 2022
epoch 1 training:  38%|███▊      | 74599/196351 [10:37:03<17:04:39,  1.98it/s]global step 74600, epoch: 1, batch: 74600, loss: 2.88546, acc: 0.81569, time: Fri Jan 21 21:11:42 2022
epoch 1 training:  38%|███▊      | 74699/196351 [10:37:54<16:58:01,  1.99it/s]global step 74700, epoch: 1, batch: 74700, loss: 2.16291, acc: 0.81578, time: Fri Jan 21 21:12:32 2022
epoch 1 training:  38%|███▊      | 74799/196351 [10:38:44<17:06:58,  1.97it/s]global step 74800, epoch: 1, batch: 74800, loss: 1.89601, acc: 0.81588, time: Fri Jan 21 21:13:23 2022
epoch 1 training:  38%|███▊      | 74899/196351 [10:39:35<17:01:44,  1.98it/s]global step 74900, epoch: 1, batch: 74900, loss: 1.63648, acc: 0.81597, time: Fri Jan 21 21:14:13 2022
epoch 1 training:  38%|███▊      | 74999/196351 [10:40:25<17:07:57,  1.97it/s]global step 75000, epoch: 1, batch: 75000, loss: 1.77066, acc: 0.81608, time: Fri Jan 21 21:15:04 2022
epoch 1 training:  38%|███▊      | 75099/196351 [10:41:16<17:01:59,  1.98it/s]global step 75100, epoch: 1, batch: 75100, loss: 2.36436, acc: 0.81619, time: Fri Jan 21 21:15:55 2022
epoch 1 training:  38%|███▊      | 75199/196351 [10:42:07<17:16:22,  1.95it/s]global step 75200, epoch: 1, batch: 75200, loss: 1.65726, acc: 0.81626, time: Fri Jan 21 21:16:46 2022
epoch 1 training:  38%|███▊      | 75299/196351 [10:42:58<17:00:47,  1.98it/s]global step 75300, epoch: 1, batch: 75300, loss: 1.65309, acc: 0.81633, time: Fri Jan 21 21:17:37 2022
epoch 1 training:  38%|███▊      | 75399/196351 [10:43:49<17:10:40,  1.96it/s]global step 75400, epoch: 1, batch: 75400, loss: 2.24639, acc: 0.81641, time: Fri Jan 21 21:18:28 2022
epoch 1 training:  38%|███▊      | 75499/196351 [10:44:40<17:02:39,  1.97it/s]global step 75500, epoch: 1, batch: 75500, loss: 1.99572, acc: 0.81651, time: Fri Jan 21 21:19:19 2022
epoch 1 training:  39%|███▊      | 75599/196351 [10:45:32<18:55:07,  1.77it/s]global step 75600, epoch: 1, batch: 75600, loss: 2.41313, acc: 0.81660, time: Fri Jan 21 21:20:11 2022
epoch 1 training:  39%|███▊      | 75699/196351 [10:46:25<16:56:11,  1.98it/s]global step 75700, epoch: 1, batch: 75700, loss: 2.10088, acc: 0.81670, time: Fri Jan 21 21:21:04 2022
epoch 1 training:  39%|███▊      | 75799/196351 [10:47:16<17:02:58,  1.96it/s]global step 75800, epoch: 1, batch: 75800, loss: 3.19109, acc: 0.81678, time: Fri Jan 21 21:21:55 2022
epoch 1 training:  39%|███▊      | 75899/196351 [10:48:08<16:58:43,  1.97it/s]global step 75900, epoch: 1, batch: 75900, loss: 3.61009, acc: 0.81690, time: Fri Jan 21 21:22:46 2022
epoch 1 training:  39%|███▊      | 75999/196351 [10:49:00<18:11:02,  1.84it/s]global step 76000, epoch: 1, batch: 76000, loss: 1.46225, acc: 0.81696, time: Fri Jan 21 21:23:39 2022
epoch 1 training:  39%|███▉      | 76099/196351 [10:49:51<17:03:16,  1.96it/s]global step 76100, epoch: 1, batch: 76100, loss: 1.58288, acc: 0.81703, time: Fri Jan 21 21:24:30 2022
epoch 1 training:  39%|███▉      | 76199/196351 [10:50:42<16:45:51,  1.99it/s]global step 76200, epoch: 1, batch: 76200, loss: 2.11509, acc: 0.81715, time: Fri Jan 21 21:25:20 2022
epoch 1 training:  39%|███▉      | 76299/196351 [10:51:33<17:06:06,  1.95it/s]global step 76300, epoch: 1, batch: 76300, loss: 2.89531, acc: 0.81722, time: Fri Jan 21 21:26:11 2022
epoch 1 training:  39%|███▉      | 76399/196351 [10:52:23<16:45:32,  1.99it/s]global step 76400, epoch: 1, batch: 76400, loss: 2.02157, acc: 0.81730, time: Fri Jan 21 21:27:02 2022
epoch 1 training:  39%|███▉      | 76499/196351 [10:53:14<17:01:44,  1.96it/s]global step 76500, epoch: 1, batch: 76500, loss: 2.91573, acc: 0.81739, time: Fri Jan 21 21:27:53 2022
epoch 1 training:  39%|███▉      | 76599/196351 [10:54:05<16:41:55,  1.99it/s]global step 76600, epoch: 1, batch: 76600, loss: 1.80959, acc: 0.81749, time: Fri Jan 21 21:28:43 2022
epoch 1 training:  39%|███▉      | 76699/196351 [10:54:55<16:40:36,  1.99it/s]global step 76700, epoch: 1, batch: 76700, loss: 1.65280, acc: 0.81756, time: Fri Jan 21 21:29:34 2022
epoch 1 training:  39%|███▉      | 76799/196351 [10:55:46<16:55:22,  1.96it/s]global step 76800, epoch: 1, batch: 76800, loss: 2.63007, acc: 0.81762, time: Fri Jan 21 21:30:25 2022
epoch 1 training:  39%|███▉      | 76899/196351 [10:56:37<17:04:00,  1.94it/s]global step 76900, epoch: 1, batch: 76900, loss: 3.56152, acc: 0.81774, time: Fri Jan 21 21:31:16 2022
epoch 1 training:  39%|███▉      | 76999/196351 [10:57:29<16:45:48,  1.98it/s]global step 77000, epoch: 1, batch: 77000, loss: 1.27246, acc: 0.81783, time: Fri Jan 21 21:32:08 2022
epoch 1 training:  39%|███▉      | 77099/196351 [10:58:20<17:12:36,  1.92it/s]global step 77100, epoch: 1, batch: 77100, loss: 2.68355, acc: 0.81798, time: Fri Jan 21 21:32:59 2022
epoch 1 training:  39%|███▉      | 77199/196351 [10:59:12<16:51:29,  1.96it/s]global step 77200, epoch: 1, batch: 77200, loss: 2.71272, acc: 0.81808, time: Fri Jan 21 21:33:50 2022
epoch 1 training:  39%|███▉      | 77299/196351 [11:00:03<16:42:53,  1.98it/s]global step 77300, epoch: 1, batch: 77300, loss: 4.71857, acc: 0.81818, time: Fri Jan 21 21:34:41 2022
epoch 1 training:  39%|███▉      | 77399/196351 [11:00:54<16:36:54,  1.99it/s]global step 77400, epoch: 1, batch: 77400, loss: 3.75996, acc: 0.81820, time: Fri Jan 21 21:35:32 2022
epoch 1 training:  39%|███▉      | 77499/196351 [11:01:45<16:54:41,  1.95it/s]global step 77500, epoch: 1, batch: 77500, loss: 5.20619, acc: 0.81830, time: Fri Jan 21 21:36:23 2022
epoch 1 training:  40%|███▉      | 77599/196351 [11:02:36<16:45:54,  1.97it/s]global step 77600, epoch: 1, batch: 77600, loss: 3.19427, acc: 0.81836, time: Fri Jan 21 21:37:15 2022
epoch 1 training:  40%|███▉      | 77699/196351 [11:03:27<16:44:32,  1.97it/s]global step 77700, epoch: 1, batch: 77700, loss: 0.52377, acc: 0.81849, time: Fri Jan 21 21:38:06 2022
epoch 1 training:  40%|███▉      | 77799/196351 [11:04:18<17:03:44,  1.93it/s]global step 77800, epoch: 1, batch: 77800, loss: 1.42416, acc: 0.81860, time: Fri Jan 21 21:38:57 2022
epoch 1 training:  40%|███▉      | 77899/196351 [11:05:10<16:40:16,  1.97it/s]global step 77900, epoch: 1, batch: 77900, loss: 1.48324, acc: 0.81874, time: Fri Jan 21 21:39:48 2022
epoch 1 training:  40%|███▉      | 77999/196351 [11:06:01<16:49:47,  1.95it/s]global step 78000, epoch: 1, batch: 78000, loss: 1.34083, acc: 0.81881, time: Fri Jan 21 21:40:39 2022
epoch 1 training:  40%|███▉      | 78099/196351 [11:06:52<16:56:42,  1.94it/s]global step 78100, epoch: 1, batch: 78100, loss: 1.54785, acc: 0.81897, time: Fri Jan 21 21:41:31 2022
epoch 1 training:  40%|███▉      | 78199/196351 [11:07:43<16:43:10,  1.96it/s]global step 78200, epoch: 1, batch: 78200, loss: 0.84145, acc: 0.81910, time: Fri Jan 21 21:42:22 2022
epoch 1 training:  40%|███▉      | 78299/196351 [11:08:34<16:52:16,  1.94it/s]global step 78300, epoch: 1, batch: 78300, loss: 3.04663, acc: 0.81920, time: Fri Jan 21 21:43:13 2022
epoch 1 training:  40%|███▉      | 78399/196351 [11:09:25<16:48:50,  1.95it/s]global step 78400, epoch: 1, batch: 78400, loss: 1.87800, acc: 0.81928, time: Fri Jan 21 21:44:04 2022
epoch 1 training:  40%|███▉      | 78499/196351 [11:10:17<16:32:45,  1.98it/s]global step 78500, epoch: 1, batch: 78500, loss: 1.27949, acc: 0.81933, time: Fri Jan 21 21:44:55 2022
epoch 1 training:  40%|████      | 78599/196351 [11:11:08<16:37:11,  1.97it/s]global step 78600, epoch: 1, batch: 78600, loss: 3.64336, acc: 0.81941, time: Fri Jan 21 21:45:46 2022
epoch 1 training:  40%|████      | 78699/196351 [11:11:59<16:34:38,  1.97it/s]global step 78700, epoch: 1, batch: 78700, loss: 2.10636, acc: 0.81952, time: Fri Jan 21 21:46:38 2022
epoch 1 training:  40%|████      | 78799/196351 [11:12:50<16:35:24,  1.97it/s]global step 78800, epoch: 1, batch: 78800, loss: 3.61970, acc: 0.81957, time: Fri Jan 21 21:47:29 2022
epoch 1 training:  40%|████      | 78899/196351 [11:13:41<16:49:43,  1.94it/s]global step 78900, epoch: 1, batch: 78900, loss: 2.64031, acc: 0.81968, time: Fri Jan 21 21:48:20 2022
epoch 1 training:  40%|████      | 78999/196351 [11:14:33<16:50:24,  1.94it/s]global step 79000, epoch: 1, batch: 79000, loss: 2.64089, acc: 0.81978, time: Fri Jan 21 21:49:11 2022
epoch 1 training:  40%|████      | 79099/196351 [11:15:24<16:35:02,  1.96it/s]global step 79100, epoch: 1, batch: 79100, loss: 0.29271, acc: 0.81986, time: Fri Jan 21 21:50:02 2022
epoch 1 training:  40%|████      | 79199/196351 [11:16:15<16:39:57,  1.95it/s]global step 79200, epoch: 1, batch: 79200, loss: 2.24142, acc: 0.81992, time: Fri Jan 21 21:50:54 2022
epoch 1 training:  40%|████      | 79299/196351 [11:17:07<16:39:54,  1.95it/s]global step 79300, epoch: 1, batch: 79300, loss: 2.33462, acc: 0.82001, time: Fri Jan 21 21:51:45 2022
epoch 1 training:  40%|████      | 79399/196351 [11:17:58<16:34:20,  1.96it/s]global step 79400, epoch: 1, batch: 79400, loss: 1.65707, acc: 0.82007, time: Fri Jan 21 21:52:36 2022
epoch 1 training:  40%|████      | 79499/196351 [11:18:49<16:32:10,  1.96it/s]global step 79500, epoch: 1, batch: 79500, loss: 3.54501, acc: 0.82021, time: Fri Jan 21 21:53:27 2022
epoch 1 training:  41%|████      | 79599/196351 [11:19:39<16:26:47,  1.97it/s]global step 79600, epoch: 1, batch: 79600, loss: 4.20613, acc: 0.82034, time: Fri Jan 21 21:54:18 2022
epoch 1 training:  41%|████      | 79699/196351 [11:20:30<16:15:07,  1.99it/s]global step 79700, epoch: 1, batch: 79700, loss: 2.52738, acc: 0.82043, time: Fri Jan 21 21:55:09 2022
epoch 1 training:  41%|████      | 79799/196351 [11:21:21<16:23:32,  1.98it/s]global step 79800, epoch: 1, batch: 79800, loss: 1.58193, acc: 0.82051, time: Fri Jan 21 21:55:59 2022
epoch 1 training:  41%|████      | 79899/196351 [11:22:12<16:22:43,  1.97it/s]global step 79900, epoch: 1, batch: 79900, loss: 2.75180, acc: 0.82061, time: Fri Jan 21 21:56:50 2022
epoch 1 training:  41%|████      | 79999/196351 [11:23:02<16:21:19,  1.98it/s]global step 80000, epoch: 1, batch: 80000, loss: 3.83488, acc: 0.82072, time: Fri Jan 21 21:57:41 2022
epoch 1 training:  41%|████      | 80099/196351 [11:23:54<16:19:57,  1.98it/s]global step 80100, epoch: 1, batch: 80100, loss: 2.31347, acc: 0.82079, time: Fri Jan 21 21:58:32 2022
epoch 1 training:  41%|████      | 80199/196351 [11:24:44<16:21:11,  1.97it/s]global step 80200, epoch: 1, batch: 80200, loss: 2.31421, acc: 0.82084, time: Fri Jan 21 21:59:23 2022
epoch 1 training:  41%|████      | 80299/196351 [11:25:36<16:32:44,  1.95it/s]global step 80300, epoch: 1, batch: 80300, loss: 3.68394, acc: 0.82092, time: Fri Jan 21 22:00:14 2022
epoch 1 training:  41%|████      | 80399/196351 [11:26:27<16:22:13,  1.97it/s]global step 80400, epoch: 1, batch: 80400, loss: 2.14703, acc: 0.82103, time: Fri Jan 21 22:01:05 2022
epoch 1 training:  41%|████      | 80499/196351 [11:27:18<16:12:43,  1.98it/s]global step 80500, epoch: 1, batch: 80500, loss: 1.34780, acc: 0.82111, time: Fri Jan 21 22:01:56 2022
epoch 1 training:  41%|████      | 80599/196351 [11:28:08<16:34:26,  1.94it/s]global step 80600, epoch: 1, batch: 80600, loss: 1.82294, acc: 0.82122, time: Fri Jan 21 22:02:47 2022
epoch 1 training:  41%|████      | 80699/196351 [11:28:59<16:22:07,  1.96it/s]global step 80700, epoch: 1, batch: 80700, loss: 1.72641, acc: 0.82134, time: Fri Jan 21 22:03:38 2022
epoch 1 training:  41%|████      | 80799/196351 [11:29:50<16:07:47,  1.99it/s]global step 80800, epoch: 1, batch: 80800, loss: 0.73352, acc: 0.82148, time: Fri Jan 21 22:04:28 2022
epoch 1 training:  41%|████      | 80899/196351 [11:30:40<16:16:25,  1.97it/s]global step 80900, epoch: 1, batch: 80900, loss: 2.76518, acc: 0.82161, time: Fri Jan 21 22:05:19 2022
epoch 1 training:  41%|████▏     | 80999/196351 [11:31:31<16:08:43,  1.98it/s]global step 81000, epoch: 1, batch: 81000, loss: 2.32637, acc: 0.82167, time: Fri Jan 21 22:06:10 2022
epoch 1 training:  41%|████▏     | 81099/196351 [11:32:22<16:10:00,  1.98it/s]global step 81100, epoch: 1, batch: 81100, loss: 1.68295, acc: 0.82173, time: Fri Jan 21 22:07:00 2022
epoch 1 training:  41%|████▏     | 81199/196351 [11:33:13<16:32:33,  1.93it/s]global step 81200, epoch: 1, batch: 81200, loss: 0.98345, acc: 0.82183, time: Fri Jan 21 22:07:52 2022
epoch 1 training:  41%|████▏     | 81299/196351 [11:34:04<16:21:11,  1.95it/s]global step 81300, epoch: 1, batch: 81300, loss: 2.81759, acc: 0.82188, time: Fri Jan 21 22:08:42 2022
epoch 1 training:  41%|████▏     | 81399/196351 [11:34:55<16:11:13,  1.97it/s]global step 81400, epoch: 1, batch: 81400, loss: 4.08335, acc: 0.82195, time: Fri Jan 21 22:09:33 2022
epoch 1 training:  42%|████▏     | 81499/196351 [11:35:46<16:11:36,  1.97it/s]global step 81500, epoch: 1, batch: 81500, loss: 3.73823, acc: 0.82205, time: Fri Jan 21 22:10:24 2022
epoch 1 training:  42%|████▏     | 81599/196351 [11:36:37<16:09:49,  1.97it/s]global step 81600, epoch: 1, batch: 81600, loss: 4.75515, acc: 0.82208, time: Fri Jan 21 22:11:15 2022
epoch 1 training:  42%|████▏     | 81699/196351 [11:37:28<16:25:23,  1.94it/s]global step 81700, epoch: 1, batch: 81700, loss: 1.92848, acc: 0.82217, time: Fri Jan 21 22:12:06 2022
epoch 1 training:  42%|████▏     | 81799/196351 [11:38:19<16:16:23,  1.96it/s]global step 81800, epoch: 1, batch: 81800, loss: 5.45738, acc: 0.82224, time: Fri Jan 21 22:12:57 2022
epoch 1 training:  42%|████▏     | 81899/196351 [11:39:09<16:03:49,  1.98it/s]global step 81900, epoch: 1, batch: 81900, loss: 1.41357, acc: 0.82234, time: Fri Jan 21 22:13:48 2022
epoch 1 training:  42%|████▏     | 81999/196351 [11:40:00<16:07:09,  1.97it/s]global step 82000, epoch: 1, batch: 82000, loss: 5.20210, acc: 0.82241, time: Fri Jan 21 22:14:39 2022
epoch 1 training:  42%|████▏     | 82099/196351 [11:40:51<15:57:17,  1.99it/s]global step 82100, epoch: 1, batch: 82100, loss: 1.89179, acc: 0.82253, time: Fri Jan 21 22:15:30 2022
epoch 1 training:  42%|████▏     | 82199/196351 [11:41:42<15:49:18,  2.00it/s]global step 82200, epoch: 1, batch: 82200, loss: 1.82467, acc: 0.82263, time: Fri Jan 21 22:16:20 2022
epoch 1 training:  42%|████▏     | 82299/196351 [11:42:32<15:48:15,  2.00it/s]global step 82300, epoch: 1, batch: 82300, loss: 0.93343, acc: 0.82267, time: Fri Jan 21 22:17:10 2022
epoch 1 training:  42%|████▏     | 82399/196351 [11:43:22<16:05:04,  1.97it/s]global step 82400, epoch: 1, batch: 82400, loss: 0.80352, acc: 0.82279, time: Fri Jan 21 22:18:01 2022
epoch 1 training:  42%|████▏     | 82499/196351 [11:44:13<15:51:03,  2.00it/s]global step 82500, epoch: 1, batch: 82500, loss: 0.40939, acc: 0.82288, time: Fri Jan 21 22:18:52 2022
epoch 1 training:  42%|████▏     | 82599/196351 [11:45:03<15:47:09,  2.00it/s]global step 82600, epoch: 1, batch: 82600, loss: 2.49235, acc: 0.82290, time: Fri Jan 21 22:19:42 2022
epoch 1 training:  42%|████▏     | 82699/196351 [11:45:54<16:15:12,  1.94it/s]global step 82700, epoch: 1, batch: 82700, loss: 2.13975, acc: 0.82302, time: Fri Jan 21 22:20:33 2022
epoch 1 training:  42%|████▏     | 82799/196351 [11:46:45<16:16:08,  1.94it/s]global step 82800, epoch: 1, batch: 82800, loss: 1.45376, acc: 0.82310, time: Fri Jan 21 22:21:24 2022
epoch 1 training:  42%|████▏     | 82899/196351 [11:47:36<16:06:34,  1.96it/s]global step 82900, epoch: 1, batch: 82900, loss: 4.18943, acc: 0.82318, time: Fri Jan 21 22:22:15 2022
epoch 1 training:  42%|████▏     | 82999/196351 [11:48:28<16:02:50,  1.96it/s]global step 83000, epoch: 1, batch: 83000, loss: 2.11879, acc: 0.82322, time: Fri Jan 21 22:23:07 2022
epoch 1 training:  42%|████▏     | 83099/196351 [11:49:19<16:03:04,  1.96it/s]global step 83100, epoch: 1, batch: 83100, loss: 2.55806, acc: 0.82332, time: Fri Jan 21 22:23:58 2022
epoch 1 training:  42%|████▏     | 83199/196351 [11:50:10<16:00:13,  1.96it/s]global step 83200, epoch: 1, batch: 83200, loss: 2.28885, acc: 0.82341, time: Fri Jan 21 22:24:49 2022
epoch 1 training:  42%|████▏     | 83299/196351 [11:51:01<16:01:01,  1.96it/s]global step 83300, epoch: 1, batch: 83300, loss: 2.03554, acc: 0.82353, time: Fri Jan 21 22:25:40 2022
epoch 1 training:  42%|████▏     | 83399/196351 [11:51:52<16:02:55,  1.96it/s]global step 83400, epoch: 1, batch: 83400, loss: 4.56346, acc: 0.82360, time: Fri Jan 21 22:26:31 2022
epoch 1 training:  43%|████▎     | 83499/196351 [11:52:43<15:53:31,  1.97it/s]global step 83500, epoch: 1, batch: 83500, loss: 3.74426, acc: 0.82363, time: Fri Jan 21 22:27:22 2022
epoch 1 training:  43%|████▎     | 83599/196351 [11:53:35<15:52:52,  1.97it/s]global step 83600, epoch: 1, batch: 83600, loss: 0.96225, acc: 0.82373, time: Fri Jan 21 22:28:13 2022
epoch 1 training:  43%|████▎     | 83699/196351 [11:54:26<15:59:27,  1.96it/s]global step 83700, epoch: 1, batch: 83700, loss: 1.45644, acc: 0.82384, time: Fri Jan 21 22:29:05 2022
epoch 1 training:  43%|████▎     | 83799/196351 [11:55:17<15:52:32,  1.97it/s]global step 83800, epoch: 1, batch: 83800, loss: 4.95810, acc: 0.82383, time: Fri Jan 21 22:29:56 2022
epoch 1 training:  43%|████▎     | 83899/196351 [11:56:08<15:58:24,  1.96it/s]global step 83900, epoch: 1, batch: 83900, loss: 1.16134, acc: 0.82392, time: Fri Jan 21 22:30:47 2022
epoch 1 training:  43%|████▎     | 83999/196351 [11:56:59<15:47:08,  1.98it/s]global step 84000, epoch: 1, batch: 84000, loss: 1.35368, acc: 0.82399, time: Fri Jan 21 22:31:38 2022
epoch 1 training:  43%|████▎     | 84099/196351 [11:57:50<15:42:43,  1.98it/s]global step 84100, epoch: 1, batch: 84100, loss: 3.13029, acc: 0.82407, time: Fri Jan 21 22:32:29 2022
epoch 1 training:  43%|████▎     | 84199/196351 [11:58:40<15:41:58,  1.98it/s]global step 84200, epoch: 1, batch: 84200, loss: 1.56359, acc: 0.82416, time: Fri Jan 21 22:33:19 2022
epoch 1 training:  43%|████▎     | 84299/196351 [11:59:31<15:39:24,  1.99it/s]global step 84300, epoch: 1, batch: 84300, loss: 2.24608, acc: 0.82421, time: Fri Jan 21 22:34:10 2022
epoch 1 training:  43%|████▎     | 84399/196351 [12:00:22<15:51:11,  1.96it/s]global step 84400, epoch: 1, batch: 84400, loss: 2.93524, acc: 0.82426, time: Fri Jan 21 22:35:01 2022
epoch 1 training:  43%|████▎     | 84499/196351 [12:01:13<16:04:37,  1.93it/s]global step 84500, epoch: 1, batch: 84500, loss: 2.94478, acc: 0.82431, time: Fri Jan 21 22:35:52 2022
epoch 1 training:  43%|████▎     | 84599/196351 [12:02:04<15:52:42,  1.95it/s]global step 84600, epoch: 1, batch: 84600, loss: 1.99153, acc: 0.82431, time: Fri Jan 21 22:36:43 2022
epoch 1 training:  43%|████▎     | 84699/196351 [12:02:55<15:33:03,  1.99it/s]global step 84700, epoch: 1, batch: 84700, loss: 2.93673, acc: 0.82437, time: Fri Jan 21 22:37:33 2022
epoch 1 training:  43%|████▎     | 84799/196351 [12:03:46<15:45:06,  1.97it/s]global step 84800, epoch: 1, batch: 84800, loss: 3.80167, acc: 0.82448, time: Fri Jan 21 22:38:24 2022
epoch 1 training:  43%|████▎     | 84899/196351 [12:04:37<15:43:28,  1.97it/s]global step 84900, epoch: 1, batch: 84900, loss: 1.49393, acc: 0.82459, time: Fri Jan 21 22:39:16 2022
epoch 1 training:  43%|████▎     | 84999/196351 [12:05:28<15:39:08,  1.98it/s]global step 85000, epoch: 1, batch: 85000, loss: 1.83524, acc: 0.82466, time: Fri Jan 21 22:40:06 2022
epoch 1 training:  43%|████▎     | 85099/196351 [12:06:18<15:29:01,  2.00it/s]global step 85100, epoch: 1, batch: 85100, loss: 2.14404, acc: 0.82477, time: Fri Jan 21 22:40:57 2022
epoch 1 training:  43%|████▎     | 85199/196351 [12:07:09<15:44:34,  1.96it/s]global step 85200, epoch: 1, batch: 85200, loss: 2.61978, acc: 0.82484, time: Fri Jan 21 22:41:48 2022
epoch 1 training:  43%|████▎     | 85299/196351 [12:08:00<15:46:19,  1.96it/s]global step 85300, epoch: 1, batch: 85300, loss: 1.97716, acc: 0.82496, time: Fri Jan 21 22:42:38 2022
epoch 1 training:  43%|████▎     | 85399/196351 [12:08:50<15:30:26,  1.99it/s]global step 85400, epoch: 1, batch: 85400, loss: 2.74550, acc: 0.82500, time: Fri Jan 21 22:43:29 2022
epoch 1 training:  44%|████▎     | 85499/196351 [12:09:41<15:36:38,  1.97it/s]global step 85500, epoch: 1, batch: 85500, loss: 1.67010, acc: 0.82506, time: Fri Jan 21 22:44:20 2022
epoch 1 training:  44%|████▎     | 85599/196351 [12:10:32<15:21:35,  2.00it/s]global step 85600, epoch: 1, batch: 85600, loss: 2.75554, acc: 0.82512, time: Fri Jan 21 22:45:10 2022
epoch 1 training:  44%|████▎     | 85699/196351 [12:11:22<15:28:29,  1.99it/s]global step 85700, epoch: 1, batch: 85700, loss: 0.24029, acc: 0.82522, time: Fri Jan 21 22:46:01 2022
epoch 1 training:  44%|████▎     | 85799/196351 [12:12:13<15:39:11,  1.96it/s]global step 85800, epoch: 1, batch: 85800, loss: 1.89590, acc: 0.82535, time: Fri Jan 21 22:46:52 2022
epoch 1 training:  44%|████▎     | 85899/196351 [12:13:05<16:11:37,  1.89it/s]global step 85900, epoch: 1, batch: 85900, loss: 1.87236, acc: 0.82541, time: Fri Jan 21 22:47:43 2022
epoch 1 training:  44%|████▍     | 85999/196351 [12:13:56<15:35:48,  1.97it/s]global step 86000, epoch: 1, batch: 86000, loss: 3.52003, acc: 0.82551, time: Fri Jan 21 22:48:34 2022
epoch 1 training:  44%|████▍     | 86099/196351 [12:14:47<15:44:24,  1.95it/s]global step 86100, epoch: 1, batch: 86100, loss: 2.33668, acc: 0.82563, time: Fri Jan 21 22:49:26 2022
epoch 1 training:  44%|████▍     | 86199/196351 [12:15:37<16:03:39,  1.91it/s]global step 86200, epoch: 1, batch: 86200, loss: 1.90778, acc: 0.82573, time: Fri Jan 21 22:50:16 2022
epoch 1 training:  44%|████▍     | 86299/196351 [12:16:28<15:29:53,  1.97it/s]global step 86300, epoch: 1, batch: 86300, loss: 2.67015, acc: 0.82580, time: Fri Jan 21 22:51:07 2022
epoch 1 training:  44%|████▍     | 86399/196351 [12:17:19<15:30:08,  1.97it/s]global step 86400, epoch: 1, batch: 86400, loss: 1.07737, acc: 0.82583, time: Fri Jan 21 22:51:58 2022
epoch 1 training:  44%|████▍     | 86499/196351 [12:18:10<15:29:15,  1.97it/s]global step 86500, epoch: 1, batch: 86500, loss: 0.66415, acc: 0.82587, time: Fri Jan 21 22:52:49 2022
epoch 1 training:  44%|████▍     | 86599/196351 [12:19:01<15:26:28,  1.97it/s]global step 86600, epoch: 1, batch: 86600, loss: 2.94923, acc: 0.82594, time: Fri Jan 21 22:53:39 2022
epoch 1 training:  44%|████▍     | 86699/196351 [12:19:51<15:28:51,  1.97it/s]global step 86700, epoch: 1, batch: 86700, loss: 4.24621, acc: 0.82603, time: Fri Jan 21 22:54:30 2022
epoch 1 training:  44%|████▍     | 86799/196351 [12:20:42<15:14:46,  2.00it/s]global step 86800, epoch: 1, batch: 86800, loss: 3.54464, acc: 0.82612, time: Fri Jan 21 22:55:21 2022
epoch 1 training:  44%|████▍     | 86899/196351 [12:21:33<15:06:45,  2.01it/s]global step 86900, epoch: 1, batch: 86900, loss: 0.84833, acc: 0.82621, time: Fri Jan 21 22:56:12 2022
epoch 1 training:  44%|████▍     | 86999/196351 [12:22:24<15:26:09,  1.97it/s]global step 87000, epoch: 1, batch: 87000, loss: 0.66103, acc: 0.82630, time: Fri Jan 21 22:57:03 2022
epoch 1 training:  44%|████▍     | 87099/196351 [12:23:15<15:20:34,  1.98it/s]global step 87100, epoch: 1, batch: 87100, loss: 2.40979, acc: 0.82638, time: Fri Jan 21 22:57:53 2022
epoch 1 training:  44%|████▍     | 87199/196351 [12:24:05<15:19:26,  1.98it/s]global step 87200, epoch: 1, batch: 87200, loss: 3.84108, acc: 0.82642, time: Fri Jan 21 22:58:44 2022
epoch 1 training:  44%|████▍     | 87299/196351 [12:24:56<15:19:11,  1.98it/s]global step 87300, epoch: 1, batch: 87300, loss: 2.17750, acc: 0.82651, time: Fri Jan 21 22:59:35 2022
epoch 1 training:  45%|████▍     | 87399/196351 [12:25:46<15:23:29,  1.97it/s]global step 87400, epoch: 1, batch: 87400, loss: 2.26623, acc: 0.82658, time: Fri Jan 21 23:00:25 2022
epoch 1 training:  45%|████▍     | 87499/196351 [12:26:37<15:13:58,  1.98it/s]global step 87500, epoch: 1, batch: 87500, loss: 1.41587, acc: 0.82665, time: Fri Jan 21 23:01:16 2022
epoch 1 training:  45%|████▍     | 87599/196351 [12:27:28<15:07:44,  2.00it/s]global step 87600, epoch: 1, batch: 87600, loss: 2.78812, acc: 0.82670, time: Fri Jan 21 23:02:06 2022
epoch 1 training:  45%|████▍     | 87699/196351 [12:28:18<15:25:07,  1.96it/s]global step 87700, epoch: 1, batch: 87700, loss: 0.21928, acc: 0.82680, time: Fri Jan 21 23:02:57 2022
epoch 1 training:  45%|████▍     | 87799/196351 [12:29:08<15:06:11,  2.00it/s]global step 87800, epoch: 1, batch: 87800, loss: 1.49572, acc: 0.82685, time: Fri Jan 21 23:03:47 2022
epoch 1 training:  45%|████▍     | 87899/196351 [12:29:59<15:23:02,  1.96it/s]global step 87900, epoch: 1, batch: 87900, loss: 1.87091, acc: 0.82695, time: Fri Jan 21 23:04:38 2022
epoch 1 training:  45%|████▍     | 87999/196351 [12:30:49<15:13:13,  1.98it/s]global step 88000, epoch: 1, batch: 88000, loss: 0.70534, acc: 0.82702, time: Fri Jan 21 23:05:28 2022
epoch 1 training:  45%|████▍     | 88099/196351 [12:31:40<15:19:27,  1.96it/s]global step 88100, epoch: 1, batch: 88100, loss: 0.81717, acc: 0.82709, time: Fri Jan 21 23:06:19 2022
epoch 1 training:  45%|████▍     | 88199/196351 [12:32:31<15:10:33,  1.98it/s]global step 88200, epoch: 1, batch: 88200, loss: 1.19333, acc: 0.82717, time: Fri Jan 21 23:07:10 2022
epoch 1 training:  45%|████▍     | 88299/196351 [12:33:22<15:06:50,  1.99it/s]global step 88300, epoch: 1, batch: 88300, loss: 1.48757, acc: 0.82726, time: Fri Jan 21 23:08:00 2022
epoch 1 training:  45%|████▌     | 88399/196351 [12:34:12<15:12:08,  1.97it/s]global step 88400, epoch: 1, batch: 88400, loss: 5.63978, acc: 0.82734, time: Fri Jan 21 23:08:51 2022
epoch 1 training:  45%|████▌     | 88499/196351 [12:35:03<15:23:43,  1.95it/s]global step 88500, epoch: 1, batch: 88500, loss: 2.06688, acc: 0.82737, time: Fri Jan 21 23:09:42 2022
epoch 1 training:  45%|████▌     | 88599/196351 [12:35:54<14:50:50,  2.02it/s]global step 88600, epoch: 1, batch: 88600, loss: 3.51957, acc: 0.82742, time: Fri Jan 21 23:10:33 2022
epoch 1 training:  45%|████▌     | 88699/196351 [12:36:45<18:11:18,  1.64it/s]global step 88700, epoch: 1, batch: 88700, loss: 1.68947, acc: 0.82751, time: Fri Jan 21 23:11:24 2022
epoch 1 training:  45%|████▌     | 88799/196351 [12:37:36<14:57:21,  2.00it/s]global step 88800, epoch: 1, batch: 88800, loss: 3.30461, acc: 0.82758, time: Fri Jan 21 23:12:15 2022
epoch 1 training:  45%|████▌     | 88899/196351 [12:38:27<15:18:06,  1.95it/s]global step 88900, epoch: 1, batch: 88900, loss: 2.57589, acc: 0.82768, time: Fri Jan 21 23:13:06 2022
epoch 1 training:  45%|████▌     | 88999/196351 [12:39:18<14:54:11,  2.00it/s]global step 89000, epoch: 1, batch: 89000, loss: 4.95337, acc: 0.82776, time: Fri Jan 21 23:13:56 2022
epoch 1 training:  45%|████▌     | 89099/196351 [12:40:08<15:31:16,  1.92it/s]global step 89100, epoch: 1, batch: 89100, loss: 0.57424, acc: 0.82777, time: Fri Jan 21 23:14:47 2022
epoch 1 training:  45%|████▌     | 89199/196351 [12:40:59<15:06:25,  1.97it/s]global step 89200, epoch: 1, batch: 89200, loss: 1.92659, acc: 0.82781, time: Fri Jan 21 23:15:38 2022
epoch 1 training:  45%|████▌     | 89299/196351 [12:41:50<15:06:11,  1.97it/s]global step 89300, epoch: 1, batch: 89300, loss: 0.49474, acc: 0.82791, time: Fri Jan 21 23:16:29 2022
epoch 1 training:  46%|████▌     | 89399/196351 [12:42:41<15:15:39,  1.95it/s]global step 89400, epoch: 1, batch: 89400, loss: 1.54287, acc: 0.82795, time: Fri Jan 21 23:17:20 2022
epoch 1 training:  46%|████▌     | 89499/196351 [12:43:32<15:22:01,  1.93it/s]global step 89500, epoch: 1, batch: 89500, loss: 1.57834, acc: 0.82801, time: Fri Jan 21 23:18:11 2022
epoch 1 training:  46%|████▌     | 89599/196351 [12:44:23<15:13:21,  1.95it/s]global step 89600, epoch: 1, batch: 89600, loss: 2.70019, acc: 0.82805, time: Fri Jan 21 23:19:02 2022
epoch 1 training:  46%|████▌     | 89699/196351 [12:45:14<15:13:11,  1.95it/s]global step 89700, epoch: 1, batch: 89700, loss: 2.84566, acc: 0.82809, time: Fri Jan 21 23:19:53 2022
epoch 1 training:  46%|████▌     | 89799/196351 [12:46:06<14:57:17,  1.98it/s]global step 89800, epoch: 1, batch: 89800, loss: 4.30282, acc: 0.82812, time: Fri Jan 21 23:20:44 2022
epoch 1 training:  46%|████▌     | 89899/196351 [12:46:56<15:05:28,  1.96it/s]global step 89900, epoch: 1, batch: 89900, loss: 2.74284, acc: 0.82819, time: Fri Jan 21 23:21:35 2022
epoch 1 training:  46%|████▌     | 89999/196351 [12:47:47<14:55:49,  1.98it/s]global step 90000, epoch: 1, batch: 90000, loss: 3.29461, acc: 0.82824, time: Fri Jan 21 23:22:26 2022
epoch 1 training:  46%|████▌     | 90099/196351 [12:48:38<15:01:51,  1.96it/s]global step 90100, epoch: 1, batch: 90100, loss: 0.70287, acc: 0.82832, time: Fri Jan 21 23:23:16 2022
epoch 1 training:  46%|████▌     | 90199/196351 [12:49:28<14:47:25,  1.99it/s]global step 90200, epoch: 1, batch: 90200, loss: 2.29151, acc: 0.82839, time: Fri Jan 21 23:24:07 2022
epoch 1 training:  46%|████▌     | 90299/196351 [12:50:19<14:53:52,  1.98it/s]global step 90300, epoch: 1, batch: 90300, loss: 3.80130, acc: 0.82842, time: Fri Jan 21 23:24:58 2022
epoch 1 training:  46%|████▌     | 90399/196351 [12:51:10<15:00:37,  1.96it/s]global step 90400, epoch: 1, batch: 90400, loss: 2.00527, acc: 0.82851, time: Fri Jan 21 23:25:48 2022
epoch 1 training:  46%|████▌     | 90499/196351 [12:52:01<15:04:31,  1.95it/s]global step 90500, epoch: 1, batch: 90500, loss: 1.32846, acc: 0.82860, time: Fri Jan 21 23:26:39 2022
epoch 1 training:  46%|████▌     | 90599/196351 [12:52:52<15:02:12,  1.95it/s]global step 90600, epoch: 1, batch: 90600, loss: 2.11933, acc: 0.82869, time: Fri Jan 21 23:27:30 2022
epoch 1 training:  46%|████▌     | 90699/196351 [12:53:43<14:51:35,  1.97it/s]global step 90700, epoch: 1, batch: 90700, loss: 0.64279, acc: 0.82878, time: Fri Jan 21 23:28:21 2022
epoch 1 training:  46%|████▌     | 90799/196351 [12:54:34<14:58:01,  1.96it/s]global step 90800, epoch: 1, batch: 90800, loss: 1.96238, acc: 0.82886, time: Fri Jan 21 23:29:13 2022
epoch 1 training:  46%|████▋     | 90899/196351 [12:55:25<14:56:13,  1.96it/s]global step 90900, epoch: 1, batch: 90900, loss: 3.79697, acc: 0.82892, time: Fri Jan 21 23:30:04 2022
epoch 1 training:  46%|████▋     | 90999/196351 [12:56:16<15:09:04,  1.93it/s]global step 91000, epoch: 1, batch: 91000, loss: 1.72945, acc: 0.82897, time: Fri Jan 21 23:30:54 2022
epoch 1 training:  46%|████▋     | 91099/196351 [12:57:06<14:51:03,  1.97it/s]global step 91100, epoch: 1, batch: 91100, loss: 0.69897, acc: 0.82906, time: Fri Jan 21 23:31:45 2022
epoch 1 training:  46%|████▋     | 91199/196351 [12:57:57<14:51:03,  1.97it/s]global step 91200, epoch: 1, batch: 91200, loss: 4.64018, acc: 0.82912, time: Fri Jan 21 23:32:36 2022
epoch 1 training:  46%|████▋     | 91299/196351 [12:58:48<14:33:10,  2.01it/s]global step 91300, epoch: 1, batch: 91300, loss: 1.17941, acc: 0.82919, time: Fri Jan 21 23:33:27 2022
epoch 1 training:  47%|████▋     | 91399/196351 [12:59:39<14:46:22,  1.97it/s]global step 91400, epoch: 1, batch: 91400, loss: 2.89735, acc: 0.82928, time: Fri Jan 21 23:34:18 2022
epoch 1 training:  47%|████▋     | 91499/196351 [13:00:30<14:42:16,  1.98it/s]global step 91500, epoch: 1, batch: 91500, loss: 3.14301, acc: 0.82934, time: Fri Jan 21 23:35:09 2022
epoch 1 training:  47%|████▋     | 91599/196351 [13:01:21<14:42:05,  1.98it/s]global step 91600, epoch: 1, batch: 91600, loss: 3.58990, acc: 0.82946, time: Fri Jan 21 23:35:59 2022
epoch 1 training:  47%|████▋     | 91699/196351 [13:02:11<14:51:44,  1.96it/s]global step 91700, epoch: 1, batch: 91700, loss: 2.44727, acc: 0.82953, time: Fri Jan 21 23:36:50 2022
epoch 1 training:  47%|████▋     | 91799/196351 [13:03:02<14:42:52,  1.97it/s]global step 91800, epoch: 1, batch: 91800, loss: 2.36977, acc: 0.82957, time: Fri Jan 21 23:37:41 2022
epoch 1 training:  47%|████▋     | 91899/196351 [13:03:53<14:47:45,  1.96it/s]global step 91900, epoch: 1, batch: 91900, loss: 3.32533, acc: 0.82966, time: Fri Jan 21 23:38:32 2022
epoch 1 training:  47%|████▋     | 91999/196351 [13:04:43<14:38:17,  1.98it/s]global step 92000, epoch: 1, batch: 92000, loss: 3.19302, acc: 0.82971, time: Fri Jan 21 23:39:22 2022
epoch 1 training:  47%|████▋     | 92099/196351 [13:05:34<14:44:03,  1.97it/s]global step 92100, epoch: 1, batch: 92100, loss: 2.02538, acc: 0.82977, time: Fri Jan 21 23:40:13 2022
epoch 1 training:  47%|████▋     | 92199/196351 [13:06:25<14:36:31,  1.98it/s]global step 92200, epoch: 1, batch: 92200, loss: 2.33776, acc: 0.82985, time: Fri Jan 21 23:41:04 2022
epoch 1 training:  47%|████▋     | 92299/196351 [13:07:16<14:37:46,  1.98it/s]global step 92300, epoch: 1, batch: 92300, loss: 0.57451, acc: 0.82993, time: Fri Jan 21 23:41:54 2022
epoch 1 training:  47%|████▋     | 92399/196351 [13:08:07<14:47:33,  1.95it/s]global step 92400, epoch: 1, batch: 92400, loss: 1.06556, acc: 0.83001, time: Fri Jan 21 23:42:45 2022
epoch 1 training:  47%|████▋     | 92499/196351 [13:08:58<14:37:01,  1.97it/s]global step 92500, epoch: 1, batch: 92500, loss: 1.79659, acc: 0.83009, time: Fri Jan 21 23:43:36 2022
epoch 1 training:  47%|████▋     | 92599/196351 [13:09:49<14:35:20,  1.98it/s]global step 92600, epoch: 1, batch: 92600, loss: 0.84570, acc: 0.83015, time: Fri Jan 21 23:44:28 2022
epoch 1 training:  47%|████▋     | 92699/196351 [13:10:40<14:22:27,  2.00it/s]global step 92700, epoch: 1, batch: 92700, loss: 2.48304, acc: 0.83026, time: Fri Jan 21 23:45:18 2022
epoch 1 training:  47%|████▋     | 92799/196351 [13:11:30<14:42:24,  1.96it/s]global step 92800, epoch: 1, batch: 92800, loss: 2.49419, acc: 0.83033, time: Fri Jan 21 23:46:09 2022
epoch 1 training:  47%|████▋     | 92899/196351 [13:12:21<14:27:55,  1.99it/s]global step 92900, epoch: 1, batch: 92900, loss: 0.49539, acc: 0.83038, time: Fri Jan 21 23:47:00 2022
epoch 1 training:  47%|████▋     | 92999/196351 [13:13:12<14:35:13,  1.97it/s]global step 93000, epoch: 1, batch: 93000, loss: 2.46512, acc: 0.83045, time: Fri Jan 21 23:47:51 2022
epoch 1 training:  47%|████▋     | 93099/196351 [13:14:02<14:53:18,  1.93it/s]global step 93100, epoch: 1, batch: 93100, loss: 0.57374, acc: 0.83054, time: Fri Jan 21 23:48:41 2022
epoch 1 training:  47%|████▋     | 93199/196351 [13:14:53<14:27:02,  1.98it/s]global step 93200, epoch: 1, batch: 93200, loss: 3.59998, acc: 0.83057, time: Fri Jan 21 23:49:32 2022
epoch 1 training:  48%|████▊     | 93299/196351 [13:15:43<14:31:44,  1.97it/s]global step 93300, epoch: 1, batch: 93300, loss: 5.06992, acc: 0.83068, time: Fri Jan 21 23:50:22 2022
epoch 1 training:  48%|████▊     | 93399/196351 [13:16:34<14:26:20,  1.98it/s]global step 93400, epoch: 1, batch: 93400, loss: 1.57360, acc: 0.83073, time: Fri Jan 21 23:51:13 2022
epoch 1 training:  48%|████▊     | 93499/196351 [13:17:25<14:28:12,  1.97it/s]global step 93500, epoch: 1, batch: 93500, loss: 3.24822, acc: 0.83078, time: Fri Jan 21 23:52:04 2022
epoch 1 training:  48%|████▊     | 93599/196351 [13:18:16<14:35:14,  1.96it/s]global step 93600, epoch: 1, batch: 93600, loss: 0.31712, acc: 0.83087, time: Fri Jan 21 23:52:55 2022
epoch 1 training:  48%|████▊     | 93699/196351 [13:19:07<14:25:12,  1.98it/s]global step 93700, epoch: 1, batch: 93700, loss: 2.78037, acc: 0.83096, time: Fri Jan 21 23:53:46 2022
epoch 1 training:  48%|████▊     | 93799/196351 [13:19:58<14:27:44,  1.97it/s]global step 93800, epoch: 1, batch: 93800, loss: 1.92257, acc: 0.83101, time: Fri Jan 21 23:54:37 2022
epoch 1 training:  48%|████▊     | 93899/196351 [13:20:49<14:23:24,  1.98it/s]global step 93900, epoch: 1, batch: 93900, loss: 2.19959, acc: 0.83105, time: Fri Jan 21 23:55:28 2022
epoch 1 training:  48%|████▊     | 93999/196351 [13:21:40<14:18:31,  1.99it/s]global step 94000, epoch: 1, batch: 94000, loss: 3.03904, acc: 0.83114, time: Fri Jan 21 23:56:19 2022
epoch 1 training:  48%|████▊     | 94099/196351 [13:22:31<14:22:05,  1.98it/s]global step 94100, epoch: 1, batch: 94100, loss: 4.37958, acc: 0.83123, time: Fri Jan 21 23:57:09 2022
epoch 1 training:  48%|████▊     | 94199/196351 [13:23:21<14:18:35,  1.98it/s]global step 94200, epoch: 1, batch: 94200, loss: 0.42881, acc: 0.83130, time: Fri Jan 21 23:58:00 2022
epoch 1 training:  48%|████▊     | 94299/196351 [13:24:12<14:29:14,  1.96it/s]global step 94300, epoch: 1, batch: 94300, loss: 0.89391, acc: 0.83137, time: Fri Jan 21 23:58:51 2022
epoch 1 training:  48%|████▊     | 94399/196351 [13:25:03<14:25:48,  1.96it/s]global step 94400, epoch: 1, batch: 94400, loss: 2.36560, acc: 0.83139, time: Fri Jan 21 23:59:41 2022
epoch 1 training:  48%|████▊     | 94499/196351 [13:25:53<14:13:51,  1.99it/s]global step 94500, epoch: 1, batch: 94500, loss: 1.48696, acc: 0.83144, time: Sat Jan 22 00:00:32 2022
epoch 1 training:  48%|████▊     | 94599/196351 [13:26:44<14:20:16,  1.97it/s]global step 94600, epoch: 1, batch: 94600, loss: 2.49890, acc: 0.83147, time: Sat Jan 22 00:01:23 2022
epoch 1 training:  48%|████▊     | 94699/196351 [13:27:35<14:18:02,  1.97it/s]global step 94700, epoch: 1, batch: 94700, loss: 2.10725, acc: 0.83148, time: Sat Jan 22 00:02:14 2022
epoch 1 training:  48%|████▊     | 94799/196351 [13:28:26<14:11:48,  1.99it/s]global step 94800, epoch: 1, batch: 94800, loss: 3.79612, acc: 0.83156, time: Sat Jan 22 00:03:04 2022
epoch 1 training:  48%|████▊     | 94899/196351 [13:29:16<14:15:33,  1.98it/s]global step 94900, epoch: 1, batch: 94900, loss: 1.55342, acc: 0.83164, time: Sat Jan 22 00:03:55 2022
epoch 1 training:  48%|████▊     | 94999/196351 [13:30:07<14:06:39,  2.00it/s]global step 95000, epoch: 1, batch: 95000, loss: 2.13318, acc: 0.83172, time: Sat Jan 22 00:04:46 2022
epoch 1 training:  48%|████▊     | 95099/196351 [13:30:58<14:21:01,  1.96it/s]global step 95100, epoch: 1, batch: 95100, loss: 4.44977, acc: 0.83180, time: Sat Jan 22 00:05:37 2022
epoch 1 training:  48%|████▊     | 95199/196351 [13:31:49<14:19:01,  1.96it/s]global step 95200, epoch: 1, batch: 95200, loss: 2.41154, acc: 0.83188, time: Sat Jan 22 00:06:28 2022
epoch 1 training:  49%|████▊     | 95299/196351 [13:32:40<14:27:09,  1.94it/s]global step 95300, epoch: 1, batch: 95300, loss: 0.62037, acc: 0.83193, time: Sat Jan 22 00:07:19 2022
epoch 1 training:  49%|████▊     | 95399/196351 [13:33:31<14:30:36,  1.93it/s]global step 95400, epoch: 1, batch: 95400, loss: 3.15715, acc: 0.83203, time: Sat Jan 22 00:08:10 2022
epoch 1 training:  49%|████▊     | 95499/196351 [13:34:22<14:28:25,  1.94it/s]global step 95500, epoch: 1, batch: 95500, loss: 1.10232, acc: 0.83214, time: Sat Jan 22 00:09:01 2022
epoch 1 training:  49%|████▊     | 95599/196351 [13:35:13<14:14:12,  1.97it/s]global step 95600, epoch: 1, batch: 95600, loss: 1.31105, acc: 0.83220, time: Sat Jan 22 00:09:52 2022
epoch 1 training:  49%|████▊     | 95699/196351 [13:36:04<14:25:46,  1.94it/s]global step 95700, epoch: 1, batch: 95700, loss: 0.70753, acc: 0.83226, time: Sat Jan 22 00:10:43 2022
epoch 1 training:  49%|████▉     | 95799/196351 [13:36:55<14:05:52,  1.98it/s]global step 95800, epoch: 1, batch: 95800, loss: 2.83184, acc: 0.83232, time: Sat Jan 22 00:11:34 2022
epoch 1 training:  49%|████▉     | 95899/196351 [13:37:46<14:19:21,  1.95it/s]global step 95900, epoch: 1, batch: 95900, loss: 3.28846, acc: 0.83237, time: Sat Jan 22 00:12:24 2022
epoch 1 training:  49%|████▉     | 95999/196351 [13:38:36<14:07:46,  1.97it/s]global step 96000, epoch: 1, batch: 96000, loss: 2.81338, acc: 0.83246, time: Sat Jan 22 00:13:15 2022
epoch 1 training:  49%|████▉     | 96099/196351 [13:39:27<14:12:41,  1.96it/s]global step 96100, epoch: 1, batch: 96100, loss: 1.36781, acc: 0.83251, time: Sat Jan 22 00:14:06 2022
epoch 1 training:  49%|████▉     | 96199/196351 [13:40:18<13:58:42,  1.99it/s]global step 96200, epoch: 1, batch: 96200, loss: 4.03193, acc: 0.83258, time: Sat Jan 22 00:14:57 2022
epoch 1 training:  49%|████▉     | 96299/196351 [13:41:09<14:08:54,  1.96it/s]global step 96300, epoch: 1, batch: 96300, loss: 2.48532, acc: 0.83258, time: Sat Jan 22 00:15:47 2022
epoch 1 training:  49%|████▉     | 96399/196351 [13:41:59<14:07:29,  1.97it/s]global step 96400, epoch: 1, batch: 96400, loss: 1.24195, acc: 0.83265, time: Sat Jan 22 00:16:38 2022
epoch 1 training:  49%|████▉     | 96499/196351 [13:42:52<14:15:58,  1.94it/s]global step 96500, epoch: 1, batch: 96500, loss: 4.86235, acc: 0.83269, time: Sat Jan 22 00:17:30 2022
epoch 1 training:  49%|████▉     | 96599/196351 [13:43:43<14:00:54,  1.98it/s]global step 96600, epoch: 1, batch: 96600, loss: 1.67625, acc: 0.83277, time: Sat Jan 22 00:18:21 2022
epoch 1 training:  49%|████▉     | 96699/196351 [13:44:34<14:04:48,  1.97it/s]global step 96700, epoch: 1, batch: 96700, loss: 3.62467, acc: 0.83285, time: Sat Jan 22 00:19:12 2022
epoch 1 training:  49%|████▉     | 96799/196351 [13:45:25<14:10:31,  1.95it/s]global step 96800, epoch: 1, batch: 96800, loss: 1.34150, acc: 0.83291, time: Sat Jan 22 00:20:03 2022
epoch 1 training:  49%|████▉     | 96899/196351 [13:46:16<14:11:46,  1.95it/s]global step 96900, epoch: 1, batch: 96900, loss: 3.08461, acc: 0.83298, time: Sat Jan 22 00:20:54 2022
epoch 1 training:  49%|████▉     | 96999/196351 [13:47:06<13:52:06,  1.99it/s]global step 97000, epoch: 1, batch: 97000, loss: 4.08863, acc: 0.83303, time: Sat Jan 22 00:21:45 2022
epoch 1 training:  49%|████▉     | 97099/196351 [13:47:57<14:06:38,  1.95it/s]global step 97100, epoch: 1, batch: 97100, loss: 2.65287, acc: 0.83310, time: Sat Jan 22 00:22:36 2022
epoch 1 training:  50%|████▉     | 97199/196351 [13:48:48<13:56:47,  1.97it/s]global step 97200, epoch: 1, batch: 97200, loss: 2.35957, acc: 0.83315, time: Sat Jan 22 00:23:27 2022
epoch 1 training:  50%|████▉     | 97299/196351 [13:49:39<13:55:56,  1.97it/s]global step 97300, epoch: 1, batch: 97300, loss: 2.57339, acc: 0.83324, time: Sat Jan 22 00:24:18 2022
epoch 1 training:  50%|████▉     | 97399/196351 [13:50:30<14:00:15,  1.96it/s]global step 97400, epoch: 1, batch: 97400, loss: 2.68779, acc: 0.83330, time: Sat Jan 22 00:25:09 2022
epoch 1 training:  50%|████▉     | 97499/196351 [13:51:22<14:01:01,  1.96it/s]global step 97500, epoch: 1, batch: 97500, loss: 0.46937, acc: 0.83338, time: Sat Jan 22 00:26:00 2022
epoch 1 training:  50%|████▉     | 97599/196351 [13:52:12<14:02:35,  1.95it/s]global step 97600, epoch: 1, batch: 97600, loss: 2.98336, acc: 0.83342, time: Sat Jan 22 00:26:51 2022
epoch 1 training:  50%|████▉     | 97699/196351 [13:53:03<13:56:28,  1.97it/s]global step 97700, epoch: 1, batch: 97700, loss: 2.24375, acc: 0.83344, time: Sat Jan 22 00:27:42 2022
epoch 1 training:  50%|████▉     | 97799/196351 [13:53:54<13:49:18,  1.98it/s]global step 97800, epoch: 1, batch: 97800, loss: 3.85041, acc: 0.83354, time: Sat Jan 22 00:28:33 2022
epoch 1 training:  50%|████▉     | 97899/196351 [13:54:44<13:45:56,  1.99it/s]global step 97900, epoch: 1, batch: 97900, loss: 0.91851, acc: 0.83360, time: Sat Jan 22 00:29:23 2022
epoch 1 training:  50%|████▉     | 97999/196351 [13:55:35<13:49:33,  1.98it/s]global step 98000, epoch: 1, batch: 98000, loss: 2.62702, acc: 0.83365, time: Sat Jan 22 00:30:14 2022
epoch 1 training:  50%|████▉     | 98099/196351 [13:56:26<13:47:37,  1.98it/s]global step 98100, epoch: 1, batch: 98100, loss: 0.76512, acc: 0.83371, time: Sat Jan 22 00:31:04 2022
epoch 1 training:  50%|█████     | 98199/196351 [13:57:16<13:42:50,  1.99it/s]global step 98200, epoch: 1, batch: 98200, loss: 3.68812, acc: 0.83376, time: Sat Jan 22 00:31:55 2022
epoch 1 training:  50%|█████     | 98299/196351 [13:58:08<13:49:38,  1.97it/s]global step 98300, epoch: 1, batch: 98300, loss: 1.14265, acc: 0.83387, time: Sat Jan 22 00:32:46 2022
epoch 1 training:  50%|█████     | 98399/196351 [13:58:58<13:37:19,  2.00it/s]global step 98400, epoch: 1, batch: 98400, loss: 2.83902, acc: 0.83392, time: Sat Jan 22 00:33:37 2022
epoch 1 training:  50%|█████     | 98499/196351 [13:59:49<13:51:50,  1.96it/s]global step 98500, epoch: 1, batch: 98500, loss: 4.22052, acc: 0.83397, time: Sat Jan 22 00:34:28 2022
epoch 1 training:  50%|█████     | 98599/196351 [14:00:41<13:49:55,  1.96it/s]global step 98600, epoch: 1, batch: 98600, loss: 2.40471, acc: 0.83404, time: Sat Jan 22 00:35:19 2022
epoch 1 training:  50%|█████     | 98699/196351 [14:01:31<13:32:30,  2.00it/s]global step 98700, epoch: 1, batch: 98700, loss: 2.23724, acc: 0.83409, time: Sat Jan 22 00:36:10 2022
epoch 1 training:  50%|█████     | 98799/196351 [14:02:22<13:47:03,  1.97it/s]global step 98800, epoch: 1, batch: 98800, loss: 2.14691, acc: 0.83413, time: Sat Jan 22 00:37:01 2022
epoch 1 training:  50%|█████     | 98899/196351 [14:03:12<13:55:36,  1.94it/s]global step 98900, epoch: 1, batch: 98900, loss: 1.74262, acc: 0.83420, time: Sat Jan 22 00:37:51 2022
epoch 1 training:  50%|█████     | 98999/196351 [14:04:03<13:58:33,  1.93it/s]global step 99000, epoch: 1, batch: 99000, loss: 0.54434, acc: 0.83428, time: Sat Jan 22 00:38:42 2022
epoch 1 training:  50%|█████     | 99099/196351 [14:04:54<13:41:11,  1.97it/s]global step 99100, epoch: 1, batch: 99100, loss: 0.82218, acc: 0.83431, time: Sat Jan 22 00:39:33 2022
epoch 1 training:  51%|█████     | 99199/196351 [14:05:45<13:43:28,  1.97it/s]global step 99200, epoch: 1, batch: 99200, loss: 3.35717, acc: 0.83435, time: Sat Jan 22 00:40:24 2022
epoch 1 training:  51%|█████     | 99299/196351 [14:06:36<13:34:29,  1.99it/s]global step 99300, epoch: 1, batch: 99300, loss: 2.69835, acc: 0.83441, time: Sat Jan 22 00:41:15 2022
epoch 1 training:  51%|█████     | 99399/196351 [14:07:27<13:40:15,  1.97it/s]global step 99400, epoch: 1, batch: 99400, loss: 1.52126, acc: 0.83450, time: Sat Jan 22 00:42:06 2022
epoch 1 training:  51%|█████     | 99499/196351 [14:08:18<13:35:37,  1.98it/s]global step 99500, epoch: 1, batch: 99500, loss: 0.80736, acc: 0.83457, time: Sat Jan 22 00:42:56 2022
epoch 1 training:  51%|█████     | 99599/196351 [14:09:09<13:41:27,  1.96it/s]global step 99600, epoch: 1, batch: 99600, loss: 4.61868, acc: 0.83464, time: Sat Jan 22 00:43:47 2022
epoch 1 training:  51%|█████     | 99699/196351 [14:09:59<13:30:48,  1.99it/s]global step 99700, epoch: 1, batch: 99700, loss: 1.81449, acc: 0.83472, time: Sat Jan 22 00:44:38 2022
epoch 1 training:  51%|█████     | 99799/196351 [14:10:50<13:26:24,  2.00it/s]global step 99800, epoch: 1, batch: 99800, loss: 4.28097, acc: 0.83479, time: Sat Jan 22 00:45:29 2022
epoch 1 training:  51%|█████     | 99899/196351 [14:11:41<13:34:53,  1.97it/s]global step 99900, epoch: 1, batch: 99900, loss: 2.92410, acc: 0.83483, time: Sat Jan 22 00:46:19 2022
epoch 1 training:  51%|█████     | 99999/196351 [14:12:32<13:37:33,  1.96it/s]global step 100000, epoch: 1, batch: 100000, loss: 3.30992, acc: 0.83488, time: Sat Jan 22 00:47:10 2022
epoch 1 training:  51%|█████     | 100099/196351 [14:13:23<13:37:40,  1.96it/s]global step 100100, epoch: 1, batch: 100100, loss: 1.72919, acc: 0.83495, time: Sat Jan 22 00:48:01 2022
epoch 1 training:  51%|█████     | 100199/196351 [14:14:13<13:39:54,  1.95it/s]global step 100200, epoch: 1, batch: 100200, loss: 2.17882, acc: 0.83500, time: Sat Jan 22 00:48:52 2022
epoch 1 training:  51%|█████     | 100299/196351 [14:15:05<14:15:04,  1.87it/s]global step 100300, epoch: 1, batch: 100300, loss: 2.42014, acc: 0.83504, time: Sat Jan 22 00:49:44 2022
epoch 1 training:  51%|█████     | 100399/196351 [14:15:56<13:22:54,  1.99it/s]global step 100400, epoch: 1, batch: 100400, loss: 3.08816, acc: 0.83510, time: Sat Jan 22 00:50:34 2022
epoch 1 training:  51%|█████     | 100499/196351 [14:16:47<13:29:39,  1.97it/s]global step 100500, epoch: 1, batch: 100500, loss: 1.36163, acc: 0.83517, time: Sat Jan 22 00:51:25 2022
epoch 1 training:  51%|█████     | 100599/196351 [14:17:38<13:44:40,  1.94it/s]global step 100600, epoch: 1, batch: 100600, loss: 4.65639, acc: 0.83523, time: Sat Jan 22 00:52:16 2022
epoch 1 training:  51%|█████▏    | 100699/196351 [14:18:28<13:18:11,  2.00it/s]global step 100700, epoch: 1, batch: 100700, loss: 2.02391, acc: 0.83527, time: Sat Jan 22 00:53:07 2022
epoch 1 training:  51%|█████▏    | 100799/196351 [14:19:19<13:22:50,  1.98it/s]global step 100800, epoch: 1, batch: 100800, loss: 2.05934, acc: 0.83530, time: Sat Jan 22 00:53:58 2022
epoch 1 training:  51%|█████▏    | 100899/196351 [14:20:10<13:26:33,  1.97it/s]global step 100900, epoch: 1, batch: 100900, loss: 1.83133, acc: 0.83537, time: Sat Jan 22 00:54:49 2022
epoch 1 training:  51%|█████▏    | 100999/196351 [14:21:01<13:19:49,  1.99it/s]global step 101000, epoch: 1, batch: 101000, loss: 0.24192, acc: 0.83545, time: Sat Jan 22 00:55:39 2022
epoch 1 training:  51%|█████▏    | 101099/196351 [14:21:51<13:28:05,  1.96it/s]global step 101100, epoch: 1, batch: 101100, loss: 3.37555, acc: 0.83549, time: Sat Jan 22 00:56:30 2022
epoch 1 training:  52%|█████▏    | 101199/196351 [14:22:42<13:20:53,  1.98it/s]global step 101200, epoch: 1, batch: 101200, loss: 0.92851, acc: 0.83558, time: Sat Jan 22 00:57:21 2022
epoch 1 training:  52%|█████▏    | 101299/196351 [14:23:33<13:20:13,  1.98it/s]global step 101300, epoch: 1, batch: 101300, loss: 1.44426, acc: 0.83562, time: Sat Jan 22 00:58:11 2022
epoch 1 training:  52%|█████▏    | 101399/196351 [14:24:24<13:25:32,  1.96it/s]global step 101400, epoch: 1, batch: 101400, loss: 2.95948, acc: 0.83571, time: Sat Jan 22 00:59:03 2022
epoch 1 training:  52%|█████▏    | 101499/196351 [14:25:15<13:24:26,  1.97it/s]global step 101500, epoch: 1, batch: 101500, loss: 0.30844, acc: 0.83581, time: Sat Jan 22 00:59:54 2022
epoch 1 training:  52%|█████▏    | 101599/196351 [14:26:06<13:26:27,  1.96it/s]global step 101600, epoch: 1, batch: 101600, loss: 0.95236, acc: 0.83586, time: Sat Jan 22 01:00:45 2022
epoch 1 training:  52%|█████▏    | 101699/196351 [14:26:57<13:32:26,  1.94it/s]global step 101700, epoch: 1, batch: 101700, loss: 2.28862, acc: 0.83594, time: Sat Jan 22 01:01:36 2022
epoch 1 training:  52%|█████▏    | 101799/196351 [14:27:48<13:32:48,  1.94it/s]global step 101800, epoch: 1, batch: 101800, loss: 0.99342, acc: 0.83602, time: Sat Jan 22 01:02:26 2022
epoch 1 training:  52%|█████▏    | 101899/196351 [14:28:39<13:13:53,  1.98it/s]global step 101900, epoch: 1, batch: 101900, loss: 1.21712, acc: 0.83607, time: Sat Jan 22 01:03:17 2022
epoch 1 training:  52%|█████▏    | 101999/196351 [14:29:30<13:14:08,  1.98it/s]global step 102000, epoch: 1, batch: 102000, loss: 0.67786, acc: 0.83613, time: Sat Jan 22 01:04:08 2022
epoch 1 training:  52%|█████▏    | 102099/196351 [14:30:21<13:33:37,  1.93it/s]global step 102100, epoch: 1, batch: 102100, loss: 1.49654, acc: 0.83621, time: Sat Jan 22 01:04:59 2022
epoch 1 training:  52%|█████▏    | 102199/196351 [14:31:11<13:14:59,  1.97it/s]global step 102200, epoch: 1, batch: 102200, loss: 0.57870, acc: 0.83625, time: Sat Jan 22 01:05:50 2022
epoch 1 training:  52%|█████▏    | 102299/196351 [14:32:02<13:19:04,  1.96it/s]global step 102300, epoch: 1, batch: 102300, loss: 3.59238, acc: 0.83630, time: Sat Jan 22 01:06:41 2022
epoch 1 training:  52%|█████▏    | 102399/196351 [14:32:53<13:13:05,  1.97it/s]global step 102400, epoch: 1, batch: 102400, loss: 3.22875, acc: 0.83633, time: Sat Jan 22 01:07:31 2022
epoch 1 training:  52%|█████▏    | 102499/196351 [14:33:43<13:09:55,  1.98it/s]global step 102500, epoch: 1, batch: 102500, loss: 1.68684, acc: 0.83634, time: Sat Jan 22 01:08:22 2022
epoch 1 training:  52%|█████▏    | 102599/196351 [14:34:34<13:16:23,  1.96it/s]global step 102600, epoch: 1, batch: 102600, loss: 3.47831, acc: 0.83639, time: Sat Jan 22 01:09:13 2022
epoch 1 training:  52%|█████▏    | 102699/196351 [14:35:25<13:03:50,  1.99it/s]global step 102700, epoch: 1, batch: 102700, loss: 4.28954, acc: 0.83646, time: Sat Jan 22 01:10:03 2022
epoch 1 training:  52%|█████▏    | 102799/196351 [14:36:15<12:57:41,  2.00it/s]global step 102800, epoch: 1, batch: 102800, loss: 1.82522, acc: 0.83647, time: Sat Jan 22 01:10:54 2022
epoch 1 training:  52%|█████▏    | 102899/196351 [14:37:06<13:17:37,  1.95it/s]global step 102900, epoch: 1, batch: 102900, loss: 1.99902, acc: 0.83655, time: Sat Jan 22 01:11:45 2022
epoch 1 training:  52%|█████▏    | 102999/196351 [14:37:57<13:21:37,  1.94it/s]global step 103000, epoch: 1, batch: 103000, loss: 1.87287, acc: 0.83659, time: Sat Jan 22 01:12:35 2022
epoch 1 training:  53%|█████▎    | 103099/196351 [14:38:48<13:10:21,  1.97it/s]global step 103100, epoch: 1, batch: 103100, loss: 3.09596, acc: 0.83661, time: Sat Jan 22 01:13:26 2022
epoch 1 training:  53%|█████▎    | 103199/196351 [14:39:39<13:26:39,  1.92it/s]global step 103200, epoch: 1, batch: 103200, loss: 1.87285, acc: 0.83668, time: Sat Jan 22 01:14:17 2022
epoch 1 training:  53%|█████▎    | 103299/196351 [14:40:30<13:14:00,  1.95it/s]global step 103300, epoch: 1, batch: 103300, loss: 0.98136, acc: 0.83672, time: Sat Jan 22 01:15:08 2022
epoch 1 training:  53%|█████▎    | 103399/196351 [14:41:21<13:22:24,  1.93it/s]global step 103400, epoch: 1, batch: 103400, loss: 1.12693, acc: 0.83676, time: Sat Jan 22 01:15:59 2022
epoch 1 training:  53%|█████▎    | 103499/196351 [14:42:12<13:10:54,  1.96it/s]global step 103500, epoch: 1, batch: 103500, loss: 2.58511, acc: 0.83684, time: Sat Jan 22 01:16:50 2022
epoch 1 training:  53%|█████▎    | 103599/196351 [14:43:03<13:03:57,  1.97it/s]global step 103600, epoch: 1, batch: 103600, loss: 3.56793, acc: 0.83688, time: Sat Jan 22 01:17:42 2022
epoch 1 training:  53%|█████▎    | 103699/196351 [14:43:54<13:02:48,  1.97it/s]global step 103700, epoch: 1, batch: 103700, loss: 0.95837, acc: 0.83691, time: Sat Jan 22 01:18:32 2022
epoch 1 training:  53%|█████▎    | 103799/196351 [14:44:44<13:03:54,  1.97it/s]global step 103800, epoch: 1, batch: 103800, loss: 0.66855, acc: 0.83698, time: Sat Jan 22 01:19:23 2022
epoch 1 training:  53%|█████▎    | 103899/196351 [14:45:35<12:55:57,  1.99it/s]global step 103900, epoch: 1, batch: 103900, loss: 0.63555, acc: 0.83704, time: Sat Jan 22 01:20:14 2022
epoch 1 training:  53%|█████▎    | 103999/196351 [14:46:26<13:00:42,  1.97it/s]global step 104000, epoch: 1, batch: 104000, loss: 2.84183, acc: 0.83708, time: Sat Jan 22 01:21:05 2022
epoch 1 training:  53%|█████▎    | 104099/196351 [14:47:17<12:58:39,  1.97it/s]global step 104100, epoch: 1, batch: 104100, loss: 4.48043, acc: 0.83712, time: Sat Jan 22 01:21:55 2022
epoch 1 training:  53%|█████▎    | 104199/196351 [14:48:08<13:01:12,  1.97it/s]global step 104200, epoch: 1, batch: 104200, loss: 3.30349, acc: 0.83715, time: Sat Jan 22 01:22:47 2022
epoch 1 training:  53%|█████▎    | 104299/196351 [14:48:59<12:53:40,  1.98it/s]global step 104300, epoch: 1, batch: 104300, loss: 0.98058, acc: 0.83719, time: Sat Jan 22 01:23:38 2022
epoch 1 training:  53%|█████▎    | 104399/196351 [14:49:50<12:58:03,  1.97it/s]global step 104400, epoch: 1, batch: 104400, loss: 1.30560, acc: 0.83727, time: Sat Jan 22 01:24:29 2022
epoch 1 training:  53%|█████▎    | 104499/196351 [14:50:41<12:48:52,  1.99it/s]global step 104500, epoch: 1, batch: 104500, loss: 1.31667, acc: 0.83730, time: Sat Jan 22 01:25:20 2022
epoch 1 training:  53%|█████▎    | 104599/196351 [14:51:32<12:56:49,  1.97it/s]global step 104600, epoch: 1, batch: 104600, loss: 2.16337, acc: 0.83735, time: Sat Jan 22 01:26:11 2022
epoch 1 training:  53%|█████▎    | 104699/196351 [14:52:23<13:06:03,  1.94it/s]global step 104700, epoch: 1, batch: 104700, loss: 2.64020, acc: 0.83740, time: Sat Jan 22 01:27:02 2022
epoch 1 training:  53%|█████▎    | 104799/196351 [14:53:14<13:02:11,  1.95it/s]global step 104800, epoch: 1, batch: 104800, loss: 2.65488, acc: 0.83745, time: Sat Jan 22 01:27:53 2022
epoch 1 training:  53%|█████▎    | 104899/196351 [14:54:05<13:05:04,  1.94it/s]global step 104900, epoch: 1, batch: 104900, loss: 0.63281, acc: 0.83749, time: Sat Jan 22 01:28:44 2022
epoch 1 training:  53%|█████▎    | 104999/196351 [14:54:56<12:51:05,  1.97it/s]global step 105000, epoch: 1, batch: 105000, loss: 2.05290, acc: 0.83759, time: Sat Jan 22 01:29:35 2022
epoch 1 training:  54%|█████▎    | 105099/196351 [14:55:48<12:52:35,  1.97it/s]global step 105100, epoch: 1, batch: 105100, loss: 2.28993, acc: 0.83762, time: Sat Jan 22 01:30:26 2022
epoch 1 training:  54%|█████▎    | 105199/196351 [14:56:39<12:46:01,  1.98it/s]global step 105200, epoch: 1, batch: 105200, loss: 1.55951, acc: 0.83769, time: Sat Jan 22 01:31:17 2022
epoch 1 training:  54%|█████▎    | 105299/196351 [14:57:30<13:02:19,  1.94it/s]global step 105300, epoch: 1, batch: 105300, loss: 1.49236, acc: 0.83771, time: Sat Jan 22 01:32:09 2022
epoch 1 training:  54%|█████▎    | 105399/196351 [14:58:21<13:05:16,  1.93it/s]global step 105400, epoch: 1, batch: 105400, loss: 4.95101, acc: 0.83780, time: Sat Jan 22 01:33:00 2022
epoch 1 training:  54%|█████▎    | 105499/196351 [14:59:12<12:50:40,  1.96it/s]global step 105500, epoch: 1, batch: 105500, loss: 1.96460, acc: 0.83787, time: Sat Jan 22 01:33:51 2022
epoch 1 training:  54%|█████▍    | 105599/196351 [15:00:03<12:58:32,  1.94it/s]global step 105600, epoch: 1, batch: 105600, loss: 0.81121, acc: 0.83793, time: Sat Jan 22 01:34:42 2022
epoch 1 training:  54%|█████▍    | 105699/196351 [15:00:54<12:48:21,  1.97it/s]global step 105700, epoch: 1, batch: 105700, loss: 2.32779, acc: 0.83796, time: Sat Jan 22 01:35:32 2022
epoch 1 training:  54%|█████▍    | 105799/196351 [15:01:44<12:43:44,  1.98it/s]global step 105800, epoch: 1, batch: 105800, loss: 2.30800, acc: 0.83804, time: Sat Jan 22 01:36:23 2022
epoch 1 training:  54%|█████▍    | 105899/196351 [15:02:35<12:40:16,  1.98it/s]global step 105900, epoch: 1, batch: 105900, loss: 1.77179, acc: 0.83813, time: Sat Jan 22 01:37:14 2022
epoch 1 training:  54%|█████▍    | 105999/196351 [15:03:25<12:43:20,  1.97it/s]global step 106000, epoch: 1, batch: 106000, loss: 0.48389, acc: 0.83822, time: Sat Jan 22 01:38:04 2022
epoch 1 training:  54%|█████▍    | 106099/196351 [15:04:16<12:37:42,  1.99it/s]global step 106100, epoch: 1, batch: 106100, loss: 1.46626, acc: 0.83828, time: Sat Jan 22 01:38:54 2022
epoch 1 training:  54%|█████▍    | 106199/196351 [15:05:06<12:38:59,  1.98it/s]global step 106200, epoch: 1, batch: 106200, loss: 0.76245, acc: 0.83835, time: Sat Jan 22 01:39:45 2022
epoch 1 training:  54%|█████▍    | 106299/196351 [15:05:58<12:56:27,  1.93it/s]global step 106300, epoch: 1, batch: 106300, loss: 1.82942, acc: 0.83841, time: Sat Jan 22 01:40:36 2022
epoch 1 training:  54%|█████▍    | 106399/196351 [15:06:49<12:49:04,  1.95it/s]global step 106400, epoch: 1, batch: 106400, loss: 0.85444, acc: 0.83843, time: Sat Jan 22 01:41:28 2022
epoch 1 training:  54%|█████▍    | 106499/196351 [15:07:40<12:50:21,  1.94it/s]global step 106500, epoch: 1, batch: 106500, loss: 0.81007, acc: 0.83846, time: Sat Jan 22 01:42:19 2022
epoch 1 training:  54%|█████▍    | 106599/196351 [15:08:30<12:43:54,  1.96it/s]global step 106600, epoch: 1, batch: 106600, loss: 3.32991, acc: 0.83852, time: Sat Jan 22 01:43:09 2022
epoch 1 training:  54%|█████▍    | 106699/196351 [15:09:21<12:36:29,  1.98it/s]global step 106700, epoch: 1, batch: 106700, loss: 1.90204, acc: 0.83857, time: Sat Jan 22 01:44:00 2022
epoch 1 training:  54%|█████▍    | 106799/196351 [15:10:12<12:39:12,  1.97it/s]global step 106800, epoch: 1, batch: 106800, loss: 3.07478, acc: 0.83861, time: Sat Jan 22 01:44:51 2022
epoch 1 training:  54%|█████▍    | 106899/196351 [15:11:03<12:42:25,  1.96it/s]global step 106900, epoch: 1, batch: 106900, loss: 0.74323, acc: 0.83868, time: Sat Jan 22 01:45:42 2022
epoch 1 training:  54%|█████▍    | 106999/196351 [15:11:54<12:33:34,  1.98it/s]global step 107000, epoch: 1, batch: 107000, loss: 3.62584, acc: 0.83874, time: Sat Jan 22 01:46:33 2022
epoch 1 training:  55%|█████▍    | 107099/196351 [15:12:45<12:27:29,  1.99it/s]global step 107100, epoch: 1, batch: 107100, loss: 1.08337, acc: 0.83880, time: Sat Jan 22 01:47:24 2022
epoch 1 training:  55%|█████▍    | 107199/196351 [15:13:36<12:43:51,  1.95it/s]global step 107200, epoch: 1, batch: 107200, loss: 0.68167, acc: 0.83888, time: Sat Jan 22 01:48:15 2022
epoch 1 training:  55%|█████▍    | 107299/196351 [15:14:27<12:48:23,  1.93it/s]global step 107300, epoch: 1, batch: 107300, loss: 2.11135, acc: 0.83894, time: Sat Jan 22 01:49:06 2022
epoch 1 training:  55%|█████▍    | 107399/196351 [15:15:18<12:29:48,  1.98it/s]global step 107400, epoch: 1, batch: 107400, loss: 1.79079, acc: 0.83900, time: Sat Jan 22 01:49:57 2022
epoch 1 training:  55%|█████▍    | 107499/196351 [15:16:09<12:30:19,  1.97it/s]global step 107500, epoch: 1, batch: 107500, loss: 3.54908, acc: 0.83907, time: Sat Jan 22 01:50:47 2022
epoch 1 training:  55%|█████▍    | 107599/196351 [15:16:59<12:27:05,  1.98it/s]global step 107600, epoch: 1, batch: 107600, loss: 0.96365, acc: 0.83912, time: Sat Jan 22 01:51:38 2022
epoch 1 training:  55%|█████▍    | 107699/196351 [15:17:50<12:16:45,  2.01it/s]global step 107700, epoch: 1, batch: 107700, loss: 1.63341, acc: 0.83916, time: Sat Jan 22 01:52:29 2022
epoch 1 training:  55%|█████▍    | 107799/196351 [15:18:41<12:23:33,  1.98it/s]global step 107800, epoch: 1, batch: 107800, loss: 2.86365, acc: 0.83920, time: Sat Jan 22 01:53:19 2022
epoch 1 training:  55%|█████▍    | 107899/196351 [15:19:32<12:35:47,  1.95it/s]global step 107900, epoch: 1, batch: 107900, loss: 3.47412, acc: 0.83922, time: Sat Jan 22 01:54:10 2022
epoch 1 training:  55%|█████▌    | 107999/196351 [15:20:23<12:27:24,  1.97it/s]global step 108000, epoch: 1, batch: 108000, loss: 2.71441, acc: 0.83923, time: Sat Jan 22 01:55:02 2022
epoch 1 training:  55%|█████▌    | 108099/196351 [15:21:14<12:33:06,  1.95it/s]global step 108100, epoch: 1, batch: 108100, loss: 1.85819, acc: 0.83930, time: Sat Jan 22 01:55:53 2022
epoch 1 training:  55%|█████▌    | 108199/196351 [15:22:05<12:28:40,  1.96it/s]global step 108200, epoch: 1, batch: 108200, loss: 2.33640, acc: 0.83937, time: Sat Jan 22 01:56:44 2022
epoch 1 training:  55%|█████▌    | 108299/196351 [15:22:56<12:21:29,  1.98it/s]global step 108300, epoch: 1, batch: 108300, loss: 1.96221, acc: 0.83941, time: Sat Jan 22 01:57:35 2022
epoch 1 training:  55%|█████▌    | 108399/196351 [15:23:47<12:28:59,  1.96it/s]global step 108400, epoch: 1, batch: 108400, loss: 2.12742, acc: 0.83946, time: Sat Jan 22 01:58:26 2022
epoch 1 training:  55%|█████▌    | 108499/196351 [15:24:38<12:35:27,  1.94it/s]global step 108500, epoch: 1, batch: 108500, loss: 2.27240, acc: 0.83948, time: Sat Jan 22 01:59:17 2022
epoch 1 training:  55%|█████▌    | 108599/196351 [15:25:29<12:30:59,  1.95it/s]global step 108600, epoch: 1, batch: 108600, loss: 3.01675, acc: 0.83954, time: Sat Jan 22 02:00:08 2022
epoch 1 training:  55%|█████▌    | 108699/196351 [15:26:20<12:11:50,  2.00it/s]global step 108700, epoch: 1, batch: 108700, loss: 1.28484, acc: 0.83958, time: Sat Jan 22 02:00:59 2022
epoch 1 training:  55%|█████▌    | 108799/196351 [15:27:11<12:17:27,  1.98it/s]global step 108800, epoch: 1, batch: 108800, loss: 1.12502, acc: 0.83963, time: Sat Jan 22 02:01:50 2022
epoch 1 training:  55%|█████▌    | 108899/196351 [15:28:02<12:22:33,  1.96it/s]global step 108900, epoch: 1, batch: 108900, loss: 1.20074, acc: 0.83969, time: Sat Jan 22 02:02:41 2022
epoch 1 training:  56%|█████▌    | 108999/196351 [15:28:53<12:24:43,  1.95it/s]global step 109000, epoch: 1, batch: 109000, loss: 0.62031, acc: 0.83972, time: Sat Jan 22 02:03:32 2022
epoch 1 training:  56%|█████▌    | 109099/196351 [15:29:45<12:13:55,  1.98it/s]global step 109100, epoch: 1, batch: 109100, loss: 3.87682, acc: 0.83975, time: Sat Jan 22 02:04:23 2022
epoch 1 training:  56%|█████▌    | 109199/196351 [15:30:35<12:07:52,  2.00it/s]global step 109200, epoch: 1, batch: 109200, loss: 3.91421, acc: 0.83983, time: Sat Jan 22 02:05:14 2022
epoch 1 training:  56%|█████▌    | 109299/196351 [15:31:26<12:11:03,  1.98it/s]global step 109300, epoch: 1, batch: 109300, loss: 2.77072, acc: 0.83986, time: Sat Jan 22 02:06:05 2022
epoch 1 training:  56%|█████▌    | 109399/196351 [15:32:17<12:15:57,  1.97it/s]global step 109400, epoch: 1, batch: 109400, loss: 1.17561, acc: 0.83989, time: Sat Jan 22 02:06:56 2022
epoch 1 training:  56%|█████▌    | 109499/196351 [15:33:08<12:18:36,  1.96it/s]global step 109500, epoch: 1, batch: 109500, loss: 0.34565, acc: 0.83995, time: Sat Jan 22 02:07:46 2022
epoch 1 training:  56%|█████▌    | 109599/196351 [15:33:59<12:15:10,  1.97it/s]global step 109600, epoch: 1, batch: 109600, loss: 2.27732, acc: 0.84002, time: Sat Jan 22 02:08:37 2022
epoch 1 training:  56%|█████▌    | 109699/196351 [15:34:50<12:07:45,  1.98it/s]global step 109700, epoch: 1, batch: 109700, loss: 0.29617, acc: 0.84009, time: Sat Jan 22 02:09:29 2022
epoch 1 training:  56%|█████▌    | 109799/196351 [15:35:41<12:26:34,  1.93it/s]global step 109800, epoch: 1, batch: 109800, loss: 1.08574, acc: 0.84013, time: Sat Jan 22 02:10:19 2022
epoch 1 training:  56%|█████▌    | 109899/196351 [15:36:32<12:35:34,  1.91it/s]global step 109900, epoch: 1, batch: 109900, loss: 2.80132, acc: 0.84020, time: Sat Jan 22 02:11:11 2022
epoch 1 training:  56%|█████▌    | 109999/196351 [15:37:23<12:19:19,  1.95it/s]global step 110000, epoch: 1, batch: 110000, loss: 2.01436, acc: 0.84023, time: Sat Jan 22 02:12:02 2022
epoch 1 training:  56%|█████▌    | 110099/196351 [15:38:14<12:11:16,  1.97it/s]global step 110100, epoch: 1, batch: 110100, loss: 0.60371, acc: 0.84028, time: Sat Jan 22 02:12:53 2022
epoch 1 training:  56%|█████▌    | 110199/196351 [15:39:05<12:10:00,  1.97it/s]global step 110200, epoch: 1, batch: 110200, loss: 3.44406, acc: 0.84031, time: Sat Jan 22 02:13:44 2022
epoch 1 training:  56%|█████▌    | 110299/196351 [15:39:56<12:15:04,  1.95it/s]global step 110300, epoch: 1, batch: 110300, loss: 2.30465, acc: 0.84039, time: Sat Jan 22 02:14:35 2022
epoch 1 training:  56%|█████▌    | 110399/196351 [15:40:47<12:05:56,  1.97it/s]global step 110400, epoch: 1, batch: 110400, loss: 2.17462, acc: 0.84047, time: Sat Jan 22 02:15:26 2022
epoch 1 training:  56%|█████▋    | 110499/196351 [15:41:39<12:02:08,  1.98it/s]global step 110500, epoch: 1, batch: 110500, loss: 1.13557, acc: 0.84053, time: Sat Jan 22 02:16:17 2022
epoch 1 training:  56%|█████▋    | 110599/196351 [15:42:29<12:08:52,  1.96it/s]global step 110600, epoch: 1, batch: 110600, loss: 1.86657, acc: 0.84059, time: Sat Jan 22 02:17:08 2022
epoch 1 training:  56%|█████▋    | 110699/196351 [15:43:20<12:02:20,  1.98it/s]global step 110700, epoch: 1, batch: 110700, loss: 1.00645, acc: 0.84063, time: Sat Jan 22 02:17:59 2022
epoch 1 training:  56%|█████▋    | 110799/196351 [15:44:11<12:14:50,  1.94it/s]global step 110800, epoch: 1, batch: 110800, loss: 2.91674, acc: 0.84065, time: Sat Jan 22 02:18:50 2022
epoch 1 training:  56%|█████▋    | 110899/196351 [15:45:02<11:56:48,  1.99it/s]global step 110900, epoch: 1, batch: 110900, loss: 2.10216, acc: 0.84068, time: Sat Jan 22 02:19:41 2022
epoch 1 training:  57%|█████▋    | 110999/196351 [15:45:53<11:58:06,  1.98it/s]global step 111000, epoch: 1, batch: 111000, loss: 1.47446, acc: 0.84069, time: Sat Jan 22 02:20:32 2022
epoch 1 training:  57%|█████▋    | 111099/196351 [15:46:44<11:52:08,  2.00it/s]global step 111100, epoch: 1, batch: 111100, loss: 0.96389, acc: 0.84077, time: Sat Jan 22 02:21:22 2022
epoch 1 training:  57%|█████▋    | 111199/196351 [15:47:34<11:48:07,  2.00it/s]global step 111200, epoch: 1, batch: 111200, loss: 2.43762, acc: 0.84084, time: Sat Jan 22 02:22:13 2022
epoch 1 training:  57%|█████▋    | 111299/196351 [15:48:25<11:53:21,  1.99it/s]global step 111300, epoch: 1, batch: 111300, loss: 2.80104, acc: 0.84088, time: Sat Jan 22 02:23:03 2022
epoch 1 training:  57%|█████▋    | 111399/196351 [15:49:15<12:00:49,  1.96it/s]global step 111400, epoch: 1, batch: 111400, loss: 3.57110, acc: 0.84092, time: Sat Jan 22 02:23:54 2022
epoch 1 training:  57%|█████▋    | 111499/196351 [15:50:06<11:50:38,  1.99it/s]global step 111500, epoch: 1, batch: 111500, loss: 0.75615, acc: 0.84100, time: Sat Jan 22 02:24:45 2022
epoch 1 training:  57%|█████▋    | 111599/196351 [15:50:57<11:58:14,  1.97it/s]global step 111600, epoch: 1, batch: 111600, loss: 0.80847, acc: 0.84108, time: Sat Jan 22 02:25:35 2022
epoch 1 training:  57%|█████▋    | 111699/196351 [15:51:47<12:02:04,  1.95it/s]global step 111700, epoch: 1, batch: 111700, loss: 0.62033, acc: 0.84111, time: Sat Jan 22 02:26:26 2022
epoch 1 training:  57%|█████▋    | 111799/196351 [15:52:38<12:03:18,  1.95it/s]global step 111800, epoch: 1, batch: 111800, loss: 2.98556, acc: 0.84117, time: Sat Jan 22 02:27:17 2022
epoch 1 training:  57%|█████▋    | 111899/196351 [15:53:30<12:04:35,  1.94it/s]global step 111900, epoch: 1, batch: 111900, loss: 3.09611, acc: 0.84120, time: Sat Jan 22 02:28:09 2022
epoch 1 training:  57%|█████▋    | 111999/196351 [15:54:20<11:51:44,  1.98it/s]global step 112000, epoch: 1, batch: 112000, loss: 3.23300, acc: 0.84125, time: Sat Jan 22 02:28:59 2022
epoch 1 training:  57%|█████▋    | 112099/196351 [15:55:11<11:54:37,  1.96it/s]global step 112100, epoch: 1, batch: 112100, loss: 3.17215, acc: 0.84128, time: Sat Jan 22 02:29:50 2022
epoch 1 training:  57%|█████▋    | 112199/196351 [15:56:02<11:45:08,  1.99it/s]global step 112200, epoch: 1, batch: 112200, loss: 2.29180, acc: 0.84130, time: Sat Jan 22 02:30:41 2022
epoch 1 training:  57%|█████▋    | 112299/196351 [15:56:53<11:52:46,  1.97it/s]global step 112300, epoch: 1, batch: 112300, loss: 0.37531, acc: 0.84133, time: Sat Jan 22 02:31:31 2022
epoch 1 training:  57%|█████▋    | 112399/196351 [15:57:43<11:45:59,  1.98it/s]global step 112400, epoch: 1, batch: 112400, loss: 3.39669, acc: 0.84135, time: Sat Jan 22 02:32:22 2022
epoch 1 training:  57%|█████▋    | 112499/196351 [15:58:34<11:42:11,  1.99it/s]global step 112500, epoch: 1, batch: 112500, loss: 4.75053, acc: 0.84138, time: Sat Jan 22 02:33:12 2022
epoch 1 training:  57%|█████▋    | 112599/196351 [15:59:24<11:33:05,  2.01it/s]global step 112600, epoch: 1, batch: 112600, loss: 2.82372, acc: 0.84146, time: Sat Jan 22 02:34:03 2022
epoch 1 training:  57%|█████▋    | 112699/196351 [16:00:14<11:40:25,  1.99it/s]global step 112700, epoch: 1, batch: 112700, loss: 3.11437, acc: 0.84151, time: Sat Jan 22 02:34:53 2022
epoch 1 training:  57%|█████▋    | 112799/196351 [16:01:05<11:50:39,  1.96it/s]global step 112800, epoch: 1, batch: 112800, loss: 2.46346, acc: 0.84155, time: Sat Jan 22 02:35:44 2022
epoch 1 training:  57%|█████▋    | 112899/196351 [16:01:56<11:49:07,  1.96it/s]global step 112900, epoch: 1, batch: 112900, loss: 1.23964, acc: 0.84158, time: Sat Jan 22 02:36:35 2022
epoch 1 training:  58%|█████▊    | 112999/196351 [16:02:47<11:51:45,  1.95it/s]global step 113000, epoch: 1, batch: 113000, loss: 3.07445, acc: 0.84164, time: Sat Jan 22 02:37:26 2022
epoch 1 training:  58%|█████▊    | 113099/196351 [16:03:38<11:40:25,  1.98it/s]global step 113100, epoch: 1, batch: 113100, loss: 2.34541, acc: 0.84169, time: Sat Jan 22 02:38:17 2022
epoch 1 training:  58%|█████▊    | 113199/196351 [16:04:29<11:44:56,  1.97it/s]global step 113200, epoch: 1, batch: 113200, loss: 1.67123, acc: 0.84171, time: Sat Jan 22 02:39:08 2022
epoch 1 training:  58%|█████▊    | 113299/196351 [16:05:20<11:45:54,  1.96it/s]global step 113300, epoch: 1, batch: 113300, loss: 2.08146, acc: 0.84172, time: Sat Jan 22 02:39:59 2022
epoch 1 training:  58%|█████▊    | 113399/196351 [16:06:11<11:36:25,  1.99it/s]global step 113400, epoch: 1, batch: 113400, loss: 2.85237, acc: 0.84176, time: Sat Jan 22 02:40:50 2022
epoch 1 training:  58%|█████▊    | 113499/196351 [16:07:02<11:44:55,  1.96it/s]global step 113500, epoch: 1, batch: 113500, loss: 1.06196, acc: 0.84179, time: Sat Jan 22 02:41:41 2022
epoch 1 training:  58%|█████▊    | 113599/196351 [16:07:53<11:47:20,  1.95it/s]global step 113600, epoch: 1, batch: 113600, loss: 0.71793, acc: 0.84186, time: Sat Jan 22 02:42:31 2022
epoch 1 training:  58%|█████▊    | 113699/196351 [16:08:43<11:45:02,  1.95it/s]global step 113700, epoch: 1, batch: 113700, loss: 0.50619, acc: 0.84192, time: Sat Jan 22 02:43:22 2022
epoch 1 training:  58%|█████▊    | 113799/196351 [16:09:34<11:28:13,  2.00it/s]global step 113800, epoch: 1, batch: 113800, loss: 1.61614, acc: 0.84199, time: Sat Jan 22 02:44:13 2022
epoch 1 training:  58%|█████▊    | 113899/196351 [16:10:25<11:38:46,  1.97it/s]global step 113900, epoch: 1, batch: 113900, loss: 2.64509, acc: 0.84203, time: Sat Jan 22 02:45:03 2022
epoch 1 training:  58%|█████▊    | 113999/196351 [16:11:16<11:27:48,  2.00it/s]global step 114000, epoch: 1, batch: 114000, loss: 1.72446, acc: 0.84211, time: Sat Jan 22 02:45:55 2022
epoch 1 training:  58%|█████▊    | 114099/196351 [16:12:06<11:30:30,  1.99it/s]global step 114100, epoch: 1, batch: 114100, loss: 2.23064, acc: 0.84213, time: Sat Jan 22 02:46:45 2022
epoch 1 training:  58%|█████▊    | 114199/196351 [16:12:57<11:27:22,  1.99it/s]global step 114200, epoch: 1, batch: 114200, loss: 1.70178, acc: 0.84215, time: Sat Jan 22 02:47:36 2022
epoch 1 training:  58%|█████▊    | 114299/196351 [16:13:47<11:27:47,  1.99it/s]global step 114300, epoch: 1, batch: 114300, loss: 1.01835, acc: 0.84217, time: Sat Jan 22 02:48:26 2022
epoch 1 training:  58%|█████▊    | 114399/196351 [16:14:38<11:34:13,  1.97it/s]global step 114400, epoch: 1, batch: 114400, loss: 2.64807, acc: 0.84222, time: Sat Jan 22 02:49:17 2022
epoch 1 training:  58%|█████▊    | 114499/196351 [16:15:29<11:33:49,  1.97it/s]global step 114500, epoch: 1, batch: 114500, loss: 3.28621, acc: 0.84227, time: Sat Jan 22 02:50:07 2022
epoch 1 training:  58%|█████▊    | 114599/196351 [16:16:20<11:32:01,  1.97it/s]global step 114600, epoch: 1, batch: 114600, loss: 1.16151, acc: 0.84229, time: Sat Jan 22 02:50:58 2022
epoch 1 training:  58%|█████▊    | 114699/196351 [16:17:11<11:40:52,  1.94it/s]global step 114700, epoch: 1, batch: 114700, loss: 3.89224, acc: 0.84236, time: Sat Jan 22 02:51:49 2022
epoch 1 training:  58%|█████▊    | 114799/196351 [16:18:02<11:34:06,  1.96it/s]global step 114800, epoch: 1, batch: 114800, loss: 2.46396, acc: 0.84242, time: Sat Jan 22 02:52:40 2022
epoch 1 training:  59%|█████▊    | 114899/196351 [16:18:52<11:25:43,  1.98it/s]global step 114900, epoch: 1, batch: 114900, loss: 1.78356, acc: 0.84248, time: Sat Jan 22 02:53:31 2022
epoch 1 training:  59%|█████▊    | 114999/196351 [16:19:43<11:38:46,  1.94it/s]global step 115000, epoch: 1, batch: 115000, loss: 2.74574, acc: 0.84251, time: Sat Jan 22 02:54:22 2022
epoch 1 training:  59%|█████▊    | 115099/196351 [16:20:34<11:40:38,  1.93it/s]global step 115100, epoch: 1, batch: 115100, loss: 1.99988, acc: 0.84258, time: Sat Jan 22 02:55:13 2022
epoch 1 training:  59%|█████▊    | 115199/196351 [16:21:25<11:19:33,  1.99it/s]global step 115200, epoch: 1, batch: 115200, loss: 3.59227, acc: 0.84263, time: Sat Jan 22 02:56:04 2022
epoch 1 training:  59%|█████▊    | 115299/196351 [16:22:16<11:22:04,  1.98it/s]global step 115300, epoch: 1, batch: 115300, loss: 1.72263, acc: 0.84268, time: Sat Jan 22 02:56:55 2022
epoch 1 training:  59%|█████▉    | 115399/196351 [16:23:07<11:17:24,  1.99it/s]global step 115400, epoch: 1, batch: 115400, loss: 0.57655, acc: 0.84272, time: Sat Jan 22 02:57:46 2022
epoch 1 training:  59%|█████▉    | 115499/196351 [16:23:58<11:20:27,  1.98it/s]global step 115500, epoch: 1, batch: 115500, loss: 5.84849, acc: 0.84258, time: Sat Jan 22 02:58:37 2022
epoch 1 training:  59%|█████▉    | 115599/196351 [16:24:49<11:22:41,  1.97it/s]global step 115600, epoch: 1, batch: 115600, loss: 5.76690, acc: 0.84231, time: Sat Jan 22 02:59:27 2022
epoch 1 training:  59%|█████▉    | 115699/196351 [16:25:39<11:23:02,  1.97it/s]global step 115700, epoch: 1, batch: 115700, loss: 3.45657, acc: 0.84210, time: Sat Jan 22 03:00:18 2022
epoch 1 training:  59%|█████▉    | 115799/196351 [16:26:31<11:20:42,  1.97it/s]global step 115800, epoch: 1, batch: 115800, loss: 4.54365, acc: 0.84212, time: Sat Jan 22 03:01:09 2022
epoch 1 training:  59%|█████▉    | 115899/196351 [16:27:21<11:18:54,  1.98it/s]global step 115900, epoch: 1, batch: 115900, loss: 5.21050, acc: 0.84204, time: Sat Jan 22 03:02:00 2022
epoch 1 training:  59%|█████▉    | 115999/196351 [16:28:12<11:13:31,  1.99it/s]global step 116000, epoch: 1, batch: 116000, loss: 4.16470, acc: 0.84203, time: Sat Jan 22 03:02:50 2022
epoch 1 training:  59%|█████▉    | 116099/196351 [16:29:02<11:24:20,  1.95it/s]global step 116100, epoch: 1, batch: 116100, loss: 3.77877, acc: 0.84210, time: Sat Jan 22 03:03:41 2022
epoch 1 training:  59%|█████▉    | 116199/196351 [16:29:53<11:23:02,  1.96it/s]global step 116200, epoch: 1, batch: 116200, loss: 1.15712, acc: 0.84211, time: Sat Jan 22 03:04:32 2022
epoch 1 training:  59%|█████▉    | 116299/196351 [16:30:43<11:11:27,  1.99it/s]global step 116300, epoch: 1, batch: 116300, loss: 2.53014, acc: 0.84216, time: Sat Jan 22 03:05:22 2022
epoch 1 training:  59%|█████▉    | 116399/196351 [16:31:34<11:08:28,  1.99it/s]global step 116400, epoch: 1, batch: 116400, loss: 2.41453, acc: 0.84217, time: Sat Jan 22 03:06:13 2022
epoch 1 training:  59%|█████▉    | 116499/196351 [16:32:25<11:13:42,  1.98it/s]global step 116500, epoch: 1, batch: 116500, loss: 0.69525, acc: 0.84219, time: Sat Jan 22 03:07:04 2022
epoch 1 training:  59%|█████▉    | 116599/196351 [16:33:16<11:12:20,  1.98it/s]global step 116600, epoch: 1, batch: 116600, loss: 1.24956, acc: 0.84227, time: Sat Jan 22 03:07:54 2022
epoch 1 training:  59%|█████▉    | 116699/196351 [16:34:07<11:14:54,  1.97it/s]global step 116700, epoch: 1, batch: 116700, loss: 2.02991, acc: 0.84233, time: Sat Jan 22 03:08:45 2022
epoch 1 training:  59%|█████▉    | 116799/196351 [16:34:57<11:10:13,  1.98it/s]global step 116800, epoch: 1, batch: 116800, loss: 3.62464, acc: 0.84233, time: Sat Jan 22 03:09:36 2022
epoch 1 training:  60%|█████▉    | 116899/196351 [16:35:48<11:05:48,  1.99it/s]global step 116900, epoch: 1, batch: 116900, loss: 1.59073, acc: 0.84239, time: Sat Jan 22 03:10:27 2022
epoch 1 training:  60%|█████▉    | 116999/196351 [16:36:39<11:05:41,  1.99it/s]global step 117000, epoch: 1, batch: 117000, loss: 1.56374, acc: 0.84245, time: Sat Jan 22 03:11:18 2022
epoch 1 training:  60%|█████▉    | 117099/196351 [16:37:30<11:15:23,  1.96it/s]global step 117100, epoch: 1, batch: 117100, loss: 1.07307, acc: 0.84250, time: Sat Jan 22 03:12:09 2022
epoch 1 training:  60%|█████▉    | 117199/196351 [16:38:21<11:06:18,  1.98it/s]global step 117200, epoch: 1, batch: 117200, loss: 4.27398, acc: 0.84259, time: Sat Jan 22 03:12:59 2022
epoch 1 training:  60%|█████▉    | 117299/196351 [16:39:11<11:08:48,  1.97it/s]global step 117300, epoch: 1, batch: 117300, loss: 2.79296, acc: 0.84264, time: Sat Jan 22 03:13:50 2022
epoch 1 training:  60%|█████▉    | 117399/196351 [16:40:02<11:05:37,  1.98it/s]global step 117400, epoch: 1, batch: 117400, loss: 1.59866, acc: 0.84269, time: Sat Jan 22 03:14:41 2022
epoch 1 training:  60%|█████▉    | 117499/196351 [16:40:53<11:00:57,  1.99it/s]global step 117500, epoch: 1, batch: 117500, loss: 1.08032, acc: 0.84271, time: Sat Jan 22 03:15:31 2022
epoch 1 training:  60%|█████▉    | 117599/196351 [16:41:43<11:01:42,  1.98it/s]global step 117600, epoch: 1, batch: 117600, loss: 1.51157, acc: 0.84273, time: Sat Jan 22 03:16:22 2022
epoch 1 training:  60%|█████▉    | 117699/196351 [16:42:34<11:09:02,  1.96it/s]global step 117700, epoch: 1, batch: 117700, loss: 1.55729, acc: 0.84280, time: Sat Jan 22 03:17:13 2022
epoch 1 training:  60%|█████▉    | 117799/196351 [16:43:25<10:56:30,  1.99it/s]global step 117800, epoch: 1, batch: 117800, loss: 3.98575, acc: 0.84286, time: Sat Jan 22 03:18:03 2022
epoch 1 training:  60%|██████    | 117899/196351 [16:44:16<11:08:56,  1.95it/s]global step 117900, epoch: 1, batch: 117900, loss: 2.04465, acc: 0.84292, time: Sat Jan 22 03:18:54 2022
epoch 1 training:  60%|██████    | 117999/196351 [16:45:06<11:05:26,  1.96it/s]global step 118000, epoch: 1, batch: 118000, loss: 2.41581, acc: 0.84298, time: Sat Jan 22 03:19:45 2022
epoch 1 training:  60%|██████    | 118099/196351 [16:45:57<10:59:31,  1.98it/s]global step 118100, epoch: 1, batch: 118100, loss: 0.58181, acc: 0.84300, time: Sat Jan 22 03:20:36 2022
epoch 1 training:  60%|██████    | 118199/196351 [16:46:48<11:06:47,  1.95it/s]global step 118200, epoch: 1, batch: 118200, loss: 2.91146, acc: 0.84304, time: Sat Jan 22 03:21:26 2022
epoch 1 training:  60%|██████    | 118299/196351 [16:47:38<10:57:25,  1.98it/s]global step 118300, epoch: 1, batch: 118300, loss: 2.30931, acc: 0.84309, time: Sat Jan 22 03:22:17 2022
epoch 1 training:  60%|██████    | 118399/196351 [16:48:29<11:04:36,  1.95it/s]global step 118400, epoch: 1, batch: 118400, loss: 1.35664, acc: 0.84315, time: Sat Jan 22 03:23:08 2022
epoch 1 training:  60%|██████    | 118499/196351 [16:49:20<11:05:26,  1.95it/s]global step 118500, epoch: 1, batch: 118500, loss: 2.36627, acc: 0.84319, time: Sat Jan 22 03:23:59 2022
epoch 1 training:  60%|██████    | 118599/196351 [16:50:11<10:54:44,  1.98it/s]global step 118600, epoch: 1, batch: 118600, loss: 1.57578, acc: 0.84323, time: Sat Jan 22 03:24:50 2022
epoch 1 training:  60%|██████    | 118699/196351 [16:51:02<10:53:27,  1.98it/s]global step 118700, epoch: 1, batch: 118700, loss: 1.13746, acc: 0.84325, time: Sat Jan 22 03:25:41 2022
epoch 1 training:  61%|██████    | 118799/196351 [16:51:53<10:54:16,  1.98it/s]global step 118800, epoch: 1, batch: 118800, loss: 4.09534, acc: 0.84332, time: Sat Jan 22 03:26:31 2022
epoch 1 training:  61%|██████    | 118899/196351 [16:52:43<10:50:22,  1.98it/s]global step 118900, epoch: 1, batch: 118900, loss: 1.72072, acc: 0.84335, time: Sat Jan 22 03:27:22 2022
epoch 1 training:  61%|██████    | 118999/196351 [16:53:34<10:55:46,  1.97it/s]global step 119000, epoch: 1, batch: 119000, loss: 1.91612, acc: 0.84339, time: Sat Jan 22 03:28:13 2022
epoch 1 training:  61%|██████    | 119099/196351 [16:54:25<11:01:40,  1.95it/s]global step 119100, epoch: 1, batch: 119100, loss: 0.99750, acc: 0.84345, time: Sat Jan 22 03:29:03 2022
epoch 1 training:  61%|██████    | 119199/196351 [16:55:16<11:01:38,  1.94it/s]global step 119200, epoch: 1, batch: 119200, loss: 2.03300, acc: 0.84351, time: Sat Jan 22 03:29:54 2022
epoch 1 training:  61%|██████    | 119299/196351 [16:56:07<10:58:46,  1.95it/s]global step 119300, epoch: 1, batch: 119300, loss: 0.63819, acc: 0.84356, time: Sat Jan 22 03:30:45 2022
epoch 1 training:  61%|██████    | 119399/196351 [16:56:58<10:44:43,  1.99it/s]global step 119400, epoch: 1, batch: 119400, loss: 3.63686, acc: 0.84361, time: Sat Jan 22 03:31:37 2022
epoch 1 training:  61%|██████    | 119499/196351 [16:57:49<10:45:12,  1.99it/s]global step 119500, epoch: 1, batch: 119500, loss: 1.50001, acc: 0.84364, time: Sat Jan 22 03:32:28 2022
epoch 1 training:  61%|██████    | 119599/196351 [16:58:40<10:55:27,  1.95it/s]global step 119600, epoch: 1, batch: 119600, loss: 2.78703, acc: 0.84367, time: Sat Jan 22 03:33:19 2022
epoch 1 training:  61%|██████    | 119699/196351 [16:59:31<10:44:26,  1.98it/s]global step 119700, epoch: 1, batch: 119700, loss: 0.95424, acc: 0.84372, time: Sat Jan 22 03:34:10 2022
epoch 1 training:  61%|██████    | 119799/196351 [17:00:22<10:42:53,  1.98it/s]global step 119800, epoch: 1, batch: 119800, loss: 1.83778, acc: 0.84374, time: Sat Jan 22 03:35:01 2022
epoch 1 training:  61%|██████    | 119899/196351 [17:01:13<10:45:32,  1.97it/s]global step 119900, epoch: 1, batch: 119900, loss: 0.72099, acc: 0.84381, time: Sat Jan 22 03:35:51 2022
epoch 1 training:  61%|██████    | 119999/196351 [17:02:03<10:47:09,  1.97it/s]global step 120000, epoch: 1, batch: 120000, loss: 4.26081, acc: 0.84387, time: Sat Jan 22 03:36:42 2022
epoch 1 training:  61%|██████    | 120099/196351 [17:02:54<10:48:40,  1.96it/s]global step 120100, epoch: 1, batch: 120100, loss: 2.59368, acc: 0.84390, time: Sat Jan 22 03:37:33 2022
epoch 1 training:  61%|██████    | 120199/196351 [17:03:45<10:46:11,  1.96it/s]global step 120200, epoch: 1, batch: 120200, loss: 0.71954, acc: 0.84395, time: Sat Jan 22 03:38:24 2022
epoch 1 training:  61%|██████▏   | 120299/196351 [17:04:35<10:42:19,  1.97it/s]global step 120300, epoch: 1, batch: 120300, loss: 1.61351, acc: 0.84397, time: Sat Jan 22 03:39:14 2022
epoch 1 training:  61%|██████▏   | 120399/196351 [17:05:26<10:32:19,  2.00it/s]global step 120400, epoch: 1, batch: 120400, loss: 0.51428, acc: 0.84400, time: Sat Jan 22 03:40:04 2022
epoch 1 training:  61%|██████▏   | 120499/196351 [17:06:16<10:42:46,  1.97it/s]global step 120500, epoch: 1, batch: 120500, loss: 3.69769, acc: 0.84401, time: Sat Jan 22 03:40:55 2022
epoch 1 training:  61%|██████▏   | 120599/196351 [17:07:07<10:37:30,  1.98it/s]global step 120600, epoch: 1, batch: 120600, loss: 1.65307, acc: 0.84408, time: Sat Jan 22 03:41:46 2022
epoch 1 training:  61%|██████▏   | 120699/196351 [17:07:58<10:39:44,  1.97it/s]global step 120700, epoch: 1, batch: 120700, loss: 1.53409, acc: 0.84413, time: Sat Jan 22 03:42:37 2022
epoch 1 training:  62%|██████▏   | 120799/196351 [17:08:48<10:28:25,  2.00it/s]global step 120800, epoch: 1, batch: 120800, loss: 0.67608, acc: 0.84420, time: Sat Jan 22 03:43:27 2022
epoch 1 training:  62%|██████▏   | 120899/196351 [17:09:39<10:47:52,  1.94it/s]global step 120900, epoch: 1, batch: 120900, loss: 4.75817, acc: 0.84423, time: Sat Jan 22 03:44:18 2022
epoch 1 training:  62%|██████▏   | 120999/196351 [17:10:30<10:44:59,  1.95it/s]global step 121000, epoch: 1, batch: 121000, loss: 0.77721, acc: 0.84425, time: Sat Jan 22 03:45:09 2022
epoch 1 training:  62%|██████▏   | 121099/196351 [17:11:21<10:38:37,  1.96it/s]global step 121100, epoch: 1, batch: 121100, loss: 1.54671, acc: 0.84429, time: Sat Jan 22 03:45:59 2022
epoch 1 training:  62%|██████▏   | 121199/196351 [17:12:11<10:34:10,  1.98it/s]global step 121200, epoch: 1, batch: 121200, loss: 2.93412, acc: 0.84431, time: Sat Jan 22 03:46:50 2022
epoch 1 training:  62%|██████▏   | 121299/196351 [17:13:02<10:34:00,  1.97it/s]global step 121300, epoch: 1, batch: 121300, loss: 3.73004, acc: 0.84432, time: Sat Jan 22 03:47:41 2022
epoch 1 training:  62%|██████▏   | 121399/196351 [17:13:53<10:28:46,  1.99it/s]global step 121400, epoch: 1, batch: 121400, loss: 2.13973, acc: 0.84436, time: Sat Jan 22 03:48:32 2022
epoch 1 training:  62%|██████▏   | 121499/196351 [17:14:44<10:33:19,  1.97it/s]global step 121500, epoch: 1, batch: 121500, loss: 2.78128, acc: 0.84440, time: Sat Jan 22 03:49:22 2022
epoch 1 training:  62%|██████▏   | 121599/196351 [17:15:34<10:27:27,  1.99it/s]global step 121600, epoch: 1, batch: 121600, loss: 0.36741, acc: 0.84449, time: Sat Jan 22 03:50:13 2022
epoch 1 training:  62%|██████▏   | 121699/196351 [17:16:26<10:24:32,  1.99it/s]global step 121700, epoch: 1, batch: 121700, loss: 2.65448, acc: 0.84453, time: Sat Jan 22 03:51:04 2022
epoch 1 training:  62%|██████▏   | 121799/196351 [17:17:16<10:23:34,  1.99it/s]global step 121800, epoch: 1, batch: 121800, loss: 1.39740, acc: 0.84454, time: Sat Jan 22 03:51:55 2022
epoch 1 training:  62%|██████▏   | 121899/196351 [17:18:07<10:24:29,  1.99it/s]global step 121900, epoch: 1, batch: 121900, loss: 3.38422, acc: 0.84456, time: Sat Jan 22 03:52:46 2022
epoch 1 training:  62%|██████▏   | 121999/196351 [17:18:57<10:36:52,  1.95it/s]global step 122000, epoch: 1, batch: 122000, loss: 2.57829, acc: 0.84461, time: Sat Jan 22 03:53:36 2022
epoch 1 training:  62%|██████▏   | 122099/196351 [17:19:48<10:26:48,  1.97it/s]global step 122100, epoch: 1, batch: 122100, loss: 2.67569, acc: 0.84465, time: Sat Jan 22 03:54:27 2022
epoch 1 training:  62%|██████▏   | 122199/196351 [17:20:38<10:23:16,  1.98it/s]global step 122200, epoch: 1, batch: 122200, loss: 1.49430, acc: 0.84468, time: Sat Jan 22 03:55:17 2022
epoch 1 training:  62%|██████▏   | 122299/196351 [17:21:29<10:23:06,  1.98it/s]global step 122300, epoch: 1, batch: 122300, loss: 3.59816, acc: 0.84473, time: Sat Jan 22 03:56:08 2022
epoch 1 training:  62%|██████▏   | 122399/196351 [17:22:19<10:24:16,  1.97it/s]global step 122400, epoch: 1, batch: 122400, loss: 0.68908, acc: 0.84476, time: Sat Jan 22 03:56:58 2022
epoch 1 training:  62%|██████▏   | 122499/196351 [17:23:10<10:25:56,  1.97it/s]global step 122500, epoch: 1, batch: 122500, loss: 1.30877, acc: 0.84483, time: Sat Jan 22 03:57:49 2022
epoch 1 training:  62%|██████▏   | 122599/196351 [17:24:01<10:27:18,  1.96it/s]global step 122600, epoch: 1, batch: 122600, loss: 3.12148, acc: 0.84487, time: Sat Jan 22 03:58:40 2022
epoch 1 training:  62%|██████▏   | 122699/196351 [17:24:52<10:30:16,  1.95it/s]global step 122700, epoch: 1, batch: 122700, loss: 1.61088, acc: 0.84490, time: Sat Jan 22 03:59:31 2022
epoch 1 training:  63%|██████▎   | 122799/196351 [17:25:43<10:19:55,  1.98it/s]global step 122800, epoch: 1, batch: 122800, loss: 1.02177, acc: 0.84494, time: Sat Jan 22 04:00:22 2022
epoch 1 training:  63%|██████▎   | 122899/196351 [17:26:33<10:22:02,  1.97it/s]global step 122900, epoch: 1, batch: 122900, loss: 2.24705, acc: 0.84497, time: Sat Jan 22 04:01:12 2022
epoch 1 training:  63%|██████▎   | 122999/196351 [17:27:24<10:08:47,  2.01it/s]global step 123000, epoch: 1, batch: 123000, loss: 3.58795, acc: 0.84500, time: Sat Jan 22 04:02:03 2022
epoch 1 training:  63%|██████▎   | 123099/196351 [17:28:15<10:13:00,  1.99it/s]global step 123100, epoch: 1, batch: 123100, loss: 1.90815, acc: 0.84503, time: Sat Jan 22 04:02:53 2022
epoch 1 training:  63%|██████▎   | 123199/196351 [17:29:05<10:22:23,  1.96it/s]global step 123200, epoch: 1, batch: 123200, loss: 1.89298, acc: 0.84511, time: Sat Jan 22 04:03:44 2022
epoch 1 training:  63%|██████▎   | 123299/196351 [17:29:56<10:12:37,  1.99it/s]global step 123300, epoch: 1, batch: 123300, loss: 2.61147, acc: 0.84515, time: Sat Jan 22 04:04:34 2022
epoch 1 training:  63%|██████▎   | 123399/196351 [17:30:47<10:16:50,  1.97it/s]global step 123400, epoch: 1, batch: 123400, loss: 3.45077, acc: 0.84519, time: Sat Jan 22 04:05:25 2022
epoch 1 training:  63%|██████▎   | 123499/196351 [17:31:37<10:17:11,  1.97it/s]global step 123500, epoch: 1, batch: 123500, loss: 1.30646, acc: 0.84524, time: Sat Jan 22 04:06:16 2022
epoch 1 training:  63%|██████▎   | 123599/196351 [17:32:28<10:13:12,  1.98it/s]global step 123600, epoch: 1, batch: 123600, loss: 2.39923, acc: 0.84527, time: Sat Jan 22 04:07:07 2022
epoch 1 training:  63%|██████▎   | 123699/196351 [17:33:19<10:10:07,  1.98it/s]global step 123700, epoch: 1, batch: 123700, loss: 2.35530, acc: 0.84530, time: Sat Jan 22 04:07:57 2022
epoch 1 training:  63%|██████▎   | 123799/196351 [17:34:10<10:23:06,  1.94it/s]global step 123800, epoch: 1, batch: 123800, loss: 4.59421, acc: 0.84532, time: Sat Jan 22 04:08:48 2022
epoch 1 training:  63%|██████▎   | 123899/196351 [17:35:00<10:24:29,  1.93it/s]global step 123900, epoch: 1, batch: 123900, loss: 1.55028, acc: 0.84538, time: Sat Jan 22 04:09:39 2022
epoch 1 training:  63%|██████▎   | 123999/196351 [17:35:51<10:24:39,  1.93it/s]global step 124000, epoch: 1, batch: 124000, loss: 4.23578, acc: 0.84541, time: Sat Jan 22 04:10:30 2022
epoch 1 training:  63%|██████▎   | 124099/196351 [17:36:43<10:11:11,  1.97it/s]global step 124100, epoch: 1, batch: 124100, loss: 1.20172, acc: 0.84544, time: Sat Jan 22 04:11:21 2022
epoch 1 training:  63%|██████▎   | 124199/196351 [17:37:33<10:03:23,  1.99it/s]global step 124200, epoch: 1, batch: 124200, loss: 1.07001, acc: 0.84552, time: Sat Jan 22 04:12:12 2022
epoch 1 training:  63%|██████▎   | 124299/196351 [17:38:24<10:17:56,  1.94it/s]global step 124300, epoch: 1, batch: 124300, loss: 0.46178, acc: 0.84558, time: Sat Jan 22 04:13:03 2022
epoch 1 training:  63%|██████▎   | 124399/196351 [17:39:15<10:14:14,  1.95it/s]global step 124400, epoch: 1, batch: 124400, loss: 3.94688, acc: 0.84559, time: Sat Jan 22 04:13:54 2022
epoch 1 training:  63%|██████▎   | 124499/196351 [17:40:06<10:05:08,  1.98it/s]global step 124500, epoch: 1, batch: 124500, loss: 1.45305, acc: 0.84563, time: Sat Jan 22 04:14:45 2022
epoch 1 training:  63%|██████▎   | 124599/196351 [17:40:58<10:08:18,  1.97it/s]global step 124600, epoch: 1, batch: 124600, loss: 2.17805, acc: 0.84567, time: Sat Jan 22 04:15:36 2022
epoch 1 training:  64%|██████▎   | 124699/196351 [17:41:48<10:06:16,  1.97it/s]global step 124700, epoch: 1, batch: 124700, loss: 2.01819, acc: 0.84569, time: Sat Jan 22 04:16:27 2022
epoch 1 training:  64%|██████▎   | 124799/196351 [17:42:39<10:04:00,  1.97it/s]global step 124800, epoch: 1, batch: 124800, loss: 0.26172, acc: 0.84574, time: Sat Jan 22 04:17:18 2022
epoch 1 training:  64%|██████▎   | 124899/196351 [17:43:30<10:09:56,  1.95it/s]global step 124900, epoch: 1, batch: 124900, loss: 0.45412, acc: 0.84573, time: Sat Jan 22 04:18:08 2022
epoch 1 training:  64%|██████▎   | 124999/196351 [17:44:21<9:57:54,  1.99it/s] global step 125000, epoch: 1, batch: 125000, loss: 1.17895, acc: 0.84579, time: Sat Jan 22 04:18:59 2022
epoch 1 training:  64%|██████▎   | 125099/196351 [17:45:11<9:58:12,  1.99it/s]global step 125100, epoch: 1, batch: 125100, loss: 2.60270, acc: 0.84582, time: Sat Jan 22 04:19:50 2022
epoch 1 training:  64%|██████▍   | 125199/196351 [17:46:02<9:54:52,  1.99it/s]global step 125200, epoch: 1, batch: 125200, loss: 0.84559, acc: 0.84586, time: Sat Jan 22 04:20:40 2022
epoch 1 training:  64%|██████▍   | 125299/196351 [17:46:52<10:09:34,  1.94it/s]global step 125300, epoch: 1, batch: 125300, loss: 1.44964, acc: 0.84591, time: Sat Jan 22 04:21:31 2022
epoch 1 training:  64%|██████▍   | 125399/196351 [17:47:43<9:58:58,  1.97it/s]global step 125400, epoch: 1, batch: 125400, loss: 1.81108, acc: 0.84594, time: Sat Jan 22 04:22:22 2022
epoch 1 training:  64%|██████▍   | 125499/196351 [17:48:35<9:59:04,  1.97it/s] global step 125500, epoch: 1, batch: 125500, loss: 4.24532, acc: 0.84601, time: Sat Jan 22 04:23:13 2022
epoch 1 training:  64%|██████▍   | 125599/196351 [17:49:26<10:11:05,  1.93it/s]global step 125600, epoch: 1, batch: 125600, loss: 2.95895, acc: 0.84601, time: Sat Jan 22 04:24:05 2022
epoch 1 training:  64%|██████▍   | 125699/196351 [17:50:17<9:57:45,  1.97it/s]global step 125700, epoch: 1, batch: 125700, loss: 2.89784, acc: 0.84606, time: Sat Jan 22 04:24:56 2022
epoch 1 training:  64%|██████▍   | 125799/196351 [17:51:08<9:54:42,  1.98it/s]global step 125800, epoch: 1, batch: 125800, loss: 2.13868, acc: 0.84611, time: Sat Jan 22 04:25:46 2022
epoch 1 training:  64%|██████▍   | 125899/196351 [17:51:59<9:56:57,  1.97it/s] global step 125900, epoch: 1, batch: 125900, loss: 0.30860, acc: 0.84615, time: Sat Jan 22 04:26:37 2022
epoch 1 training:  64%|██████▍   | 125999/196351 [17:52:50<10:00:46,  1.95it/s]global step 126000, epoch: 1, batch: 126000, loss: 0.65100, acc: 0.84622, time: Sat Jan 22 04:27:28 2022
epoch 1 training:  64%|██████▍   | 126099/196351 [17:53:41<10:04:55,  1.94it/s]global step 126100, epoch: 1, batch: 126100, loss: 1.35099, acc: 0.84626, time: Sat Jan 22 04:28:19 2022
epoch 1 training:  64%|██████▍   | 126199/196351 [17:54:32<9:56:58,  1.96it/s]global step 126200, epoch: 1, batch: 126200, loss: 2.96287, acc: 0.84626, time: Sat Jan 22 04:29:11 2022
epoch 1 training:  64%|██████▍   | 126299/196351 [17:55:23<9:59:28,  1.95it/s]global step 126300, epoch: 1, batch: 126300, loss: 2.18893, acc: 0.84632, time: Sat Jan 22 04:30:01 2022
epoch 1 training:  64%|██████▍   | 126399/196351 [17:56:14<9:54:59,  1.96it/s]global step 126400, epoch: 1, batch: 126400, loss: 0.79918, acc: 0.84634, time: Sat Jan 22 04:30:52 2022
epoch 1 training:  64%|██████▍   | 126499/196351 [17:57:05<9:52:11,  1.97it/s]global step 126500, epoch: 1, batch: 126500, loss: 2.07947, acc: 0.84638, time: Sat Jan 22 04:31:44 2022
epoch 1 training:  64%|██████▍   | 126599/196351 [17:57:56<9:58:30,  1.94it/s] global step 126600, epoch: 1, batch: 126600, loss: 2.18067, acc: 0.84644, time: Sat Jan 22 04:32:35 2022
epoch 1 training:  65%|██████▍   | 126699/196351 [17:58:47<9:56:29,  1.95it/s]global step 126700, epoch: 1, batch: 126700, loss: 3.12856, acc: 0.84650, time: Sat Jan 22 04:33:26 2022
epoch 1 training:  65%|██████▍   | 126799/196351 [17:59:38<9:44:29,  1.98it/s]global step 126800, epoch: 1, batch: 126800, loss: 2.32809, acc: 0.84653, time: Sat Jan 22 04:34:17 2022
epoch 1 training:  65%|██████▍   | 126899/196351 [18:00:29<9:51:00,  1.96it/s]global step 126900, epoch: 1, batch: 126900, loss: 1.04252, acc: 0.84654, time: Sat Jan 22 04:35:07 2022
epoch 1 training:  65%|██████▍   | 126999/196351 [18:01:19<9:44:11,  1.98it/s]global step 127000, epoch: 1, batch: 127000, loss: 1.74239, acc: 0.84658, time: Sat Jan 22 04:35:58 2022
epoch 1 training:  65%|██████▍   | 127099/196351 [18:02:10<9:50:55,  1.95it/s]global step 127100, epoch: 1, batch: 127100, loss: 1.13371, acc: 0.84662, time: Sat Jan 22 04:36:48 2022
epoch 1 training:  65%|██████▍   | 127199/196351 [18:03:00<9:43:56,  1.97it/s]global step 127200, epoch: 1, batch: 127200, loss: 3.78653, acc: 0.84665, time: Sat Jan 22 04:37:39 2022
epoch 1 training:  65%|██████▍   | 127299/196351 [18:03:51<9:44:21,  1.97it/s]global step 127300, epoch: 1, batch: 127300, loss: 2.11979, acc: 0.84667, time: Sat Jan 22 04:38:30 2022
epoch 1 training:  65%|██████▍   | 127399/196351 [18:04:43<9:44:55,  1.96it/s]global step 127400, epoch: 1, batch: 127400, loss: 1.12064, acc: 0.84670, time: Sat Jan 22 04:39:22 2022
epoch 1 training:  65%|██████▍   | 127499/196351 [18:05:34<9:43:59,  1.97it/s]global step 127500, epoch: 1, batch: 127500, loss: 3.99067, acc: 0.84672, time: Sat Jan 22 04:40:13 2022
epoch 1 training:  65%|██████▍   | 127599/196351 [18:06:25<9:41:33,  1.97it/s]global step 127600, epoch: 1, batch: 127600, loss: 0.56778, acc: 0.84674, time: Sat Jan 22 04:41:04 2022
epoch 1 training:  65%|██████▌   | 127699/196351 [18:07:16<9:40:54,  1.97it/s]global step 127700, epoch: 1, batch: 127700, loss: 1.81113, acc: 0.84677, time: Sat Jan 22 04:41:54 2022
epoch 1 training:  65%|██████▌   | 127799/196351 [18:08:07<9:45:24,  1.95it/s]global step 127800, epoch: 1, batch: 127800, loss: 4.28409, acc: 0.84683, time: Sat Jan 22 04:42:46 2022
epoch 1 training:  65%|██████▌   | 127899/196351 [18:08:58<9:38:13,  1.97it/s]global step 127900, epoch: 1, batch: 127900, loss: 2.56365, acc: 0.84686, time: Sat Jan 22 04:43:37 2022
epoch 1 training:  65%|██████▌   | 127999/196351 [18:09:49<9:44:18,  1.95it/s]global step 128000, epoch: 1, batch: 128000, loss: 1.81081, acc: 0.84690, time: Sat Jan 22 04:44:28 2022
epoch 1 training:  65%|██████▌   | 128099/196351 [18:10:40<9:43:29,  1.95it/s]global step 128100, epoch: 1, batch: 128100, loss: 0.62789, acc: 0.84697, time: Sat Jan 22 04:45:19 2022
epoch 1 training:  65%|██████▌   | 128199/196351 [18:11:31<9:46:55,  1.94it/s]global step 128200, epoch: 1, batch: 128200, loss: 3.98549, acc: 0.84701, time: Sat Jan 22 04:46:10 2022
epoch 1 training:  65%|██████▌   | 128299/196351 [18:12:22<9:36:39,  1.97it/s]global step 128300, epoch: 1, batch: 128300, loss: 0.66293, acc: 0.84705, time: Sat Jan 22 04:47:01 2022
epoch 1 training:  65%|██████▌   | 128399/196351 [18:13:13<9:32:52,  1.98it/s]global step 128400, epoch: 1, batch: 128400, loss: 2.12355, acc: 0.84710, time: Sat Jan 22 04:47:52 2022
epoch 1 training:  65%|██████▌   | 128499/196351 [18:14:04<9:30:13,  1.98it/s]global step 128500, epoch: 1, batch: 128500, loss: 0.68613, acc: 0.84711, time: Sat Jan 22 04:48:43 2022
epoch 1 training:  65%|██████▌   | 128599/196351 [18:14:55<9:35:09,  1.96it/s]global step 128600, epoch: 1, batch: 128600, loss: 1.51375, acc: 0.84716, time: Sat Jan 22 04:49:34 2022
epoch 1 training:  66%|██████▌   | 128699/196351 [18:15:46<9:31:34,  1.97it/s]global step 128700, epoch: 1, batch: 128700, loss: 0.25991, acc: 0.84720, time: Sat Jan 22 04:50:25 2022
epoch 1 training:  66%|██████▌   | 128799/196351 [18:16:37<9:38:47,  1.95it/s]global step 128800, epoch: 1, batch: 128800, loss: 0.84236, acc: 0.84724, time: Sat Jan 22 04:51:15 2022
epoch 1 training:  66%|██████▌   | 128899/196351 [18:17:28<9:25:02,  1.99it/s]global step 128900, epoch: 1, batch: 128900, loss: 4.06117, acc: 0.84725, time: Sat Jan 22 04:52:06 2022
epoch 1 training:  66%|██████▌   | 128999/196351 [18:18:19<9:24:40,  1.99it/s]global step 129000, epoch: 1, batch: 129000, loss: 4.25241, acc: 0.84728, time: Sat Jan 22 04:52:57 2022
epoch 1 training:  66%|██████▌   | 129099/196351 [18:19:09<9:35:28,  1.95it/s]global step 129100, epoch: 1, batch: 129100, loss: 2.47433, acc: 0.84732, time: Sat Jan 22 04:53:48 2022
epoch 1 training:  66%|██████▌   | 129199/196351 [18:20:01<9:32:44,  1.95it/s]global step 129200, epoch: 1, batch: 129200, loss: 2.11127, acc: 0.84734, time: Sat Jan 22 04:54:39 2022
epoch 1 training:  66%|██████▌   | 129299/196351 [18:20:52<9:21:04,  1.99it/s]global step 129300, epoch: 1, batch: 129300, loss: 0.29930, acc: 0.84738, time: Sat Jan 22 04:55:30 2022
epoch 1 training:  66%|██████▌   | 129399/196351 [18:21:43<10:39:47,  1.74it/s]global step 129400, epoch: 1, batch: 129400, loss: 1.51925, acc: 0.84741, time: Sat Jan 22 04:56:22 2022
epoch 1 training:  66%|██████▌   | 129499/196351 [18:22:33<9:23:58,  1.98it/s]global step 129500, epoch: 1, batch: 129500, loss: 3.35704, acc: 0.84747, time: Sat Jan 22 04:57:12 2022
epoch 1 training:  66%|██████▌   | 129599/196351 [18:23:25<9:23:15,  1.98it/s]global step 129600, epoch: 1, batch: 129600, loss: 0.84683, acc: 0.84753, time: Sat Jan 22 04:58:03 2022
epoch 1 training:  66%|██████▌   | 129699/196351 [18:24:15<9:20:51,  1.98it/s]global step 129700, epoch: 1, batch: 129700, loss: 0.34514, acc: 0.84758, time: Sat Jan 22 04:58:54 2022
epoch 1 training:  66%|██████▌   | 129799/196351 [18:25:06<9:33:16,  1.93it/s]global step 129800, epoch: 1, batch: 129800, loss: 2.65992, acc: 0.84765, time: Sat Jan 22 04:59:45 2022
epoch 1 training:  66%|██████▌   | 129899/196351 [18:25:57<9:19:52,  1.98it/s]global step 129900, epoch: 1, batch: 129900, loss: 0.78743, acc: 0.84770, time: Sat Jan 22 05:00:36 2022
epoch 1 training:  66%|██████▌   | 129999/196351 [18:26:48<9:20:48,  1.97it/s]global step 130000, epoch: 1, batch: 130000, loss: 3.47960, acc: 0.84773, time: Sat Jan 22 05:01:26 2022
epoch 1 training:  66%|██████▋   | 130099/196351 [18:27:38<9:19:51,  1.97it/s]global step 130100, epoch: 1, batch: 130100, loss: 4.70677, acc: 0.84777, time: Sat Jan 22 05:02:17 2022
epoch 1 training:  66%|██████▋   | 130199/196351 [18:28:29<9:24:35,  1.95it/s]global step 130200, epoch: 1, batch: 130200, loss: 0.60106, acc: 0.84784, time: Sat Jan 22 05:03:07 2022
epoch 1 training:  66%|██████▋   | 130299/196351 [18:29:19<9:16:15,  1.98it/s]global step 130300, epoch: 1, batch: 130300, loss: 4.26455, acc: 0.84787, time: Sat Jan 22 05:03:58 2022
epoch 1 training:  66%|██████▋   | 130399/196351 [18:30:10<9:19:56,  1.96it/s]global step 130400, epoch: 1, batch: 130400, loss: 3.05318, acc: 0.84788, time: Sat Jan 22 05:04:49 2022
epoch 1 training:  66%|██████▋   | 130499/196351 [18:31:01<9:20:07,  1.96it/s]global step 130500, epoch: 1, batch: 130500, loss: 2.58314, acc: 0.84792, time: Sat Jan 22 05:05:40 2022
epoch 1 training:  67%|██████▋   | 130599/196351 [18:31:52<9:19:37,  1.96it/s]global step 130600, epoch: 1, batch: 130600, loss: 2.27520, acc: 0.84798, time: Sat Jan 22 05:06:31 2022
epoch 1 training:  67%|██████▋   | 130699/196351 [18:32:43<9:22:23,  1.95it/s]global step 130700, epoch: 1, batch: 130700, loss: 0.45571, acc: 0.84805, time: Sat Jan 22 05:07:22 2022
epoch 1 training:  67%|██████▋   | 130799/196351 [18:33:34<9:16:33,  1.96it/s]global step 130800, epoch: 1, batch: 130800, loss: 1.66966, acc: 0.84811, time: Sat Jan 22 05:08:13 2022
epoch 1 training:  67%|██████▋   | 130899/196351 [18:34:25<9:27:08,  1.92it/s]global step 130900, epoch: 1, batch: 130900, loss: 3.02755, acc: 0.84815, time: Sat Jan 22 05:09:04 2022
epoch 1 training:  67%|██████▋   | 130999/196351 [18:35:16<9:19:53,  1.95it/s]global step 131000, epoch: 1, batch: 131000, loss: 1.36463, acc: 0.84820, time: Sat Jan 22 05:09:55 2022
epoch 1 training:  67%|██████▋   | 131099/196351 [18:36:07<9:08:06,  1.98it/s]global step 131100, epoch: 1, batch: 131100, loss: 2.13726, acc: 0.84823, time: Sat Jan 22 05:10:46 2022
epoch 1 training:  67%|██████▋   | 131199/196351 [18:36:58<9:19:15,  1.94it/s]global step 131200, epoch: 1, batch: 131200, loss: 2.46729, acc: 0.84823, time: Sat Jan 22 05:11:37 2022
epoch 1 training:  67%|██████▋   | 131299/196351 [18:37:50<9:08:22,  1.98it/s]global step 131300, epoch: 1, batch: 131300, loss: 1.42453, acc: 0.84828, time: Sat Jan 22 05:12:29 2022
epoch 1 training:  67%|██████▋   | 131399/196351 [18:38:41<9:12:56,  1.96it/s]global step 131400, epoch: 1, batch: 131400, loss: 1.23354, acc: 0.84831, time: Sat Jan 22 05:13:20 2022
epoch 1 training:  67%|██████▋   | 131499/196351 [18:39:33<9:05:57,  1.98it/s]global step 131500, epoch: 1, batch: 131500, loss: 3.70486, acc: 0.84835, time: Sat Jan 22 05:14:11 2022
epoch 1 training:  67%|██████▋   | 131599/196351 [18:40:23<9:05:00,  1.98it/s]global step 131600, epoch: 1, batch: 131600, loss: 1.73870, acc: 0.84838, time: Sat Jan 22 05:15:02 2022
epoch 1 training:  67%|██████▋   | 131699/196351 [18:41:14<9:15:39,  1.94it/s]global step 131700, epoch: 1, batch: 131700, loss: 2.52771, acc: 0.84842, time: Sat Jan 22 05:15:53 2022
epoch 1 training:  67%|██████▋   | 131799/196351 [18:42:05<9:16:17,  1.93it/s]global step 131800, epoch: 1, batch: 131800, loss: 1.52338, acc: 0.84848, time: Sat Jan 22 05:16:44 2022
epoch 1 training:  67%|██████▋   | 131899/196351 [18:42:57<9:13:48,  1.94it/s]global step 131900, epoch: 1, batch: 131900, loss: 3.04832, acc: 0.84851, time: Sat Jan 22 05:17:35 2022
epoch 1 training:  67%|██████▋   | 131999/196351 [18:43:48<9:08:28,  1.96it/s]global step 132000, epoch: 1, batch: 132000, loss: 2.78785, acc: 0.84855, time: Sat Jan 22 05:18:26 2022
epoch 1 training:  67%|██████▋   | 132099/196351 [18:44:39<9:05:55,  1.96it/s]global step 132100, epoch: 1, batch: 132100, loss: 2.33396, acc: 0.84860, time: Sat Jan 22 05:19:17 2022
epoch 1 training:  67%|██████▋   | 132199/196351 [18:45:29<8:58:05,  1.99it/s]global step 132200, epoch: 1, batch: 132200, loss: 1.18280, acc: 0.84865, time: Sat Jan 22 05:20:08 2022
epoch 1 training:  67%|██████▋   | 132299/196351 [18:46:21<8:56:55,  1.99it/s]global step 132300, epoch: 1, batch: 132300, loss: 3.82526, acc: 0.84869, time: Sat Jan 22 05:20:59 2022
epoch 1 training:  67%|██████▋   | 132399/196351 [18:47:11<9:05:22,  1.95it/s]global step 132400, epoch: 1, batch: 132400, loss: 4.43020, acc: 0.84875, time: Sat Jan 22 05:21:50 2022
epoch 1 training:  67%|██████▋   | 132499/196351 [18:48:02<8:53:45,  1.99it/s]global step 132500, epoch: 1, batch: 132500, loss: 1.28545, acc: 0.84877, time: Sat Jan 22 05:22:40 2022
epoch 1 training:  68%|██████▊   | 132599/196351 [18:48:53<8:59:55,  1.97it/s]global step 132600, epoch: 1, batch: 132600, loss: 3.63639, acc: 0.84880, time: Sat Jan 22 05:23:31 2022
epoch 1 training:  68%|██████▊   | 132699/196351 [18:49:43<8:52:16,  1.99it/s]global step 132700, epoch: 1, batch: 132700, loss: 3.32443, acc: 0.84881, time: Sat Jan 22 05:24:22 2022
epoch 1 training:  68%|██████▊   | 132799/196351 [18:50:34<8:54:04,  1.98it/s]global step 132800, epoch: 1, batch: 132800, loss: 3.65449, acc: 0.84885, time: Sat Jan 22 05:25:13 2022
epoch 1 training:  68%|██████▊   | 132899/196351 [18:51:25<8:59:49,  1.96it/s]global step 132900, epoch: 1, batch: 132900, loss: 1.20949, acc: 0.84887, time: Sat Jan 22 05:26:04 2022
epoch 1 training:  68%|██████▊   | 132999/196351 [18:52:16<9:01:01,  1.95it/s]global step 133000, epoch: 1, batch: 133000, loss: 3.16500, acc: 0.84886, time: Sat Jan 22 05:26:54 2022
epoch 1 training:  68%|██████▊   | 133099/196351 [18:53:07<8:52:26,  1.98it/s]global step 133100, epoch: 1, batch: 133100, loss: 1.17733, acc: 0.84889, time: Sat Jan 22 05:27:45 2022
epoch 1 training:  68%|██████▊   | 133199/196351 [18:53:58<8:52:01,  1.98it/s]global step 133200, epoch: 1, batch: 133200, loss: 1.40476, acc: 0.84891, time: Sat Jan 22 05:28:36 2022
epoch 1 training:  68%|██████▊   | 133299/196351 [18:54:49<8:58:09,  1.95it/s]global step 133300, epoch: 1, batch: 133300, loss: 4.12809, acc: 0.84894, time: Sat Jan 22 05:29:28 2022
epoch 1 training:  68%|██████▊   | 133399/196351 [18:55:40<8:57:35,  1.95it/s]global step 133400, epoch: 1, batch: 133400, loss: 1.18458, acc: 0.84899, time: Sat Jan 22 05:30:18 2022
epoch 1 training:  68%|██████▊   | 133499/196351 [18:56:30<8:47:53,  1.98it/s]global step 133500, epoch: 1, batch: 133500, loss: 1.59131, acc: 0.84906, time: Sat Jan 22 05:31:09 2022
epoch 1 training:  68%|██████▊   | 133599/196351 [18:57:21<8:54:14,  1.96it/s]global step 133600, epoch: 1, batch: 133600, loss: 0.13838, acc: 0.84911, time: Sat Jan 22 05:31:59 2022
epoch 1 training:  68%|██████▊   | 133699/196351 [18:58:12<8:52:26,  1.96it/s]global step 133700, epoch: 1, batch: 133700, loss: 1.41109, acc: 0.84916, time: Sat Jan 22 05:32:50 2022
epoch 1 training:  68%|██████▊   | 133799/196351 [18:59:03<8:46:13,  1.98it/s]global step 133800, epoch: 1, batch: 133800, loss: 0.49859, acc: 0.84920, time: Sat Jan 22 05:33:41 2022
epoch 1 training:  68%|██████▊   | 133899/196351 [18:59:53<8:47:11,  1.97it/s]global step 133900, epoch: 1, batch: 133900, loss: 0.84742, acc: 0.84924, time: Sat Jan 22 05:34:32 2022
epoch 1 training:  68%|██████▊   | 133999/196351 [19:00:44<8:43:48,  1.98it/s]global step 134000, epoch: 1, batch: 134000, loss: 3.43138, acc: 0.84926, time: Sat Jan 22 05:35:23 2022
epoch 1 training:  68%|██████▊   | 134099/196351 [19:01:35<9:00:04,  1.92it/s]global step 134100, epoch: 1, batch: 134100, loss: 1.44694, acc: 0.84930, time: Sat Jan 22 05:36:14 2022
epoch 1 training:  68%|██████▊   | 134199/196351 [19:02:26<8:56:56,  1.93it/s]global step 134200, epoch: 1, batch: 134200, loss: 2.13693, acc: 0.84931, time: Sat Jan 22 05:37:05 2022
epoch 1 training:  68%|██████▊   | 134299/196351 [19:03:17<8:51:02,  1.95it/s]global step 134300, epoch: 1, batch: 134300, loss: 2.77762, acc: 0.84933, time: Sat Jan 22 05:37:56 2022
epoch 1 training:  68%|██████▊   | 134399/196351 [19:04:08<8:51:29,  1.94it/s]global step 134400, epoch: 1, batch: 134400, loss: 2.45937, acc: 0.84937, time: Sat Jan 22 05:38:47 2022
epoch 1 training:  68%|██████▊   | 134499/196351 [19:04:59<8:47:40,  1.95it/s]global step 134500, epoch: 1, batch: 134500, loss: 2.81866, acc: 0.84938, time: Sat Jan 22 05:39:38 2022
epoch 1 training:  69%|██████▊   | 134599/196351 [19:05:50<8:49:41,  1.94it/s]global step 134600, epoch: 1, batch: 134600, loss: 1.24006, acc: 0.84941, time: Sat Jan 22 05:40:29 2022
epoch 1 training:  69%|██████▊   | 134699/196351 [19:06:41<8:47:39,  1.95it/s]global step 134700, epoch: 1, batch: 134700, loss: 3.63700, acc: 0.84944, time: Sat Jan 22 05:41:20 2022
epoch 1 training:  69%|██████▊   | 134799/196351 [19:07:32<8:42:09,  1.96it/s]global step 134800, epoch: 1, batch: 134800, loss: 2.64833, acc: 0.84949, time: Sat Jan 22 05:42:11 2022
epoch 1 training:  69%|██████▊   | 134899/196351 [19:08:23<8:40:06,  1.97it/s]global step 134900, epoch: 1, batch: 134900, loss: 3.41620, acc: 0.84951, time: Sat Jan 22 05:43:01 2022
epoch 1 training:  69%|██████▉   | 134999/196351 [19:09:13<8:39:24,  1.97it/s]global step 135000, epoch: 1, batch: 135000, loss: 2.82406, acc: 0.84952, time: Sat Jan 22 05:43:52 2022
epoch 1 training:  69%|██████▉   | 135099/196351 [19:10:05<8:32:26,  1.99it/s]global step 135100, epoch: 1, batch: 135100, loss: 0.82114, acc: 0.84958, time: Sat Jan 22 05:44:43 2022
epoch 1 training:  69%|██████▉   | 135199/196351 [19:10:55<8:29:44,  2.00it/s]global step 135200, epoch: 1, batch: 135200, loss: 2.16420, acc: 0.84960, time: Sat Jan 22 05:45:34 2022
epoch 1 training:  69%|██████▉   | 135299/196351 [19:11:46<8:34:08,  1.98it/s]global step 135300, epoch: 1, batch: 135300, loss: 2.83101, acc: 0.84965, time: Sat Jan 22 05:46:25 2022
epoch 1 training:  69%|██████▉   | 135399/196351 [19:12:37<8:42:55,  1.94it/s]global step 135400, epoch: 1, batch: 135400, loss: 1.84051, acc: 0.84970, time: Sat Jan 22 05:47:16 2022
epoch 1 training:  69%|██████▉   | 135499/196351 [19:13:28<8:33:02,  1.98it/s]global step 135500, epoch: 1, batch: 135500, loss: 1.99114, acc: 0.84975, time: Sat Jan 22 05:48:07 2022
epoch 1 training:  69%|██████▉   | 135599/196351 [19:14:18<8:42:42,  1.94it/s]global step 135600, epoch: 1, batch: 135600, loss: 1.05379, acc: 0.84980, time: Sat Jan 22 05:48:57 2022
epoch 1 training:  69%|██████▉   | 135699/196351 [19:15:09<8:46:03,  1.92it/s]global step 135700, epoch: 1, batch: 135700, loss: 2.46969, acc: 0.84983, time: Sat Jan 22 05:49:48 2022
epoch 1 training:  69%|██████▉   | 135799/196351 [19:16:01<8:37:03,  1.95it/s]global step 135800, epoch: 1, batch: 135800, loss: 2.56770, acc: 0.84986, time: Sat Jan 22 05:50:39 2022
epoch 1 training:  69%|██████▉   | 135899/196351 [19:16:52<8:22:51,  2.00it/s]global step 135900, epoch: 1, batch: 135900, loss: 0.47841, acc: 0.84990, time: Sat Jan 22 05:51:30 2022
epoch 1 training:  69%|██████▉   | 135999/196351 [19:17:43<8:22:52,  2.00it/s]global step 136000, epoch: 1, batch: 136000, loss: 0.92932, acc: 0.84993, time: Sat Jan 22 05:52:21 2022
epoch 1 training:  69%|██████▉   | 136099/196351 [19:18:33<8:29:37,  1.97it/s]global step 136100, epoch: 1, batch: 136100, loss: 0.83109, acc: 0.84998, time: Sat Jan 22 05:53:12 2022
epoch 1 training:  69%|██████▉   | 136199/196351 [19:19:25<8:25:12,  1.98it/s]global step 136200, epoch: 1, batch: 136200, loss: 0.46700, acc: 0.85001, time: Sat Jan 22 05:54:03 2022
epoch 1 training:  69%|██████▉   | 136299/196351 [19:20:16<8:27:25,  1.97it/s]global step 136300, epoch: 1, batch: 136300, loss: 2.81281, acc: 0.85006, time: Sat Jan 22 05:54:54 2022
epoch 1 training:  69%|██████▉   | 136399/196351 [19:21:06<8:19:54,  2.00it/s]global step 136400, epoch: 1, batch: 136400, loss: 0.84986, acc: 0.85010, time: Sat Jan 22 05:55:45 2022
epoch 1 training:  70%|██████▉   | 136499/196351 [19:21:57<8:32:10,  1.95it/s]global step 136500, epoch: 1, batch: 136500, loss: 0.59671, acc: 0.85012, time: Sat Jan 22 05:56:36 2022
epoch 1 training:  70%|██████▉   | 136599/196351 [19:22:48<8:21:33,  1.99it/s]global step 136600, epoch: 1, batch: 136600, loss: 2.32657, acc: 0.85019, time: Sat Jan 22 05:57:27 2022
epoch 1 training:  70%|██████▉   | 136699/196351 [19:23:39<8:27:32,  1.96it/s]global step 136700, epoch: 1, batch: 136700, loss: 0.36511, acc: 0.85022, time: Sat Jan 22 05:58:18 2022
epoch 1 training:  70%|██████▉   | 136799/196351 [19:24:30<8:42:38,  1.90it/s]global step 136800, epoch: 1, batch: 136800, loss: 1.39350, acc: 0.85026, time: Sat Jan 22 05:59:09 2022
epoch 1 training:  70%|██████▉   | 136899/196351 [19:25:22<8:22:23,  1.97it/s]global step 136900, epoch: 1, batch: 136900, loss: 1.82919, acc: 0.85031, time: Sat Jan 22 06:00:00 2022
epoch 1 training:  70%|██████▉   | 136999/196351 [19:26:13<8:25:43,  1.96it/s]global step 137000, epoch: 1, batch: 137000, loss: 3.64150, acc: 0.85031, time: Sat Jan 22 06:00:51 2022
epoch 1 training:  70%|██████▉   | 137099/196351 [19:27:04<8:21:34,  1.97it/s]global step 137100, epoch: 1, batch: 137100, loss: 3.73254, acc: 0.85035, time: Sat Jan 22 06:01:43 2022
epoch 1 training:  70%|██████▉   | 137199/196351 [19:27:55<8:19:38,  1.97it/s]global step 137200, epoch: 1, batch: 137200, loss: 2.13150, acc: 0.85039, time: Sat Jan 22 06:02:34 2022
epoch 1 training:  70%|██████▉   | 137299/196351 [19:28:46<8:15:08,  1.99it/s]global step 137300, epoch: 1, batch: 137300, loss: 2.38373, acc: 0.85044, time: Sat Jan 22 06:03:25 2022
epoch 1 training:  70%|██████▉   | 137399/196351 [19:29:37<8:26:28,  1.94it/s]global step 137400, epoch: 1, batch: 137400, loss: 3.71869, acc: 0.85047, time: Sat Jan 22 06:04:16 2022
epoch 1 training:  70%|███████   | 137499/196351 [19:30:28<8:25:31,  1.94it/s]global step 137500, epoch: 1, batch: 137500, loss: 2.24014, acc: 0.85053, time: Sat Jan 22 06:05:07 2022
epoch 1 training:  70%|███████   | 137599/196351 [19:31:19<8:14:59,  1.98it/s]global step 137600, epoch: 1, batch: 137600, loss: 2.17871, acc: 0.85057, time: Sat Jan 22 06:05:58 2022
epoch 1 training:  70%|███████   | 137699/196351 [19:32:10<8:12:34,  1.98it/s]global step 137700, epoch: 1, batch: 137700, loss: 1.49421, acc: 0.85060, time: Sat Jan 22 06:06:49 2022
epoch 1 training:  70%|███████   | 137799/196351 [19:33:01<8:16:00,  1.97it/s]global step 137800, epoch: 1, batch: 137800, loss: 2.97979, acc: 0.85063, time: Sat Jan 22 06:07:40 2022
epoch 1 training:  70%|███████   | 137899/196351 [19:33:52<8:20:01,  1.95it/s]global step 137900, epoch: 1, batch: 137900, loss: 1.43336, acc: 0.85068, time: Sat Jan 22 06:08:31 2022
epoch 1 training:  70%|███████   | 137999/196351 [19:34:43<8:17:49,  1.95it/s]global step 138000, epoch: 1, batch: 138000, loss: 1.95489, acc: 0.85072, time: Sat Jan 22 06:09:22 2022
epoch 1 training:  70%|███████   | 138099/196351 [19:35:34<8:13:51,  1.97it/s]global step 138100, epoch: 1, batch: 138100, loss: 2.75301, acc: 0.85074, time: Sat Jan 22 06:10:13 2022
epoch 1 training:  70%|███████   | 138199/196351 [19:36:25<8:13:00,  1.97it/s]global step 138200, epoch: 1, batch: 138200, loss: 0.78806, acc: 0.85077, time: Sat Jan 22 06:11:03 2022
epoch 1 training:  70%|███████   | 138299/196351 [19:37:15<8:14:02,  1.96it/s]global step 138300, epoch: 1, batch: 138300, loss: 3.53538, acc: 0.85082, time: Sat Jan 22 06:11:54 2022
epoch 1 training:  70%|███████   | 138399/196351 [19:38:06<8:13:06,  1.96it/s]global step 138400, epoch: 1, batch: 138400, loss: 2.23854, acc: 0.85088, time: Sat Jan 22 06:12:45 2022
epoch 1 training:  71%|███████   | 138499/196351 [19:38:57<8:14:14,  1.95it/s]global step 138500, epoch: 1, batch: 138500, loss: 0.80295, acc: 0.85088, time: Sat Jan 22 06:13:35 2022
epoch 1 training:  71%|███████   | 138599/196351 [19:39:47<8:19:42,  1.93it/s]global step 138600, epoch: 1, batch: 138600, loss: 1.27218, acc: 0.85091, time: Sat Jan 22 06:14:26 2022
epoch 1 training:  71%|███████   | 138699/196351 [19:40:38<8:11:52,  1.95it/s]global step 138700, epoch: 1, batch: 138700, loss: 3.55852, acc: 0.85094, time: Sat Jan 22 06:15:17 2022
epoch 1 training:  71%|███████   | 138799/196351 [19:41:29<8:04:22,  1.98it/s]global step 138800, epoch: 1, batch: 138800, loss: 1.59550, acc: 0.85095, time: Sat Jan 22 06:16:08 2022
epoch 1 training:  71%|███████   | 138899/196351 [19:42:20<8:10:56,  1.95it/s]global step 138900, epoch: 1, batch: 138900, loss: 0.71650, acc: 0.85099, time: Sat Jan 22 06:16:59 2022
epoch 1 training:  71%|███████   | 138999/196351 [19:43:12<8:06:22,  1.97it/s]global step 139000, epoch: 1, batch: 139000, loss: 0.81382, acc: 0.85105, time: Sat Jan 22 06:17:51 2022
epoch 1 training:  71%|███████   | 139099/196351 [19:44:03<8:01:50,  1.98it/s]global step 139100, epoch: 1, batch: 139100, loss: 0.48152, acc: 0.85109, time: Sat Jan 22 06:18:41 2022
epoch 1 training:  71%|███████   | 139199/196351 [19:44:53<8:01:45,  1.98it/s]global step 139200, epoch: 1, batch: 139200, loss: 3.10539, acc: 0.85111, time: Sat Jan 22 06:19:32 2022
epoch 1 training:  71%|███████   | 139299/196351 [19:45:44<8:11:05,  1.94it/s]global step 139300, epoch: 1, batch: 139300, loss: 2.83280, acc: 0.85111, time: Sat Jan 22 06:20:23 2022
epoch 1 training:  71%|███████   | 139399/196351 [19:46:36<8:13:26,  1.92it/s]global step 139400, epoch: 1, batch: 139400, loss: 1.32759, acc: 0.85114, time: Sat Jan 22 06:21:14 2022
epoch 1 training:  71%|███████   | 139499/196351 [19:47:27<8:03:39,  1.96it/s]global step 139500, epoch: 1, batch: 139500, loss: 4.93088, acc: 0.85118, time: Sat Jan 22 06:22:05 2022
epoch 1 training:  71%|███████   | 139599/196351 [19:48:18<8:02:41,  1.96it/s]global step 139600, epoch: 1, batch: 139600, loss: 1.45118, acc: 0.85122, time: Sat Jan 22 06:22:57 2022
epoch 1 training:  71%|███████   | 139699/196351 [19:49:09<8:01:23,  1.96it/s]global step 139700, epoch: 1, batch: 139700, loss: 2.32409, acc: 0.85124, time: Sat Jan 22 06:23:48 2022
epoch 1 training:  71%|███████   | 139799/196351 [19:50:00<7:56:57,  1.98it/s]global step 139800, epoch: 1, batch: 139800, loss: 2.62353, acc: 0.85125, time: Sat Jan 22 06:24:38 2022
epoch 1 training:  71%|███████   | 139899/196351 [19:50:51<8:03:58,  1.94it/s]global step 139900, epoch: 1, batch: 139900, loss: 3.35382, acc: 0.85129, time: Sat Jan 22 06:25:29 2022
epoch 1 training:  71%|███████▏  | 139999/196351 [19:51:41<7:57:21,  1.97it/s]global step 140000, epoch: 1, batch: 140000, loss: 3.09263, acc: 0.85132, time: Sat Jan 22 06:26:20 2022
epoch 1 training:  71%|███████▏  | 140099/196351 [19:52:33<7:57:26,  1.96it/s]global step 140100, epoch: 1, batch: 140100, loss: 0.62576, acc: 0.85137, time: Sat Jan 22 06:27:12 2022
epoch 1 training:  71%|███████▏  | 140199/196351 [19:53:24<7:55:13,  1.97it/s]global step 140200, epoch: 1, batch: 140200, loss: 2.80919, acc: 0.85138, time: Sat Jan 22 06:28:02 2022
epoch 1 training:  71%|███████▏  | 140299/196351 [19:54:14<8:00:50,  1.94it/s]global step 140300, epoch: 1, batch: 140300, loss: 2.46465, acc: 0.85141, time: Sat Jan 22 06:28:53 2022
epoch 1 training:  72%|███████▏  | 140399/196351 [19:55:05<7:55:23,  1.96it/s]global step 140400, epoch: 1, batch: 140400, loss: 2.69143, acc: 0.85143, time: Sat Jan 22 06:29:44 2022
epoch 1 training:  72%|███████▏  | 140499/196351 [19:55:56<7:49:12,  1.98it/s]global step 140500, epoch: 1, batch: 140500, loss: 2.52380, acc: 0.85143, time: Sat Jan 22 06:30:35 2022
epoch 1 training:  72%|███████▏  | 140599/196351 [19:56:47<7:56:23,  1.95it/s]global step 140600, epoch: 1, batch: 140600, loss: 1.58801, acc: 0.85150, time: Sat Jan 22 06:31:26 2022
epoch 1 training:  72%|███████▏  | 140699/196351 [19:57:38<7:49:23,  1.98it/s]global step 140700, epoch: 1, batch: 140700, loss: 3.84230, acc: 0.85153, time: Sat Jan 22 06:32:16 2022
epoch 1 training:  72%|███████▏  | 140799/196351 [19:58:28<7:47:56,  1.98it/s]global step 140800, epoch: 1, batch: 140800, loss: 1.09851, acc: 0.85157, time: Sat Jan 22 06:33:07 2022
epoch 1 training:  72%|███████▏  | 140899/196351 [19:59:19<7:43:48,  1.99it/s]global step 140900, epoch: 1, batch: 140900, loss: 0.80149, acc: 0.85164, time: Sat Jan 22 06:33:58 2022
epoch 1 training:  72%|███████▏  | 140999/196351 [20:00:10<8:03:53,  1.91it/s]global step 141000, epoch: 1, batch: 141000, loss: 1.86621, acc: 0.85166, time: Sat Jan 22 06:34:49 2022
epoch 1 training:  72%|███████▏  | 141099/196351 [20:01:01<7:51:37,  1.95it/s]global step 141100, epoch: 1, batch: 141100, loss: 3.39149, acc: 0.85169, time: Sat Jan 22 06:35:40 2022
epoch 1 training:  72%|███████▏  | 141199/196351 [20:01:52<7:56:23,  1.93it/s]global step 141200, epoch: 1, batch: 141200, loss: 2.83339, acc: 0.85171, time: Sat Jan 22 06:36:31 2022
epoch 1 training:  72%|███████▏  | 141299/196351 [20:02:43<7:42:30,  1.98it/s]global step 141300, epoch: 1, batch: 141300, loss: 1.66547, acc: 0.85175, time: Sat Jan 22 06:37:22 2022
epoch 1 training:  72%|███████▏  | 141399/196351 [20:03:34<7:53:09,  1.94it/s]global step 141400, epoch: 1, batch: 141400, loss: 0.38421, acc: 0.85178, time: Sat Jan 22 06:38:13 2022
epoch 1 training:  72%|███████▏  | 141499/196351 [20:04:26<7:46:21,  1.96it/s]global step 141500, epoch: 1, batch: 141500, loss: 1.49335, acc: 0.85180, time: Sat Jan 22 06:39:04 2022
epoch 1 training:  72%|███████▏  | 141599/196351 [20:05:17<7:46:33,  1.96it/s]global step 141600, epoch: 1, batch: 141600, loss: 2.63667, acc: 0.85184, time: Sat Jan 22 06:39:56 2022
epoch 1 training:  72%|███████▏  | 141699/196351 [20:06:08<7:53:45,  1.92it/s]global step 141700, epoch: 1, batch: 141700, loss: 1.61868, acc: 0.85185, time: Sat Jan 22 06:40:47 2022
epoch 1 training:  72%|███████▏  | 141799/196351 [20:06:59<7:43:20,  1.96it/s]global step 141800, epoch: 1, batch: 141800, loss: 3.11054, acc: 0.85188, time: Sat Jan 22 06:41:38 2022
epoch 1 training:  72%|███████▏  | 141899/196351 [20:07:50<7:38:29,  1.98it/s]global step 141900, epoch: 1, batch: 141900, loss: 2.65772, acc: 0.85193, time: Sat Jan 22 06:42:29 2022
epoch 1 training:  72%|███████▏  | 141999/196351 [20:08:41<7:43:37,  1.95it/s]global step 142000, epoch: 1, batch: 142000, loss: 1.90770, acc: 0.85195, time: Sat Jan 22 06:43:20 2022
epoch 1 training:  72%|███████▏  | 142099/196351 [20:09:32<7:37:17,  1.98it/s]global step 142100, epoch: 1, batch: 142100, loss: 2.57844, acc: 0.85199, time: Sat Jan 22 06:44:10 2022
epoch 1 training:  72%|███████▏  | 142199/196351 [20:10:23<7:45:17,  1.94it/s]global step 142200, epoch: 1, batch: 142200, loss: 2.14644, acc: 0.85203, time: Sat Jan 22 06:45:02 2022
epoch 1 training:  72%|███████▏  | 142299/196351 [20:11:14<7:48:56,  1.92it/s]global step 142300, epoch: 1, batch: 142300, loss: 2.42808, acc: 0.85205, time: Sat Jan 22 06:45:53 2022
epoch 1 training:  73%|███████▎  | 142399/196351 [20:12:05<7:34:35,  1.98it/s]global step 142400, epoch: 1, batch: 142400, loss: 0.72344, acc: 0.85207, time: Sat Jan 22 06:46:44 2022
epoch 1 training:  73%|███████▎  | 142499/196351 [20:12:56<7:36:57,  1.96it/s]global step 142500, epoch: 1, batch: 142500, loss: 3.03766, acc: 0.85207, time: Sat Jan 22 06:47:35 2022
epoch 1 training:  73%|███████▎  | 142599/196351 [20:13:47<7:37:07,  1.96it/s]global step 142600, epoch: 1, batch: 142600, loss: 3.65645, acc: 0.85211, time: Sat Jan 22 06:48:26 2022
epoch 1 training:  73%|███████▎  | 142699/196351 [20:14:38<7:40:23,  1.94it/s]global step 142700, epoch: 1, batch: 142700, loss: 0.74391, acc: 0.85215, time: Sat Jan 22 06:49:17 2022
epoch 1 training:  73%|███████▎  | 142799/196351 [20:15:29<7:38:58,  1.94it/s]global step 142800, epoch: 1, batch: 142800, loss: 2.34841, acc: 0.85220, time: Sat Jan 22 06:50:08 2022
epoch 1 training:  73%|███████▎  | 142899/196351 [20:16:21<7:32:53,  1.97it/s]global step 142900, epoch: 1, batch: 142900, loss: 2.81717, acc: 0.85223, time: Sat Jan 22 06:50:59 2022
epoch 1 training:  73%|███████▎  | 142999/196351 [20:17:12<7:38:40,  1.94it/s]global step 143000, epoch: 1, batch: 143000, loss: 1.60444, acc: 0.85228, time: Sat Jan 22 06:51:50 2022
epoch 1 training:  73%|███████▎  | 143099/196351 [20:18:03<7:32:07,  1.96it/s]global step 143100, epoch: 1, batch: 143100, loss: 0.80041, acc: 0.85232, time: Sat Jan 22 06:52:42 2022
epoch 1 training:  73%|███████▎  | 143199/196351 [20:18:54<7:35:29,  1.94it/s]global step 143200, epoch: 1, batch: 143200, loss: 1.62084, acc: 0.85235, time: Sat Jan 22 06:53:33 2022
epoch 1 training:  73%|███████▎  | 143299/196351 [20:19:45<7:27:53,  1.97it/s]global step 143300, epoch: 1, batch: 143300, loss: 1.94552, acc: 0.85237, time: Sat Jan 22 06:54:24 2022
epoch 1 training:  73%|███████▎  | 143399/196351 [20:20:37<7:40:45,  1.92it/s]global step 143400, epoch: 1, batch: 143400, loss: 3.37718, acc: 0.85237, time: Sat Jan 22 06:55:15 2022
epoch 1 training:  73%|███████▎  | 143499/196351 [20:21:27<7:27:20,  1.97it/s]global step 143500, epoch: 1, batch: 143500, loss: 4.37381, acc: 0.85239, time: Sat Jan 22 06:56:06 2022
epoch 1 training:  73%|███████▎  | 143599/196351 [20:22:18<7:29:35,  1.96it/s]global step 143600, epoch: 1, batch: 143600, loss: 1.34296, acc: 0.85242, time: Sat Jan 22 06:56:57 2022
epoch 1 training:  73%|███████▎  | 143699/196351 [20:23:09<7:30:53,  1.95it/s]global step 143700, epoch: 1, batch: 143700, loss: 3.46767, acc: 0.85246, time: Sat Jan 22 06:57:48 2022
epoch 1 training:  73%|███████▎  | 143799/196351 [20:24:00<7:22:16,  1.98it/s]global step 143800, epoch: 1, batch: 143800, loss: 2.56882, acc: 0.85250, time: Sat Jan 22 06:58:39 2022
epoch 1 training:  73%|███████▎  | 143899/196351 [20:24:51<7:15:27,  2.01it/s]global step 143900, epoch: 1, batch: 143900, loss: 0.44459, acc: 0.85256, time: Sat Jan 22 06:59:30 2022
epoch 1 training:  73%|███████▎  | 143999/196351 [20:25:42<7:20:25,  1.98it/s]global step 144000, epoch: 1, batch: 144000, loss: 1.13185, acc: 0.85258, time: Sat Jan 22 07:00:20 2022
epoch 1 training:  73%|███████▎  | 144099/196351 [20:26:32<7:27:08,  1.95it/s]global step 144100, epoch: 1, batch: 144100, loss: 1.39197, acc: 0.85262, time: Sat Jan 22 07:01:11 2022
epoch 1 training:  73%|███████▎  | 144199/196351 [20:27:23<7:16:56,  1.99it/s]global step 144200, epoch: 1, batch: 144200, loss: 2.36271, acc: 0.85264, time: Sat Jan 22 07:02:02 2022
epoch 1 training:  73%|███████▎  | 144299/196351 [20:28:14<7:21:09,  1.97it/s]global step 144300, epoch: 1, batch: 144300, loss: 1.89276, acc: 0.85268, time: Sat Jan 22 07:02:53 2022
epoch 1 training:  74%|███████▎  | 144399/196351 [20:29:05<7:20:08,  1.97it/s]global step 144400, epoch: 1, batch: 144400, loss: 2.82243, acc: 0.85274, time: Sat Jan 22 07:03:43 2022
epoch 1 training:  74%|███████▎  | 144499/196351 [20:29:56<7:25:38,  1.94it/s]global step 144500, epoch: 1, batch: 144500, loss: 1.90230, acc: 0.85278, time: Sat Jan 22 07:04:34 2022
epoch 1 training:  74%|███████▎  | 144599/196351 [20:30:47<7:25:33,  1.94it/s]global step 144600, epoch: 1, batch: 144600, loss: 4.19025, acc: 0.85279, time: Sat Jan 22 07:05:26 2022
epoch 1 training:  74%|███████▎  | 144699/196351 [20:31:38<7:20:13,  1.96it/s]global step 144700, epoch: 1, batch: 144700, loss: 2.22116, acc: 0.85280, time: Sat Jan 22 07:06:17 2022
epoch 1 training:  74%|███████▎  | 144799/196351 [20:32:29<7:09:57,  2.00it/s]global step 144800, epoch: 1, batch: 144800, loss: 0.55711, acc: 0.85282, time: Sat Jan 22 07:07:08 2022
epoch 1 training:  74%|███████▍  | 144899/196351 [20:33:21<7:15:51,  1.97it/s]global step 144900, epoch: 1, batch: 144900, loss: 2.07727, acc: 0.85286, time: Sat Jan 22 07:08:00 2022
epoch 1 training:  74%|███████▍  | 144999/196351 [20:34:12<7:18:28,  1.95it/s]global step 145000, epoch: 1, batch: 145000, loss: 1.91207, acc: 0.85290, time: Sat Jan 22 07:08:51 2022
epoch 1 training:  74%|███████▍  | 145099/196351 [20:35:03<7:18:34,  1.95it/s]global step 145100, epoch: 1, batch: 145100, loss: 2.05510, acc: 0.85295, time: Sat Jan 22 07:09:42 2022
epoch 1 training:  74%|███████▍  | 145199/196351 [20:35:54<7:11:18,  1.98it/s]global step 145200, epoch: 1, batch: 145200, loss: 3.20922, acc: 0.85295, time: Sat Jan 22 07:10:32 2022
epoch 1 training:  74%|███████▍  | 145299/196351 [20:36:44<7:02:43,  2.01it/s]global step 145300, epoch: 1, batch: 145300, loss: 0.65185, acc: 0.85301, time: Sat Jan 22 07:11:23 2022
epoch 1 training:  74%|███████▍  | 145399/196351 [20:37:35<7:16:07,  1.95it/s]global step 145400, epoch: 1, batch: 145400, loss: 0.59196, acc: 0.85304, time: Sat Jan 22 07:12:14 2022
epoch 1 training:  74%|███████▍  | 145499/196351 [20:38:25<7:08:15,  1.98it/s]global step 145500, epoch: 1, batch: 145500, loss: 0.52157, acc: 0.85303, time: Sat Jan 22 07:13:04 2022
epoch 1 training:  74%|███████▍  | 145599/196351 [20:39:16<7:15:27,  1.94it/s]global step 145600, epoch: 1, batch: 145600, loss: 1.58939, acc: 0.85306, time: Sat Jan 22 07:13:55 2022
epoch 1 training:  74%|███████▍  | 145699/196351 [20:40:07<7:13:24,  1.95it/s]global step 145700, epoch: 1, batch: 145700, loss: 2.74406, acc: 0.85311, time: Sat Jan 22 07:14:46 2022
epoch 1 training:  74%|███████▍  | 145799/196351 [20:40:58<7:15:50,  1.93it/s]global step 145800, epoch: 1, batch: 145800, loss: 2.65685, acc: 0.85316, time: Sat Jan 22 07:15:37 2022
epoch 1 training:  74%|███████▍  | 145899/196351 [20:41:49<7:05:15,  1.98it/s]global step 145900, epoch: 1, batch: 145900, loss: 2.00305, acc: 0.85320, time: Sat Jan 22 07:16:28 2022
epoch 1 training:  74%|███████▍  | 145999/196351 [20:42:40<7:02:56,  1.98it/s]global step 146000, epoch: 1, batch: 146000, loss: 2.26645, acc: 0.85326, time: Sat Jan 22 07:17:19 2022
epoch 1 training:  74%|███████▍  | 146099/196351 [20:43:31<7:07:55,  1.96it/s]global step 146100, epoch: 1, batch: 146100, loss: 1.95845, acc: 0.85330, time: Sat Jan 22 07:18:10 2022
epoch 1 training:  74%|███████▍  | 146199/196351 [20:44:22<7:01:53,  1.98it/s]global step 146200, epoch: 1, batch: 146200, loss: 1.42349, acc: 0.85335, time: Sat Jan 22 07:19:00 2022
epoch 1 training:  75%|███████▍  | 146299/196351 [20:45:12<7:08:44,  1.95it/s]global step 146300, epoch: 1, batch: 146300, loss: 0.66165, acc: 0.85338, time: Sat Jan 22 07:19:51 2022
epoch 1 training:  75%|███████▍  | 146399/196351 [20:46:03<7:00:43,  1.98it/s]global step 146400, epoch: 1, batch: 146400, loss: 1.13974, acc: 0.85341, time: Sat Jan 22 07:20:42 2022
epoch 1 training:  75%|███████▍  | 146499/196351 [20:46:54<7:05:33,  1.95it/s]global step 146500, epoch: 1, batch: 146500, loss: 2.39498, acc: 0.85344, time: Sat Jan 22 07:21:32 2022
epoch 1 training:  75%|███████▍  | 146599/196351 [20:47:45<6:58:01,  1.98it/s]global step 146600, epoch: 1, batch: 146600, loss: 2.72339, acc: 0.85344, time: Sat Jan 22 07:22:23 2022
epoch 1 training:  75%|███████▍  | 146699/196351 [20:48:36<6:59:09,  1.97it/s]global step 146700, epoch: 1, batch: 146700, loss: 1.24358, acc: 0.85345, time: Sat Jan 22 07:23:15 2022
epoch 1 training:  75%|███████▍  | 146799/196351 [20:49:27<6:56:53,  1.98it/s]global step 146800, epoch: 1, batch: 146800, loss: 3.74415, acc: 0.85348, time: Sat Jan 22 07:24:05 2022
epoch 1 training:  75%|███████▍  | 146899/196351 [20:50:17<6:56:44,  1.98it/s]global step 146900, epoch: 1, batch: 146900, loss: 1.81217, acc: 0.85353, time: Sat Jan 22 07:24:56 2022
epoch 1 training:  75%|███████▍  | 146999/196351 [20:51:08<6:59:35,  1.96it/s]global step 147000, epoch: 1, batch: 147000, loss: 2.26837, acc: 0.85356, time: Sat Jan 22 07:25:47 2022
epoch 1 training:  75%|███████▍  | 147099/196351 [20:51:59<7:02:11,  1.94it/s]global step 147100, epoch: 1, batch: 147100, loss: 1.92249, acc: 0.85359, time: Sat Jan 22 07:26:38 2022
epoch 1 training:  75%|███████▍  | 147199/196351 [20:52:50<6:52:59,  1.98it/s]global step 147200, epoch: 1, batch: 147200, loss: 2.62648, acc: 0.85361, time: Sat Jan 22 07:27:29 2022
epoch 1 training:  75%|███████▌  | 147299/196351 [20:53:41<6:52:35,  1.98it/s]global step 147300, epoch: 1, batch: 147300, loss: 0.98729, acc: 0.85364, time: Sat Jan 22 07:28:20 2022
epoch 1 training:  75%|███████▌  | 147399/196351 [20:54:32<6:54:56,  1.97it/s]global step 147400, epoch: 1, batch: 147400, loss: 0.73690, acc: 0.85368, time: Sat Jan 22 07:29:11 2022
epoch 1 training:  75%|███████▌  | 147499/196351 [20:55:23<7:00:12,  1.94it/s]global step 147500, epoch: 1, batch: 147500, loss: 0.35306, acc: 0.85374, time: Sat Jan 22 07:30:02 2022
epoch 1 training:  75%|███████▌  | 147599/196351 [20:56:14<6:51:13,  1.98it/s]global step 147600, epoch: 1, batch: 147600, loss: 1.30023, acc: 0.85377, time: Sat Jan 22 07:30:53 2022
epoch 1 training:  75%|███████▌  | 147699/196351 [20:57:05<6:51:58,  1.97it/s]global step 147700, epoch: 1, batch: 147700, loss: 2.88295, acc: 0.85380, time: Sat Jan 22 07:31:44 2022
epoch 1 training:  75%|███████▌  | 147799/196351 [20:57:57<6:56:23,  1.94it/s]global step 147800, epoch: 1, batch: 147800, loss: 1.72043, acc: 0.85383, time: Sat Jan 22 07:32:35 2022
epoch 1 training:  75%|███████▌  | 147899/196351 [20:58:48<6:54:32,  1.95it/s]global step 147900, epoch: 1, batch: 147900, loss: 1.91435, acc: 0.85386, time: Sat Jan 22 07:33:26 2022
epoch 1 training:  75%|███████▌  | 147999/196351 [20:59:39<6:55:14,  1.94it/s]global step 148000, epoch: 1, batch: 148000, loss: 3.22431, acc: 0.85391, time: Sat Jan 22 07:34:18 2022
epoch 1 training:  75%|███████▌  | 148099/196351 [21:00:29<6:48:56,  1.97it/s]global step 148100, epoch: 1, batch: 148100, loss: 1.18568, acc: 0.85394, time: Sat Jan 22 07:35:08 2022
epoch 1 training:  75%|███████▌  | 148199/196351 [21:01:20<6:43:17,  1.99it/s]global step 148200, epoch: 1, batch: 148200, loss: 1.89487, acc: 0.85397, time: Sat Jan 22 07:35:59 2022
epoch 1 training:  76%|███████▌  | 148299/196351 [21:02:11<6:40:05,  2.00it/s]global step 148300, epoch: 1, batch: 148300, loss: 2.26344, acc: 0.85401, time: Sat Jan 22 07:36:49 2022
epoch 1 training:  76%|███████▌  | 148399/196351 [21:03:02<6:51:44,  1.94it/s]global step 148400, epoch: 1, batch: 148400, loss: 4.34209, acc: 0.85404, time: Sat Jan 22 07:37:40 2022
epoch 1 training:  76%|███████▌  | 148499/196351 [21:03:52<6:44:34,  1.97it/s]global step 148500, epoch: 1, batch: 148500, loss: 3.49891, acc: 0.85406, time: Sat Jan 22 07:38:31 2022
epoch 1 training:  76%|███████▌  | 148599/196351 [21:04:43<6:49:25,  1.94it/s]global step 148600, epoch: 1, batch: 148600, loss: 2.06172, acc: 0.85408, time: Sat Jan 22 07:39:22 2022
epoch 1 training:  76%|███████▌  | 148699/196351 [21:05:34<6:41:35,  1.98it/s]global step 148700, epoch: 1, batch: 148700, loss: 2.33738, acc: 0.85411, time: Sat Jan 22 07:40:12 2022
epoch 1 training:  76%|███████▌  | 148799/196351 [21:06:25<6:39:00,  1.99it/s]global step 148800, epoch: 1, batch: 148800, loss: 2.95892, acc: 0.85414, time: Sat Jan 22 07:41:04 2022
epoch 1 training:  76%|███████▌  | 148899/196351 [21:07:16<6:39:44,  1.98it/s]global step 148900, epoch: 1, batch: 148900, loss: 2.53095, acc: 0.85417, time: Sat Jan 22 07:41:55 2022
epoch 1 training:  76%|███████▌  | 148999/196351 [21:08:07<6:54:18,  1.90it/s]global step 149000, epoch: 1, batch: 149000, loss: 1.75237, acc: 0.85418, time: Sat Jan 22 07:42:46 2022
epoch 1 training:  76%|███████▌  | 149099/196351 [21:08:58<6:44:17,  1.95it/s]global step 149100, epoch: 1, batch: 149100, loss: 0.83180, acc: 0.85425, time: Sat Jan 22 07:43:37 2022
epoch 1 training:  76%|███████▌  | 149199/196351 [21:09:49<6:42:40,  1.95it/s]global step 149200, epoch: 1, batch: 149200, loss: 1.46103, acc: 0.85427, time: Sat Jan 22 07:44:28 2022
epoch 1 training:  76%|███████▌  | 149299/196351 [21:10:40<6:39:38,  1.96it/s]global step 149300, epoch: 1, batch: 149300, loss: 0.97294, acc: 0.85428, time: Sat Jan 22 07:45:19 2022
epoch 1 training:  76%|███████▌  | 149399/196351 [21:11:32<6:39:46,  1.96it/s]global step 149400, epoch: 1, batch: 149400, loss: 3.70136, acc: 0.85434, time: Sat Jan 22 07:46:10 2022
epoch 1 training:  76%|███████▌  | 149499/196351 [21:12:23<6:45:21,  1.93it/s]global step 149500, epoch: 1, batch: 149500, loss: 2.09687, acc: 0.85439, time: Sat Jan 22 07:47:01 2022
epoch 1 training:  76%|███████▌  | 149599/196351 [21:13:14<6:31:17,  1.99it/s]global step 149600, epoch: 1, batch: 149600, loss: 2.38758, acc: 0.85442, time: Sat Jan 22 07:47:52 2022
epoch 1 training:  76%|███████▌  | 149699/196351 [21:14:04<6:36:04,  1.96it/s]global step 149700, epoch: 1, batch: 149700, loss: 1.04657, acc: 0.85442, time: Sat Jan 22 07:48:43 2022
epoch 1 training:  76%|███████▋  | 149799/196351 [21:14:55<6:32:04,  1.98it/s]global step 149800, epoch: 1, batch: 149800, loss: 0.85565, acc: 0.85445, time: Sat Jan 22 07:49:34 2022
epoch 1 training:  76%|███████▋  | 149899/196351 [21:15:46<6:30:29,  1.98it/s]global step 149900, epoch: 1, batch: 149900, loss: 1.18479, acc: 0.85449, time: Sat Jan 22 07:50:25 2022
epoch 1 training:  76%|███████▋  | 149999/196351 [21:16:37<6:32:59,  1.97it/s]global step 150000, epoch: 1, batch: 150000, loss: 4.03183, acc: 0.85451, time: Sat Jan 22 07:51:16 2022
epoch 1 training:  76%|███████▋  | 150099/196351 [21:17:28<6:31:02,  1.97it/s]global step 150100, epoch: 1, batch: 150100, loss: 2.22116, acc: 0.85455, time: Sat Jan 22 07:52:06 2022
epoch 1 training:  76%|███████▋  | 150199/196351 [21:18:18<6:28:08,  1.98it/s]global step 150200, epoch: 1, batch: 150200, loss: 0.85887, acc: 0.85460, time: Sat Jan 22 07:52:57 2022
epoch 1 training:  77%|███████▋  | 150299/196351 [21:19:09<6:28:24,  1.98it/s]global step 150300, epoch: 1, batch: 150300, loss: 3.49812, acc: 0.85462, time: Sat Jan 22 07:53:48 2022
epoch 1 training:  77%|███████▋  | 150399/196351 [21:20:00<6:29:47,  1.96it/s]global step 150400, epoch: 1, batch: 150400, loss: 1.39889, acc: 0.85465, time: Sat Jan 22 07:54:39 2022
epoch 1 training:  77%|███████▋  | 150499/196351 [21:20:51<6:25:27,  1.98it/s]global step 150500, epoch: 1, batch: 150500, loss: 1.71274, acc: 0.85468, time: Sat Jan 22 07:55:30 2022
epoch 1 training:  77%|███████▋  | 150599/196351 [21:21:43<6:32:46,  1.94it/s]global step 150600, epoch: 1, batch: 150600, loss: 2.14576, acc: 0.85470, time: Sat Jan 22 07:56:22 2022
epoch 1 training:  77%|███████▋  | 150699/196351 [21:22:34<6:29:17,  1.95it/s]global step 150700, epoch: 1, batch: 150700, loss: 0.90849, acc: 0.85473, time: Sat Jan 22 07:57:13 2022
epoch 1 training:  77%|███████▋  | 150799/196351 [21:23:25<6:33:23,  1.93it/s]global step 150800, epoch: 1, batch: 150800, loss: 3.45219, acc: 0.85477, time: Sat Jan 22 07:58:04 2022
epoch 1 training:  77%|███████▋  | 150899/196351 [21:24:16<6:20:53,  1.99it/s]global step 150900, epoch: 1, batch: 150900, loss: 1.00178, acc: 0.85481, time: Sat Jan 22 07:58:55 2022
epoch 1 training:  77%|███████▋  | 150999/196351 [21:25:07<6:17:11,  2.00it/s]global step 151000, epoch: 1, batch: 151000, loss: 3.25390, acc: 0.85483, time: Sat Jan 22 07:59:46 2022
epoch 1 training:  77%|███████▋  | 151099/196351 [21:25:58<6:23:27,  1.97it/s]global step 151100, epoch: 1, batch: 151100, loss: 1.82613, acc: 0.85483, time: Sat Jan 22 08:00:37 2022
epoch 1 training:  77%|███████▋  | 151199/196351 [21:26:49<6:25:11,  1.95it/s]global step 151200, epoch: 1, batch: 151200, loss: 2.14041, acc: 0.85485, time: Sat Jan 22 08:01:28 2022
epoch 1 training:  77%|███████▋  | 151299/196351 [21:27:40<6:17:58,  1.99it/s]global step 151300, epoch: 1, batch: 151300, loss: 2.53724, acc: 0.85487, time: Sat Jan 22 08:02:18 2022
epoch 1 training:  77%|███████▋  | 151399/196351 [21:28:30<6:21:45,  1.96it/s]global step 151400, epoch: 1, batch: 151400, loss: 1.69068, acc: 0.85489, time: Sat Jan 22 08:03:09 2022
epoch 1 training:  77%|███████▋  | 151499/196351 [21:29:21<6:22:28,  1.95it/s]global step 151500, epoch: 1, batch: 151500, loss: 2.91910, acc: 0.85491, time: Sat Jan 22 08:04:00 2022
epoch 1 training:  77%|███████▋  | 151599/196351 [21:30:12<7:34:17,  1.64it/s]global step 151600, epoch: 1, batch: 151600, loss: 2.00946, acc: 0.85493, time: Sat Jan 22 08:04:51 2022
epoch 1 training:  77%|███████▋  | 151699/196351 [21:31:03<6:21:55,  1.95it/s]global step 151700, epoch: 1, batch: 151700, loss: 4.91824, acc: 0.85494, time: Sat Jan 22 08:05:42 2022
epoch 1 training:  77%|███████▋  | 151799/196351 [21:31:54<6:26:28,  1.92it/s]global step 151800, epoch: 1, batch: 151800, loss: 4.07081, acc: 0.85497, time: Sat Jan 22 08:06:33 2022
epoch 1 training:  77%|███████▋  | 151899/196351 [21:32:46<6:21:02,  1.94it/s]global step 151900, epoch: 1, batch: 151900, loss: 2.23107, acc: 0.85497, time: Sat Jan 22 08:07:24 2022
epoch 1 training:  77%|███████▋  | 151999/196351 [21:33:37<6:21:36,  1.94it/s]global step 152000, epoch: 1, batch: 152000, loss: 2.14649, acc: 0.85501, time: Sat Jan 22 08:08:15 2022
epoch 1 training:  77%|███████▋  | 152099/196351 [21:34:28<6:20:35,  1.94it/s]global step 152100, epoch: 1, batch: 152100, loss: 1.52180, acc: 0.85505, time: Sat Jan 22 08:09:06 2022
epoch 1 training:  78%|███████▊  | 152199/196351 [21:35:19<6:18:45,  1.94it/s]global step 152200, epoch: 1, batch: 152200, loss: 3.35625, acc: 0.85506, time: Sat Jan 22 08:09:57 2022
epoch 1 training:  78%|███████▊  | 152299/196351 [21:36:09<6:15:04,  1.96it/s]global step 152300, epoch: 1, batch: 152300, loss: 3.37453, acc: 0.85504, time: Sat Jan 22 08:10:48 2022
epoch 1 training:  78%|███████▊  | 152399/196351 [21:37:00<6:17:39,  1.94it/s]global step 152400, epoch: 1, batch: 152400, loss: 3.60760, acc: 0.85506, time: Sat Jan 22 08:11:39 2022
epoch 1 training:  78%|███████▊  | 152499/196351 [21:37:51<6:09:43,  1.98it/s]global step 152500, epoch: 1, batch: 152500, loss: 2.71196, acc: 0.85508, time: Sat Jan 22 08:12:30 2022
epoch 1 training:  78%|███████▊  | 152599/196351 [21:38:43<6:14:19,  1.95it/s]global step 152600, epoch: 1, batch: 152600, loss: 1.09894, acc: 0.85510, time: Sat Jan 22 08:13:22 2022
epoch 1 training:  78%|███████▊  | 152699/196351 [21:39:33<6:16:23,  1.93it/s]global step 152700, epoch: 1, batch: 152700, loss: 2.71407, acc: 0.85517, time: Sat Jan 22 08:14:12 2022
epoch 1 training:  78%|███████▊  | 152799/196351 [21:40:24<6:10:55,  1.96it/s]global step 152800, epoch: 1, batch: 152800, loss: 0.84300, acc: 0.85520, time: Sat Jan 22 08:15:03 2022
epoch 1 training:  78%|███████▊  | 152899/196351 [21:41:15<6:05:11,  1.98it/s]global step 152900, epoch: 1, batch: 152900, loss: 4.31509, acc: 0.85522, time: Sat Jan 22 08:15:54 2022
epoch 1 training:  78%|███████▊  | 152999/196351 [21:42:06<6:04:22,  1.98it/s]global step 153000, epoch: 1, batch: 153000, loss: 4.11273, acc: 0.85525, time: Sat Jan 22 08:16:45 2022
epoch 1 training:  78%|███████▊  | 153099/196351 [21:42:57<6:14:10,  1.93it/s]global step 153100, epoch: 1, batch: 153100, loss: 0.67090, acc: 0.85526, time: Sat Jan 22 08:17:35 2022
epoch 1 training:  78%|███████▊  | 153199/196351 [21:43:47<6:13:54,  1.92it/s]global step 153200, epoch: 1, batch: 153200, loss: 3.91648, acc: 0.85530, time: Sat Jan 22 08:18:26 2022
epoch 1 training:  78%|███████▊  | 153299/196351 [21:44:38<6:04:34,  1.97it/s]global step 153300, epoch: 1, batch: 153300, loss: 2.45511, acc: 0.85534, time: Sat Jan 22 08:19:17 2022
epoch 1 training:  78%|███████▊  | 153399/196351 [21:45:29<6:04:46,  1.96it/s]global step 153400, epoch: 1, batch: 153400, loss: 2.00976, acc: 0.85537, time: Sat Jan 22 08:20:08 2022
epoch 1 training:  78%|███████▊  | 153499/196351 [21:46:20<5:58:48,  1.99it/s]global step 153500, epoch: 1, batch: 153500, loss: 1.82012, acc: 0.85538, time: Sat Jan 22 08:20:59 2022
epoch 1 training:  78%|███████▊  | 153599/196351 [21:47:11<6:04:52,  1.95it/s]global step 153600, epoch: 1, batch: 153600, loss: 4.46292, acc: 0.85539, time: Sat Jan 22 08:21:49 2022
epoch 1 training:  78%|███████▊  | 153699/196351 [21:48:01<6:00:14,  1.97it/s]global step 153700, epoch: 1, batch: 153700, loss: 1.81754, acc: 0.85543, time: Sat Jan 22 08:22:40 2022
epoch 1 training:  78%|███████▊  | 153799/196351 [21:48:52<6:00:25,  1.97it/s]global step 153800, epoch: 1, batch: 153800, loss: 4.01484, acc: 0.85545, time: Sat Jan 22 08:23:31 2022
epoch 1 training:  78%|███████▊  | 153899/196351 [21:49:43<5:58:23,  1.97it/s]global step 153900, epoch: 1, batch: 153900, loss: 0.81471, acc: 0.85550, time: Sat Jan 22 08:24:22 2022
epoch 1 training:  78%|███████▊  | 153999/196351 [21:50:33<5:53:15,  2.00it/s]global step 154000, epoch: 1, batch: 154000, loss: 2.26151, acc: 0.85553, time: Sat Jan 22 08:25:12 2022
epoch 1 training:  78%|███████▊  | 154099/196351 [21:51:24<6:00:01,  1.96it/s]global step 154100, epoch: 1, batch: 154100, loss: 3.09509, acc: 0.85553, time: Sat Jan 22 08:26:03 2022
epoch 1 training:  79%|███████▊  | 154199/196351 [21:52:15<5:55:08,  1.98it/s]global step 154200, epoch: 1, batch: 154200, loss: 2.30961, acc: 0.85556, time: Sat Jan 22 08:26:53 2022
epoch 1 training:  79%|███████▊  | 154299/196351 [21:53:05<5:54:13,  1.98it/s]global step 154300, epoch: 1, batch: 154300, loss: 4.24445, acc: 0.85557, time: Sat Jan 22 08:27:44 2022
epoch 1 training:  79%|███████▊  | 154399/196351 [21:53:56<5:57:29,  1.96it/s]global step 154400, epoch: 1, batch: 154400, loss: 1.06574, acc: 0.85559, time: Sat Jan 22 08:28:35 2022
epoch 1 training:  79%|███████▊  | 154499/196351 [21:54:47<5:54:39,  1.97it/s]global step 154500, epoch: 1, batch: 154500, loss: 0.78563, acc: 0.85561, time: Sat Jan 22 08:29:26 2022
epoch 1 training:  79%|███████▊  | 154599/196351 [21:55:39<5:59:07,  1.94it/s]global step 154600, epoch: 1, batch: 154600, loss: 1.24009, acc: 0.85564, time: Sat Jan 22 08:30:17 2022
epoch 1 training:  79%|███████▉  | 154699/196351 [21:56:30<5:51:06,  1.98it/s]global step 154700, epoch: 1, batch: 154700, loss: 0.55163, acc: 0.85567, time: Sat Jan 22 08:31:09 2022
epoch 1 training:  79%|███████▉  | 154799/196351 [21:57:21<5:51:58,  1.97it/s]global step 154800, epoch: 1, batch: 154800, loss: 3.17782, acc: 0.85572, time: Sat Jan 22 08:32:00 2022
epoch 1 training:  79%|███████▉  | 154899/196351 [21:58:12<5:55:46,  1.94it/s]global step 154900, epoch: 1, batch: 154900, loss: 4.14332, acc: 0.85574, time: Sat Jan 22 08:32:51 2022
epoch 1 training:  79%|███████▉  | 154999/196351 [21:59:03<5:55:01,  1.94it/s]global step 155000, epoch: 1, batch: 155000, loss: 1.50542, acc: 0.85576, time: Sat Jan 22 08:33:42 2022
epoch 1 training:  79%|███████▉  | 155099/196351 [21:59:54<5:49:55,  1.96it/s]global step 155100, epoch: 1, batch: 155100, loss: 1.53128, acc: 0.85578, time: Sat Jan 22 08:34:33 2022
epoch 1 training:  79%|███████▉  | 155199/196351 [22:00:45<5:45:15,  1.99it/s]global step 155200, epoch: 1, batch: 155200, loss: 4.59867, acc: 0.85582, time: Sat Jan 22 08:35:24 2022
epoch 1 training:  79%|███████▉  | 155299/196351 [22:01:36<5:43:11,  1.99it/s]global step 155300, epoch: 1, batch: 155300, loss: 3.25543, acc: 0.85582, time: Sat Jan 22 08:36:15 2022
epoch 1 training:  79%|███████▉  | 155399/196351 [22:02:27<5:44:50,  1.98it/s]global step 155400, epoch: 1, batch: 155400, loss: 2.79416, acc: 0.85584, time: Sat Jan 22 08:37:05 2022
epoch 1 training:  79%|███████▉  | 155499/196351 [22:03:18<5:41:17,  1.99it/s]global step 155500, epoch: 1, batch: 155500, loss: 0.40825, acc: 0.85586, time: Sat Jan 22 08:37:56 2022
epoch 1 training:  79%|███████▉  | 155599/196351 [22:04:08<5:43:37,  1.98it/s]global step 155600, epoch: 1, batch: 155600, loss: 3.76218, acc: 0.85587, time: Sat Jan 22 08:38:47 2022
epoch 1 training:  79%|███████▉  | 155699/196351 [22:04:59<5:49:10,  1.94it/s]global step 155700, epoch: 1, batch: 155700, loss: 0.88188, acc: 0.85589, time: Sat Jan 22 08:39:38 2022
epoch 1 training:  79%|███████▉  | 155799/196351 [22:05:50<5:45:27,  1.96it/s]global step 155800, epoch: 1, batch: 155800, loss: 1.15663, acc: 0.85591, time: Sat Jan 22 08:40:29 2022
epoch 1 training:  79%|███████▉  | 155899/196351 [22:06:41<5:43:51,  1.96it/s]global step 155900, epoch: 1, batch: 155900, loss: 0.32783, acc: 0.85593, time: Sat Jan 22 08:41:20 2022
epoch 1 training:  79%|███████▉  | 155999/196351 [22:07:32<5:40:06,  1.98it/s]global step 156000, epoch: 1, batch: 156000, loss: 1.54415, acc: 0.85593, time: Sat Jan 22 08:42:11 2022
epoch 1 training:  79%|███████▉  | 156099/196351 [22:08:24<5:47:34,  1.93it/s]global step 156100, epoch: 1, batch: 156100, loss: 3.51276, acc: 0.85595, time: Sat Jan 22 08:43:02 2022
epoch 1 training:  80%|███████▉  | 156199/196351 [22:09:15<5:46:03,  1.93it/s]global step 156200, epoch: 1, batch: 156200, loss: 2.28497, acc: 0.85597, time: Sat Jan 22 08:43:53 2022
epoch 1 training:  80%|███████▉  | 156299/196351 [22:10:06<5:38:03,  1.97it/s]global step 156300, epoch: 1, batch: 156300, loss: 1.79579, acc: 0.85599, time: Sat Jan 22 08:44:44 2022
epoch 1 training:  80%|███████▉  | 156399/196351 [22:10:57<5:37:37,  1.97it/s]global step 156400, epoch: 1, batch: 156400, loss: 2.70690, acc: 0.85601, time: Sat Jan 22 08:45:36 2022
epoch 1 training:  80%|███████▉  | 156499/196351 [22:11:48<5:39:32,  1.96it/s]global step 156500, epoch: 1, batch: 156500, loss: 1.76352, acc: 0.85603, time: Sat Jan 22 08:46:27 2022
epoch 1 training:  80%|███████▉  | 156599/196351 [22:12:39<5:37:21,  1.96it/s]global step 156600, epoch: 1, batch: 156600, loss: 1.29550, acc: 0.85606, time: Sat Jan 22 08:47:18 2022
epoch 1 training:  80%|███████▉  | 156699/196351 [22:13:30<5:33:58,  1.98it/s]global step 156700, epoch: 1, batch: 156700, loss: 2.60651, acc: 0.85608, time: Sat Jan 22 08:48:09 2022
epoch 1 training:  80%|███████▉  | 156799/196351 [22:14:21<5:36:30,  1.96it/s]global step 156800, epoch: 1, batch: 156800, loss: 4.09361, acc: 0.85609, time: Sat Jan 22 08:49:00 2022
epoch 1 training:  80%|███████▉  | 156899/196351 [22:15:11<5:37:39,  1.95it/s]global step 156900, epoch: 1, batch: 156900, loss: 1.73902, acc: 0.85612, time: Sat Jan 22 08:49:50 2022
epoch 1 training:  80%|███████▉  | 156999/196351 [22:16:02<5:32:14,  1.97it/s]global step 157000, epoch: 1, batch: 157000, loss: 1.92623, acc: 0.85615, time: Sat Jan 22 08:50:41 2022
epoch 1 training:  80%|████████  | 157099/196351 [22:16:53<5:31:40,  1.97it/s]global step 157100, epoch: 1, batch: 157100, loss: 3.46216, acc: 0.85616, time: Sat Jan 22 08:51:32 2022
epoch 1 training:  80%|████████  | 157199/196351 [22:17:44<5:34:15,  1.95it/s]global step 157200, epoch: 1, batch: 157200, loss: 1.31116, acc: 0.85616, time: Sat Jan 22 08:52:22 2022
epoch 1 training:  80%|████████  | 157299/196351 [22:18:35<5:34:07,  1.95it/s]global step 157300, epoch: 1, batch: 157300, loss: 2.31043, acc: 0.85620, time: Sat Jan 22 08:53:13 2022
epoch 1 training:  80%|████████  | 157399/196351 [22:19:26<5:31:43,  1.96it/s]global step 157400, epoch: 1, batch: 157400, loss: 0.86022, acc: 0.85623, time: Sat Jan 22 08:54:04 2022
epoch 1 training:  80%|████████  | 157499/196351 [22:20:17<5:30:14,  1.96it/s]global step 157500, epoch: 1, batch: 157500, loss: 0.38033, acc: 0.85626, time: Sat Jan 22 08:54:56 2022
epoch 1 training:  80%|████████  | 157599/196351 [22:21:08<5:25:55,  1.98it/s]global step 157600, epoch: 1, batch: 157600, loss: 0.48621, acc: 0.85629, time: Sat Jan 22 08:55:47 2022
epoch 1 training:  80%|████████  | 157699/196351 [22:21:59<5:26:29,  1.97it/s]global step 157700, epoch: 1, batch: 157700, loss: 0.95158, acc: 0.85631, time: Sat Jan 22 08:56:38 2022
epoch 1 training:  80%|████████  | 157799/196351 [22:22:50<5:29:01,  1.95it/s]global step 157800, epoch: 1, batch: 157800, loss: 1.64091, acc: 0.85631, time: Sat Jan 22 08:57:29 2022
epoch 1 training:  80%|████████  | 157899/196351 [22:23:41<5:24:50,  1.97it/s]global step 157900, epoch: 1, batch: 157900, loss: 2.58981, acc: 0.85632, time: Sat Jan 22 08:58:20 2022
epoch 1 training:  80%|████████  | 157999/196351 [22:24:32<5:20:46,  1.99it/s]global step 158000, epoch: 1, batch: 158000, loss: 2.29234, acc: 0.85635, time: Sat Jan 22 08:59:11 2022
epoch 1 training:  81%|████████  | 158099/196351 [22:25:23<5:24:20,  1.97it/s]global step 158100, epoch: 1, batch: 158100, loss: 0.81672, acc: 0.85641, time: Sat Jan 22 09:00:01 2022
epoch 1 training:  81%|████████  | 158199/196351 [22:26:13<5:19:37,  1.99it/s]global step 158200, epoch: 1, batch: 158200, loss: 1.27168, acc: 0.85645, time: Sat Jan 22 09:00:52 2022
epoch 1 training:  81%|████████  | 158299/196351 [22:27:05<5:20:12,  1.98it/s]global step 158300, epoch: 1, batch: 158300, loss: 0.89362, acc: 0.85646, time: Sat Jan 22 09:01:43 2022
epoch 1 training:  81%|████████  | 158399/196351 [22:27:55<5:22:28,  1.96it/s]global step 158400, epoch: 1, batch: 158400, loss: 2.95964, acc: 0.85648, time: Sat Jan 22 09:02:34 2022
epoch 1 training:  81%|████████  | 158499/196351 [22:28:46<5:23:46,  1.95it/s]global step 158500, epoch: 1, batch: 158500, loss: 0.74191, acc: 0.85649, time: Sat Jan 22 09:03:25 2022
epoch 1 training:  81%|████████  | 158599/196351 [22:29:37<5:25:13,  1.93it/s]global step 158600, epoch: 1, batch: 158600, loss: 2.55848, acc: 0.85653, time: Sat Jan 22 09:04:16 2022
epoch 1 training:  81%|████████  | 158699/196351 [22:30:28<5:21:23,  1.95it/s]global step 158700, epoch: 1, batch: 158700, loss: 1.38053, acc: 0.85655, time: Sat Jan 22 09:05:07 2022
epoch 1 training:  81%|████████  | 158799/196351 [22:31:19<5:15:10,  1.99it/s]global step 158800, epoch: 1, batch: 158800, loss: 0.93333, acc: 0.85656, time: Sat Jan 22 09:05:58 2022
epoch 1 training:  81%|████████  | 158899/196351 [22:32:10<5:22:08,  1.94it/s]global step 158900, epoch: 1, batch: 158900, loss: 2.82879, acc: 0.85661, time: Sat Jan 22 09:06:49 2022
epoch 1 training:  81%|████████  | 158999/196351 [22:33:01<5:12:54,  1.99it/s]global step 159000, epoch: 1, batch: 159000, loss: 0.83586, acc: 0.85663, time: Sat Jan 22 09:07:40 2022
epoch 1 training:  81%|████████  | 159099/196351 [22:33:52<5:17:00,  1.96it/s]global step 159100, epoch: 1, batch: 159100, loss: 3.70557, acc: 0.85666, time: Sat Jan 22 09:08:31 2022
epoch 1 training:  81%|████████  | 159199/196351 [22:34:43<5:11:36,  1.99it/s]global step 159200, epoch: 1, batch: 159200, loss: 1.33316, acc: 0.85667, time: Sat Jan 22 09:09:22 2022
epoch 1 training:  81%|████████  | 159299/196351 [22:35:34<5:21:12,  1.92it/s]global step 159300, epoch: 1, batch: 159300, loss: 0.96582, acc: 0.85669, time: Sat Jan 22 09:10:13 2022
epoch 1 training:  81%|████████  | 159399/196351 [22:36:25<5:12:38,  1.97it/s]global step 159400, epoch: 1, batch: 159400, loss: 2.84048, acc: 0.85671, time: Sat Jan 22 09:11:04 2022
epoch 1 training:  81%|████████  | 159499/196351 [22:37:16<5:08:47,  1.99it/s]global step 159500, epoch: 1, batch: 159500, loss: 0.90058, acc: 0.85674, time: Sat Jan 22 09:11:55 2022
epoch 1 training:  81%|████████▏ | 159599/196351 [22:38:07<5:08:16,  1.99it/s]global step 159600, epoch: 1, batch: 159600, loss: 0.75443, acc: 0.85677, time: Sat Jan 22 09:12:45 2022
epoch 1 training:  81%|████████▏ | 159699/196351 [22:38:57<5:09:55,  1.97it/s]global step 159700, epoch: 1, batch: 159700, loss: 1.04882, acc: 0.85683, time: Sat Jan 22 09:13:36 2022
epoch 1 training:  81%|████████▏ | 159799/196351 [22:39:48<5:10:28,  1.96it/s]global step 159800, epoch: 1, batch: 159800, loss: 2.18829, acc: 0.85685, time: Sat Jan 22 09:14:27 2022
epoch 1 training:  81%|████████▏ | 159899/196351 [22:40:39<5:13:41,  1.94it/s]global step 159900, epoch: 1, batch: 159900, loss: 2.70632, acc: 0.85688, time: Sat Jan 22 09:15:18 2022
epoch 1 training:  81%|████████▏ | 159999/196351 [22:41:30<5:13:22,  1.93it/s]global step 160000, epoch: 1, batch: 160000, loss: 1.81308, acc: 0.85690, time: Sat Jan 22 09:16:09 2022
epoch 1 training:  82%|████████▏ | 160099/196351 [22:42:21<5:06:04,  1.97it/s]global step 160100, epoch: 1, batch: 160100, loss: 4.74154, acc: 0.85692, time: Sat Jan 22 09:17:00 2022
epoch 1 training:  82%|████████▏ | 160199/196351 [22:43:12<5:06:07,  1.97it/s]global step 160200, epoch: 1, batch: 160200, loss: 0.84367, acc: 0.85694, time: Sat Jan 22 09:17:50 2022
epoch 1 training:  82%|████████▏ | 160299/196351 [22:44:03<5:07:20,  1.96it/s]global step 160300, epoch: 1, batch: 160300, loss: 3.11129, acc: 0.85698, time: Sat Jan 22 09:18:41 2022
epoch 1 training:  82%|████████▏ | 160399/196351 [22:44:54<5:06:19,  1.96it/s]global step 160400, epoch: 1, batch: 160400, loss: 0.94826, acc: 0.85700, time: Sat Jan 22 09:19:33 2022
epoch 1 training:  82%|████████▏ | 160499/196351 [22:45:45<4:59:22,  2.00it/s]global step 160500, epoch: 1, batch: 160500, loss: 1.93442, acc: 0.85701, time: Sat Jan 22 09:20:24 2022
epoch 1 training:  82%|████████▏ | 160599/196351 [22:46:36<4:58:34,  2.00it/s]global step 160600, epoch: 1, batch: 160600, loss: 1.04705, acc: 0.85705, time: Sat Jan 22 09:21:14 2022
epoch 1 training:  82%|████████▏ | 160699/196351 [22:47:27<4:59:49,  1.98it/s]global step 160700, epoch: 1, batch: 160700, loss: 0.60794, acc: 0.85706, time: Sat Jan 22 09:22:05 2022
epoch 1 training:  82%|████████▏ | 160799/196351 [22:48:18<5:06:20,  1.93it/s]global step 160800, epoch: 1, batch: 160800, loss: 2.49951, acc: 0.85709, time: Sat Jan 22 09:22:56 2022
epoch 1 training:  82%|████████▏ | 160899/196351 [22:49:07<4:57:44,  1.98it/s]global step 160900, epoch: 1, batch: 160900, loss: 2.69715, acc: 0.85712, time: Sat Jan 22 09:23:46 2022
epoch 1 training:  82%|████████▏ | 160999/196351 [22:49:58<4:59:22,  1.97it/s]global step 161000, epoch: 1, batch: 161000, loss: 1.17144, acc: 0.85717, time: Sat Jan 22 09:24:37 2022
epoch 1 training:  82%|████████▏ | 161099/196351 [22:50:49<4:57:59,  1.97it/s]global step 161100, epoch: 1, batch: 161100, loss: 1.70056, acc: 0.85720, time: Sat Jan 22 09:25:28 2022
epoch 1 training:  82%|████████▏ | 161199/196351 [22:51:40<5:05:45,  1.92it/s]global step 161200, epoch: 1, batch: 161200, loss: 1.77141, acc: 0.85719, time: Sat Jan 22 09:26:19 2022
epoch 1 training:  82%|████████▏ | 161299/196351 [22:52:31<4:57:57,  1.96it/s]global step 161300, epoch: 1, batch: 161300, loss: 2.41503, acc: 0.85721, time: Sat Jan 22 09:27:10 2022
epoch 1 training:  82%|████████▏ | 161399/196351 [22:53:22<4:55:14,  1.97it/s]global step 161400, epoch: 1, batch: 161400, loss: 0.59792, acc: 0.85724, time: Sat Jan 22 09:28:01 2022
epoch 1 training:  82%|████████▏ | 161499/196351 [22:54:13<4:58:39,  1.94it/s]global step 161500, epoch: 1, batch: 161500, loss: 2.35554, acc: 0.85726, time: Sat Jan 22 09:28:52 2022
epoch 1 training:  82%|████████▏ | 161599/196351 [22:55:04<4:54:33,  1.97it/s]global step 161600, epoch: 1, batch: 161600, loss: 2.05075, acc: 0.85728, time: Sat Jan 22 09:29:43 2022
epoch 1 training:  82%|████████▏ | 161699/196351 [22:55:55<4:57:42,  1.94it/s]global step 161700, epoch: 1, batch: 161700, loss: 5.04714, acc: 0.85729, time: Sat Jan 22 09:30:34 2022
epoch 1 training:  82%|████████▏ | 161799/196351 [22:56:46<4:53:33,  1.96it/s]global step 161800, epoch: 1, batch: 161800, loss: 0.76966, acc: 0.85732, time: Sat Jan 22 09:31:25 2022
epoch 1 training:  82%|████████▏ | 161899/196351 [22:57:37<4:53:22,  1.96it/s]global step 161900, epoch: 1, batch: 161900, loss: 1.28933, acc: 0.85734, time: Sat Jan 22 09:32:16 2022
epoch 1 training:  83%|████████▎ | 161999/196351 [22:58:28<4:59:10,  1.91it/s]global step 162000, epoch: 1, batch: 162000, loss: 2.53294, acc: 0.85738, time: Sat Jan 22 09:33:07 2022
epoch 1 training:  83%|████████▎ | 162099/196351 [22:59:19<4:51:39,  1.96it/s]global step 162100, epoch: 1, batch: 162100, loss: 2.70711, acc: 0.85739, time: Sat Jan 22 09:33:58 2022
epoch 1 training:  83%|████████▎ | 162199/196351 [23:00:11<4:52:32,  1.95it/s]global step 162200, epoch: 1, batch: 162200, loss: 1.28855, acc: 0.85743, time: Sat Jan 22 09:34:49 2022
epoch 1 training:  83%|████████▎ | 162299/196351 [23:01:02<4:54:35,  1.93it/s]global step 162300, epoch: 1, batch: 162300, loss: 0.65592, acc: 0.85747, time: Sat Jan 22 09:35:41 2022
epoch 1 training:  83%|████████▎ | 162399/196351 [23:01:53<4:49:49,  1.95it/s]global step 162400, epoch: 1, batch: 162400, loss: 1.83399, acc: 0.85749, time: Sat Jan 22 09:36:32 2022
epoch 1 training:  83%|████████▎ | 162499/196351 [23:02:44<4:47:05,  1.97it/s]global step 162500, epoch: 1, batch: 162500, loss: 4.23688, acc: 0.85749, time: Sat Jan 22 09:37:23 2022
epoch 1 training:  83%|████████▎ | 162599/196351 [23:03:35<4:46:48,  1.96it/s]global step 162600, epoch: 1, batch: 162600, loss: 0.87677, acc: 0.85751, time: Sat Jan 22 09:38:14 2022
epoch 1 training:  83%|████████▎ | 162699/196351 [23:04:26<4:50:16,  1.93it/s]global step 162700, epoch: 1, batch: 162700, loss: 2.23019, acc: 0.85755, time: Sat Jan 22 09:39:05 2022
epoch 1 training:  83%|████████▎ | 162799/196351 [23:05:18<4:46:49,  1.95it/s]global step 162800, epoch: 1, batch: 162800, loss: 3.16592, acc: 0.85757, time: Sat Jan 22 09:39:56 2022
epoch 1 training:  83%|████████▎ | 162899/196351 [23:06:09<4:41:58,  1.98it/s]global step 162900, epoch: 1, batch: 162900, loss: 2.49723, acc: 0.85758, time: Sat Jan 22 09:40:48 2022
epoch 1 training:  83%|████████▎ | 162999/196351 [23:07:00<4:42:44,  1.97it/s]global step 163000, epoch: 1, batch: 163000, loss: 0.77799, acc: 0.85760, time: Sat Jan 22 09:41:39 2022
epoch 1 training:  83%|████████▎ | 163099/196351 [23:07:51<4:43:54,  1.95it/s]global step 163100, epoch: 1, batch: 163100, loss: 3.23945, acc: 0.85763, time: Sat Jan 22 09:42:30 2022
epoch 1 training:  83%|████████▎ | 163199/196351 [23:08:43<4:53:21,  1.88it/s]global step 163200, epoch: 1, batch: 163200, loss: 2.28084, acc: 0.85766, time: Sat Jan 22 09:43:22 2022
epoch 1 training:  83%|████████▎ | 163299/196351 [23:09:34<4:42:09,  1.95it/s]global step 163300, epoch: 1, batch: 163300, loss: 1.36442, acc: 0.85767, time: Sat Jan 22 09:44:13 2022
epoch 1 training:  83%|████████▎ | 163399/196351 [23:10:25<4:43:44,  1.94it/s]global step 163400, epoch: 1, batch: 163400, loss: 0.58088, acc: 0.85768, time: Sat Jan 22 09:45:03 2022
epoch 1 training:  83%|████████▎ | 163499/196351 [23:11:15<4:38:30,  1.97it/s]global step 163500, epoch: 1, batch: 163500, loss: 1.01929, acc: 0.85770, time: Sat Jan 22 09:45:54 2022
epoch 1 training:  83%|████████▎ | 163599/196351 [23:12:06<4:34:26,  1.99it/s]global step 163600, epoch: 1, batch: 163600, loss: 2.85442, acc: 0.85772, time: Sat Jan 22 09:46:45 2022
epoch 1 training:  83%|████████▎ | 163699/196351 [23:12:57<4:35:54,  1.97it/s]global step 163700, epoch: 1, batch: 163700, loss: 3.26579, acc: 0.85773, time: Sat Jan 22 09:47:36 2022
epoch 1 training:  83%|████████▎ | 163799/196351 [23:13:48<4:36:43,  1.96it/s]global step 163800, epoch: 1, batch: 163800, loss: 2.44715, acc: 0.85774, time: Sat Jan 22 09:48:26 2022
epoch 1 training:  83%|████████▎ | 163899/196351 [23:14:38<4:33:50,  1.98it/s]global step 163900, epoch: 1, batch: 163900, loss: 1.10010, acc: 0.85777, time: Sat Jan 22 09:49:17 2022
epoch 1 training:  84%|████████▎ | 163999/196351 [23:15:29<4:33:20,  1.97it/s]global step 164000, epoch: 1, batch: 164000, loss: 1.82263, acc: 0.85778, time: Sat Jan 22 09:50:08 2022
epoch 1 training:  84%|████████▎ | 164099/196351 [23:16:20<4:36:48,  1.94it/s]global step 164100, epoch: 1, batch: 164100, loss: 2.89740, acc: 0.85782, time: Sat Jan 22 09:50:59 2022
epoch 1 training:  84%|████████▎ | 164199/196351 [23:17:12<4:33:09,  1.96it/s]global step 164200, epoch: 1, batch: 164200, loss: 3.37226, acc: 0.85785, time: Sat Jan 22 09:51:51 2022
epoch 1 training:  84%|████████▎ | 164299/196351 [23:18:03<4:33:16,  1.95it/s]global step 164300, epoch: 1, batch: 164300, loss: 3.29973, acc: 0.85790, time: Sat Jan 22 09:52:42 2022
epoch 1 training:  84%|████████▎ | 164399/196351 [23:18:54<4:30:21,  1.97it/s]global step 164400, epoch: 1, batch: 164400, loss: 1.01294, acc: 0.85791, time: Sat Jan 22 09:53:33 2022
epoch 1 training:  84%|████████▍ | 164499/196351 [23:19:45<4:31:09,  1.96it/s]global step 164500, epoch: 1, batch: 164500, loss: 1.95066, acc: 0.85795, time: Sat Jan 22 09:54:24 2022
epoch 1 training:  84%|████████▍ | 164599/196351 [23:20:37<4:29:59,  1.96it/s]global step 164600, epoch: 1, batch: 164600, loss: 3.38107, acc: 0.85799, time: Sat Jan 22 09:55:15 2022
epoch 1 training:  84%|████████▍ | 164699/196351 [23:21:27<4:27:04,  1.98it/s]global step 164700, epoch: 1, batch: 164700, loss: 2.54493, acc: 0.85801, time: Sat Jan 22 09:56:06 2022
epoch 1 training:  84%|████████▍ | 164799/196351 [23:22:18<4:24:48,  1.99it/s]global step 164800, epoch: 1, batch: 164800, loss: 1.89842, acc: 0.85801, time: Sat Jan 22 09:56:56 2022
epoch 1 training:  84%|████████▍ | 164899/196351 [23:23:08<4:26:29,  1.97it/s]global step 164900, epoch: 1, batch: 164900, loss: 0.77568, acc: 0.85802, time: Sat Jan 22 09:57:47 2022
epoch 1 training:  84%|████████▍ | 164999/196351 [23:23:59<4:31:38,  1.92it/s]global step 165000, epoch: 1, batch: 165000, loss: 2.57338, acc: 0.85806, time: Sat Jan 22 09:58:38 2022
epoch 1 training:  84%|████████▍ | 165099/196351 [23:24:50<4:28:39,  1.94it/s]global step 165100, epoch: 1, batch: 165100, loss: 1.06756, acc: 0.85810, time: Sat Jan 22 09:59:29 2022
epoch 1 training:  84%|████████▍ | 165199/196351 [23:25:41<4:21:12,  1.99it/s]global step 165200, epoch: 1, batch: 165200, loss: 2.21725, acc: 0.85815, time: Sat Jan 22 10:00:20 2022
epoch 1 training:  84%|████████▍ | 165299/196351 [23:26:32<4:22:30,  1.97it/s]global step 165300, epoch: 1, batch: 165300, loss: 0.94512, acc: 0.85817, time: Sat Jan 22 10:01:10 2022
epoch 1 training:  84%|████████▍ | 165399/196351 [23:27:23<4:21:42,  1.97it/s]global step 165400, epoch: 1, batch: 165400, loss: 3.91926, acc: 0.85819, time: Sat Jan 22 10:02:01 2022
epoch 1 training:  84%|████████▍ | 165499/196351 [23:28:14<4:21:55,  1.96it/s]global step 165500, epoch: 1, batch: 165500, loss: 1.22449, acc: 0.85823, time: Sat Jan 22 10:02:52 2022
epoch 1 training:  84%|████████▍ | 165599/196351 [23:29:04<4:16:34,  2.00it/s]global step 165600, epoch: 1, batch: 165600, loss: 2.63578, acc: 0.85825, time: Sat Jan 22 10:03:43 2022
epoch 1 training:  84%|████████▍ | 165699/196351 [23:29:55<4:21:37,  1.95it/s]global step 165700, epoch: 1, batch: 165700, loss: 2.34279, acc: 0.85828, time: Sat Jan 22 10:04:34 2022
epoch 1 training:  84%|████████▍ | 165799/196351 [23:30:46<4:20:58,  1.95it/s]global step 165800, epoch: 1, batch: 165800, loss: 1.76164, acc: 0.85830, time: Sat Jan 22 10:05:25 2022
epoch 1 training:  84%|████████▍ | 165899/196351 [23:31:38<4:24:32,  1.92it/s]global step 165900, epoch: 1, batch: 165900, loss: 1.92534, acc: 0.85831, time: Sat Jan 22 10:06:16 2022
epoch 1 training:  85%|████████▍ | 165999/196351 [23:32:29<5:12:42,  1.62it/s]global step 166000, epoch: 1, batch: 166000, loss: 2.45557, acc: 0.85833, time: Sat Jan 22 10:07:08 2022
epoch 1 training:  85%|████████▍ | 166099/196351 [23:33:20<4:17:51,  1.96it/s]global step 166100, epoch: 1, batch: 166100, loss: 1.26214, acc: 0.85836, time: Sat Jan 22 10:07:59 2022
epoch 1 training:  85%|████████▍ | 166199/196351 [23:34:11<4:12:38,  1.99it/s]global step 166200, epoch: 1, batch: 166200, loss: 2.66842, acc: 0.85840, time: Sat Jan 22 10:08:50 2022
epoch 1 training:  85%|████████▍ | 166299/196351 [23:35:02<4:14:26,  1.97it/s]global step 166300, epoch: 1, batch: 166300, loss: 2.59584, acc: 0.85844, time: Sat Jan 22 10:09:41 2022
epoch 1 training:  85%|████████▍ | 166399/196351 [23:35:53<4:17:04,  1.94it/s]global step 166400, epoch: 1, batch: 166400, loss: 2.41639, acc: 0.85845, time: Sat Jan 22 10:10:32 2022
epoch 1 training:  85%|████████▍ | 166499/196351 [23:36:45<4:15:51,  1.94it/s]global step 166500, epoch: 1, batch: 166500, loss: 1.42607, acc: 0.85847, time: Sat Jan 22 10:11:23 2022
epoch 1 training:  85%|████████▍ | 166599/196351 [23:37:36<4:18:13,  1.92it/s]global step 166600, epoch: 1, batch: 166600, loss: 1.62814, acc: 0.85848, time: Sat Jan 22 10:12:15 2022
epoch 1 training:  85%|████████▍ | 166699/196351 [23:38:27<4:12:26,  1.96it/s]global step 166700, epoch: 1, batch: 166700, loss: 2.25908, acc: 0.85850, time: Sat Jan 22 10:13:06 2022
epoch 1 training:  85%|████████▍ | 166799/196351 [23:39:18<4:10:24,  1.97it/s]global step 166800, epoch: 1, batch: 166800, loss: 0.88697, acc: 0.85851, time: Sat Jan 22 10:13:57 2022
epoch 1 training:  85%|████████▌ | 166899/196351 [23:40:09<4:10:17,  1.96it/s]global step 166900, epoch: 1, batch: 166900, loss: 1.68743, acc: 0.85855, time: Sat Jan 22 10:14:48 2022
epoch 1 training:  85%|████████▌ | 166999/196351 [23:41:00<4:10:10,  1.96it/s]global step 167000, epoch: 1, batch: 167000, loss: 3.18555, acc: 0.85857, time: Sat Jan 22 10:15:39 2022
epoch 1 training:  85%|████████▌ | 167099/196351 [23:41:52<4:06:59,  1.97it/s]global step 167100, epoch: 1, batch: 167100, loss: 3.44657, acc: 0.85859, time: Sat Jan 22 10:16:31 2022
epoch 1 training:  85%|████████▌ | 167199/196351 [23:42:43<4:08:30,  1.96it/s]global step 167200, epoch: 1, batch: 167200, loss: 2.97022, acc: 0.85863, time: Sat Jan 22 10:17:22 2022
epoch 1 training:  85%|████████▌ | 167299/196351 [23:43:35<4:09:28,  1.94it/s]global step 167300, epoch: 1, batch: 167300, loss: 3.61264, acc: 0.85867, time: Sat Jan 22 10:18:13 2022
epoch 1 training:  85%|████████▌ | 167399/196351 [23:44:26<4:06:56,  1.95it/s]global step 167400, epoch: 1, batch: 167400, loss: 1.88759, acc: 0.85866, time: Sat Jan 22 10:19:04 2022
epoch 1 training:  85%|████████▌ | 167499/196351 [23:45:16<4:03:48,  1.97it/s]global step 167500, epoch: 1, batch: 167500, loss: 0.78015, acc: 0.85869, time: Sat Jan 22 10:19:55 2022
epoch 1 training:  85%|████████▌ | 167599/196351 [23:46:07<4:02:57,  1.97it/s]global step 167600, epoch: 1, batch: 167600, loss: 3.86495, acc: 0.85871, time: Sat Jan 22 10:20:46 2022
epoch 1 training:  85%|████████▌ | 167699/196351 [23:46:58<4:02:19,  1.97it/s]global step 167700, epoch: 1, batch: 167700, loss: 0.54834, acc: 0.85874, time: Sat Jan 22 10:21:37 2022
epoch 1 training:  85%|████████▌ | 167799/196351 [23:47:49<4:00:58,  1.97it/s]global step 167800, epoch: 1, batch: 167800, loss: 1.01313, acc: 0.85878, time: Sat Jan 22 10:22:28 2022
epoch 1 training:  86%|████████▌ | 167899/196351 [23:48:40<4:00:38,  1.97it/s]global step 167900, epoch: 1, batch: 167900, loss: 1.82674, acc: 0.85881, time: Sat Jan 22 10:23:19 2022
epoch 1 training:  86%|████████▌ | 167999/196351 [23:49:31<3:58:44,  1.98it/s]global step 168000, epoch: 1, batch: 168000, loss: 4.78423, acc: 0.85883, time: Sat Jan 22 10:24:10 2022
epoch 1 training:  86%|████████▌ | 168099/196351 [23:50:23<4:02:06,  1.94it/s]global step 168100, epoch: 1, batch: 168100, loss: 0.83670, acc: 0.85886, time: Sat Jan 22 10:25:01 2022
epoch 1 training:  86%|████████▌ | 168199/196351 [23:51:14<3:57:13,  1.98it/s]global step 168200, epoch: 1, batch: 168200, loss: 2.14562, acc: 0.85889, time: Sat Jan 22 10:25:53 2022
epoch 1 training:  86%|████████▌ | 168299/196351 [23:52:05<3:57:37,  1.97it/s]global step 168300, epoch: 1, batch: 168300, loss: 2.60887, acc: 0.85890, time: Sat Jan 22 10:26:44 2022
epoch 1 training:  86%|████████▌ | 168399/196351 [23:52:57<4:02:10,  1.92it/s]global step 168400, epoch: 1, batch: 168400, loss: 2.52449, acc: 0.85893, time: Sat Jan 22 10:27:35 2022
epoch 1 training:  86%|████████▌ | 168499/196351 [23:53:48<3:53:50,  1.99it/s]global step 168500, epoch: 1, batch: 168500, loss: 2.99570, acc: 0.85893, time: Sat Jan 22 10:28:26 2022
epoch 1 training:  86%|████████▌ | 168599/196351 [23:54:38<3:53:37,  1.98it/s]global step 168600, epoch: 1, batch: 168600, loss: 1.25387, acc: 0.85896, time: Sat Jan 22 10:29:17 2022
epoch 1 training:  86%|████████▌ | 168699/196351 [23:55:29<3:56:44,  1.95it/s]global step 168700, epoch: 1, batch: 168700, loss: 3.52276, acc: 0.85899, time: Sat Jan 22 10:30:08 2022
epoch 1 training:  86%|████████▌ | 168799/196351 [23:56:20<3:58:36,  1.92it/s]global step 168800, epoch: 1, batch: 168800, loss: 2.04869, acc: 0.85902, time: Sat Jan 22 10:30:59 2022
epoch 1 training:  86%|████████▌ | 168899/196351 [23:57:11<3:55:26,  1.94it/s]global step 168900, epoch: 1, batch: 168900, loss: 0.56049, acc: 0.85905, time: Sat Jan 22 10:31:50 2022
epoch 1 training:  86%|████████▌ | 168999/196351 [23:58:02<3:51:50,  1.97it/s]global step 169000, epoch: 1, batch: 169000, loss: 1.72018, acc: 0.85906, time: Sat Jan 22 10:32:41 2022
epoch 1 training:  86%|████████▌ | 169099/196351 [23:58:53<3:50:54,  1.97it/s]global step 169100, epoch: 1, batch: 169100, loss: 0.55846, acc: 0.85910, time: Sat Jan 22 10:33:32 2022
epoch 1 training:  86%|████████▌ | 169199/196351 [23:59:44<3:53:00,  1.94it/s]global step 169200, epoch: 1, batch: 169200, loss: 2.12836, acc: 0.85911, time: Sat Jan 22 10:34:23 2022
epoch 1 training:  86%|████████▌ | 169299/196351 [24:00:35<3:51:36,  1.95it/s]global step 169300, epoch: 1, batch: 169300, loss: 4.37611, acc: 0.85913, time: Sat Jan 22 10:35:14 2022
epoch 1 training:  86%|████████▋ | 169399/196351 [24:01:26<3:49:40,  1.96it/s]global step 169400, epoch: 1, batch: 169400, loss: 1.90740, acc: 0.85915, time: Sat Jan 22 10:36:05 2022
epoch 1 training:  86%|████████▋ | 169499/196351 [24:02:17<3:47:27,  1.97it/s]global step 169500, epoch: 1, batch: 169500, loss: 1.86024, acc: 0.85918, time: Sat Jan 22 10:36:55 2022
epoch 1 training:  86%|████████▋ | 169599/196351 [24:03:08<3:46:15,  1.97it/s]global step 169600, epoch: 1, batch: 169600, loss: 1.86130, acc: 0.85921, time: Sat Jan 22 10:37:46 2022
epoch 1 training:  86%|████████▋ | 169699/196351 [24:03:58<3:48:03,  1.95it/s]global step 169700, epoch: 1, batch: 169700, loss: 0.24056, acc: 0.85923, time: Sat Jan 22 10:38:37 2022
epoch 1 training:  86%|████████▋ | 169799/196351 [24:04:50<3:46:53,  1.95it/s]global step 169800, epoch: 1, batch: 169800, loss: 2.81869, acc: 0.85924, time: Sat Jan 22 10:39:28 2022
epoch 1 training:  87%|████████▋ | 169899/196351 [24:05:41<3:40:34,  2.00it/s]global step 169900, epoch: 1, batch: 169900, loss: 2.23041, acc: 0.85926, time: Sat Jan 22 10:40:20 2022
epoch 1 training:  87%|████████▋ | 169999/196351 [24:06:32<3:47:12,  1.93it/s]global step 170000, epoch: 1, batch: 170000, loss: 2.55335, acc: 0.85928, time: Sat Jan 22 10:41:11 2022
epoch 1 training:  87%|████████▋ | 170099/196351 [24:07:23<3:37:38,  2.01it/s]global step 170100, epoch: 1, batch: 170100, loss: 0.75956, acc: 0.85929, time: Sat Jan 22 10:42:02 2022
epoch 1 training:  87%|████████▋ | 170199/196351 [24:08:14<3:40:59,  1.97it/s]global step 170200, epoch: 1, batch: 170200, loss: 1.98471, acc: 0.85933, time: Sat Jan 22 10:42:52 2022
epoch 1 training:  87%|████████▋ | 170299/196351 [24:09:04<3:41:07,  1.96it/s]global step 170300, epoch: 1, batch: 170300, loss: 1.20805, acc: 0.85937, time: Sat Jan 22 10:43:43 2022
epoch 1 training:  87%|████████▋ | 170399/196351 [24:09:55<3:40:40,  1.96it/s]global step 170400, epoch: 1, batch: 170400, loss: 0.94806, acc: 0.85940, time: Sat Jan 22 10:44:34 2022
epoch 1 training:  87%|████████▋ | 170499/196351 [24:10:46<3:44:41,  1.92it/s]global step 170500, epoch: 1, batch: 170500, loss: 0.84889, acc: 0.85942, time: Sat Jan 22 10:45:25 2022
epoch 1 training:  87%|████████▋ | 170599/196351 [24:11:37<3:37:16,  1.98it/s]global step 170600, epoch: 1, batch: 170600, loss: 1.58814, acc: 0.85943, time: Sat Jan 22 10:46:16 2022
epoch 1 training:  87%|████████▋ | 170699/196351 [24:12:28<3:38:29,  1.96it/s]global step 170700, epoch: 1, batch: 170700, loss: 1.81865, acc: 0.85945, time: Sat Jan 22 10:47:07 2022
epoch 1 training:  87%|████████▋ | 170799/196351 [24:13:19<3:36:35,  1.97it/s]global step 170800, epoch: 1, batch: 170800, loss: 1.90534, acc: 0.85946, time: Sat Jan 22 10:47:58 2022
epoch 1 training:  87%|████████▋ | 170899/196351 [24:14:10<3:36:19,  1.96it/s]global step 170900, epoch: 1, batch: 170900, loss: 2.43301, acc: 0.85947, time: Sat Jan 22 10:48:49 2022
epoch 1 training:  87%|████████▋ | 170999/196351 [24:15:02<3:34:59,  1.97it/s]global step 171000, epoch: 1, batch: 171000, loss: 1.46136, acc: 0.85951, time: Sat Jan 22 10:49:40 2022
epoch 1 training:  87%|████████▋ | 171099/196351 [24:15:53<3:32:35,  1.98it/s]global step 171100, epoch: 1, batch: 171100, loss: 2.42190, acc: 0.85953, time: Sat Jan 22 10:50:31 2022
epoch 1 training:  87%|████████▋ | 171199/196351 [24:16:44<3:31:22,  1.98it/s]global step 171200, epoch: 1, batch: 171200, loss: 0.35583, acc: 0.85956, time: Sat Jan 22 10:51:22 2022
epoch 1 training:  87%|████████▋ | 171299/196351 [24:17:34<3:30:19,  1.99it/s]global step 171300, epoch: 1, batch: 171300, loss: 4.68350, acc: 0.85957, time: Sat Jan 22 10:52:13 2022
epoch 1 training:  87%|████████▋ | 171399/196351 [24:18:25<3:31:52,  1.96it/s]global step 171400, epoch: 1, batch: 171400, loss: 0.32118, acc: 0.85962, time: Sat Jan 22 10:53:04 2022
epoch 1 training:  87%|████████▋ | 171499/196351 [24:19:16<3:28:06,  1.99it/s]global step 171500, epoch: 1, batch: 171500, loss: 1.42160, acc: 0.85965, time: Sat Jan 22 10:53:55 2022
epoch 1 training:  87%|████████▋ | 171599/196351 [24:20:07<3:30:18,  1.96it/s]global step 171600, epoch: 1, batch: 171600, loss: 1.92343, acc: 0.85968, time: Sat Jan 22 10:54:46 2022
epoch 1 training:  87%|████████▋ | 171699/196351 [24:20:57<3:26:49,  1.99it/s]global step 171700, epoch: 1, batch: 171700, loss: 0.32867, acc: 0.85971, time: Sat Jan 22 10:55:36 2022
epoch 1 training:  87%|████████▋ | 171799/196351 [24:21:48<3:26:17,  1.98it/s]global step 171800, epoch: 1, batch: 171800, loss: 1.46297, acc: 0.85974, time: Sat Jan 22 10:56:27 2022
epoch 1 training:  88%|████████▊ | 171899/196351 [24:22:38<3:24:51,  1.99it/s]global step 171900, epoch: 1, batch: 171900, loss: 4.67949, acc: 0.85978, time: Sat Jan 22 10:57:17 2022
epoch 1 training:  88%|████████▊ | 171999/196351 [24:23:29<3:26:47,  1.96it/s]global step 172000, epoch: 1, batch: 172000, loss: 1.48407, acc: 0.85979, time: Sat Jan 22 10:58:08 2022
epoch 1 training:  88%|████████▊ | 172099/196351 [24:24:20<3:24:41,  1.97it/s]global step 172100, epoch: 1, batch: 172100, loss: 0.90165, acc: 0.85980, time: Sat Jan 22 10:58:59 2022
epoch 1 training:  88%|████████▊ | 172199/196351 [24:25:11<3:21:36,  2.00it/s]global step 172200, epoch: 1, batch: 172200, loss: 1.56057, acc: 0.85983, time: Sat Jan 22 10:59:49 2022
epoch 1 training:  88%|████████▊ | 172299/196351 [24:26:01<3:20:13,  2.00it/s]global step 172300, epoch: 1, batch: 172300, loss: 1.64165, acc: 0.85984, time: Sat Jan 22 11:00:40 2022
epoch 1 training:  88%|████████▊ | 172399/196351 [24:26:52<3:21:10,  1.98it/s]global step 172400, epoch: 1, batch: 172400, loss: 1.80853, acc: 0.85987, time: Sat Jan 22 11:01:31 2022
epoch 1 training:  88%|████████▊ | 172499/196351 [24:27:43<3:22:46,  1.96it/s]global step 172500, epoch: 1, batch: 172500, loss: 0.76922, acc: 0.85989, time: Sat Jan 22 11:02:22 2022
epoch 1 training:  88%|████████▊ | 172599/196351 [24:28:34<3:22:43,  1.95it/s]global step 172600, epoch: 1, batch: 172600, loss: 1.86189, acc: 0.85993, time: Sat Jan 22 11:03:13 2022
epoch 1 training:  88%|████████▊ | 172699/196351 [24:29:25<3:18:47,  1.98it/s]global step 172700, epoch: 1, batch: 172700, loss: 4.99446, acc: 0.85994, time: Sat Jan 22 11:04:03 2022
epoch 1 training:  88%|████████▊ | 172799/196351 [24:30:15<3:18:08,  1.98it/s]global step 172800, epoch: 1, batch: 172800, loss: 2.29100, acc: 0.85996, time: Sat Jan 22 11:04:54 2022
epoch 1 training:  88%|████████▊ | 172899/196351 [24:31:06<3:19:15,  1.96it/s]global step 172900, epoch: 1, batch: 172900, loss: 2.73485, acc: 0.85999, time: Sat Jan 22 11:05:45 2022
epoch 1 training:  88%|████████▊ | 172999/196351 [24:31:57<3:17:53,  1.97it/s]global step 173000, epoch: 1, batch: 173000, loss: 1.34287, acc: 0.86001, time: Sat Jan 22 11:06:36 2022
epoch 1 training:  88%|████████▊ | 173099/196351 [24:32:48<3:16:38,  1.97it/s]global step 173100, epoch: 1, batch: 173100, loss: 4.38485, acc: 0.86001, time: Sat Jan 22 11:07:26 2022
epoch 1 training:  88%|████████▊ | 173199/196351 [24:33:39<3:14:42,  1.98it/s]global step 173200, epoch: 1, batch: 173200, loss: 1.96001, acc: 0.86004, time: Sat Jan 22 11:08:17 2022
epoch 1 training:  88%|████████▊ | 173299/196351 [24:34:29<3:17:07,  1.95it/s]global step 173300, epoch: 1, batch: 173300, loss: 1.13448, acc: 0.86005, time: Sat Jan 22 11:09:08 2022
epoch 1 training:  88%|████████▊ | 173399/196351 [24:35:20<3:14:03,  1.97it/s]global step 173400, epoch: 1, batch: 173400, loss: 0.68955, acc: 0.86007, time: Sat Jan 22 11:09:59 2022
epoch 1 training:  88%|████████▊ | 173499/196351 [24:36:11<3:13:56,  1.96it/s]global step 173500, epoch: 1, batch: 173500, loss: 2.73723, acc: 0.86011, time: Sat Jan 22 11:10:50 2022
epoch 1 training:  88%|████████▊ | 173599/196351 [24:37:02<3:11:53,  1.98it/s]global step 173600, epoch: 1, batch: 173600, loss: 4.88143, acc: 0.86014, time: Sat Jan 22 11:11:41 2022
epoch 1 training:  88%|████████▊ | 173699/196351 [24:37:53<3:10:55,  1.98it/s]global step 173700, epoch: 1, batch: 173700, loss: 2.34739, acc: 0.86018, time: Sat Jan 22 11:12:32 2022
epoch 1 training:  89%|████████▊ | 173799/196351 [24:38:44<3:10:30,  1.97it/s]global step 173800, epoch: 1, batch: 173800, loss: 4.92808, acc: 0.86019, time: Sat Jan 22 11:13:23 2022
epoch 1 training:  89%|████████▊ | 173899/196351 [24:39:35<3:10:25,  1.97it/s]global step 173900, epoch: 1, batch: 173900, loss: 3.25844, acc: 0.86021, time: Sat Jan 22 11:14:14 2022
epoch 1 training:  89%|████████▊ | 173999/196351 [24:40:26<3:08:26,  1.98it/s]global step 174000, epoch: 1, batch: 174000, loss: 0.45541, acc: 0.86022, time: Sat Jan 22 11:15:05 2022
epoch 1 training:  89%|████████▊ | 174099/196351 [24:41:17<3:06:54,  1.98it/s]global step 174100, epoch: 1, batch: 174100, loss: 1.13989, acc: 0.86024, time: Sat Jan 22 11:15:55 2022
epoch 1 training:  89%|████████▊ | 174199/196351 [24:42:07<3:07:56,  1.96it/s]global step 174200, epoch: 1, batch: 174200, loss: 0.89903, acc: 0.86026, time: Sat Jan 22 11:16:46 2022
epoch 1 training:  89%|████████▉ | 174299/196351 [24:42:58<3:05:40,  1.98it/s]global step 174300, epoch: 1, batch: 174300, loss: 1.32478, acc: 0.86030, time: Sat Jan 22 11:17:37 2022
epoch 1 training:  89%|████████▉ | 174399/196351 [24:43:49<3:06:17,  1.96it/s]global step 174400, epoch: 1, batch: 174400, loss: 0.74829, acc: 0.86033, time: Sat Jan 22 11:18:27 2022
epoch 1 training:  89%|████████▉ | 174499/196351 [24:44:39<3:05:23,  1.96it/s]global step 174500, epoch: 1, batch: 174500, loss: 2.41415, acc: 0.86035, time: Sat Jan 22 11:19:18 2022
epoch 1 training:  89%|████████▉ | 174599/196351 [24:45:30<3:05:27,  1.95it/s]global step 174600, epoch: 1, batch: 174600, loss: 0.50806, acc: 0.86037, time: Sat Jan 22 11:20:09 2022
epoch 1 training:  89%|████████▉ | 174699/196351 [24:46:21<3:03:49,  1.96it/s]global step 174700, epoch: 1, batch: 174700, loss: 0.68688, acc: 0.86039, time: Sat Jan 22 11:21:00 2022
epoch 1 training:  89%|████████▉ | 174799/196351 [24:47:13<3:07:13,  1.92it/s]global step 174800, epoch: 1, batch: 174800, loss: 5.50335, acc: 0.86042, time: Sat Jan 22 11:21:52 2022
epoch 1 training:  89%|████████▉ | 174899/196351 [24:48:04<3:04:16,  1.94it/s]global step 174900, epoch: 1, batch: 174900, loss: 1.51023, acc: 0.86044, time: Sat Jan 22 11:22:43 2022
epoch 1 training:  89%|████████▉ | 174999/196351 [24:48:55<3:04:08,  1.93it/s]global step 175000, epoch: 1, batch: 175000, loss: 2.95252, acc: 0.86048, time: Sat Jan 22 11:23:34 2022
epoch 1 training:  89%|████████▉ | 175099/196351 [24:49:47<3:02:36,  1.94it/s]global step 175100, epoch: 1, batch: 175100, loss: 2.01146, acc: 0.86051, time: Sat Jan 22 11:24:25 2022
epoch 1 training:  89%|████████▉ | 175199/196351 [24:50:38<2:59:56,  1.96it/s]global step 175200, epoch: 1, batch: 175200, loss: 1.90544, acc: 0.86053, time: Sat Jan 22 11:25:16 2022
epoch 1 training:  89%|████████▉ | 175299/196351 [24:51:28<2:58:23,  1.97it/s]global step 175300, epoch: 1, batch: 175300, loss: 0.95116, acc: 0.86056, time: Sat Jan 22 11:26:07 2022
epoch 1 training:  89%|████████▉ | 175399/196351 [24:52:19<2:59:59,  1.94it/s]global step 175400, epoch: 1, batch: 175400, loss: 0.39261, acc: 0.86056, time: Sat Jan 22 11:26:58 2022
epoch 1 training:  89%|████████▉ | 175499/196351 [24:53:10<2:55:57,  1.98it/s]global step 175500, epoch: 1, batch: 175500, loss: 0.75881, acc: 0.86060, time: Sat Jan 22 11:27:49 2022
epoch 1 training:  89%|████████▉ | 175599/196351 [24:54:01<2:52:33,  2.00it/s]global step 175600, epoch: 1, batch: 175600, loss: 3.21189, acc: 0.86062, time: Sat Jan 22 11:28:39 2022
epoch 1 training:  89%|████████▉ | 175699/196351 [24:54:51<2:53:44,  1.98it/s]global step 175700, epoch: 1, batch: 175700, loss: 0.95414, acc: 0.86065, time: Sat Jan 22 11:29:30 2022
epoch 1 training:  90%|████████▉ | 175799/196351 [24:55:43<2:53:36,  1.97it/s]global step 175800, epoch: 1, batch: 175800, loss: 1.50570, acc: 0.86066, time: Sat Jan 22 11:30:21 2022
epoch 1 training:  90%|████████▉ | 175899/196351 [24:56:34<2:51:53,  1.98it/s]global step 175900, epoch: 1, batch: 175900, loss: 0.98528, acc: 0.86069, time: Sat Jan 22 11:31:12 2022
epoch 1 training:  90%|████████▉ | 175999/196351 [24:57:24<2:51:07,  1.98it/s]global step 176000, epoch: 1, batch: 176000, loss: 2.31214, acc: 0.86072, time: Sat Jan 22 11:32:03 2022
epoch 1 training:  90%|████████▉ | 176099/196351 [24:58:15<2:49:36,  1.99it/s]global step 176100, epoch: 1, batch: 176100, loss: 0.97271, acc: 0.86073, time: Sat Jan 22 11:32:54 2022
epoch 1 training:  90%|████████▉ | 176199/196351 [24:59:06<2:51:43,  1.96it/s]global step 176200, epoch: 1, batch: 176200, loss: 3.53647, acc: 0.86075, time: Sat Jan 22 11:33:44 2022
epoch 1 training:  90%|████████▉ | 176299/196351 [24:59:56<2:48:01,  1.99it/s]global step 176300, epoch: 1, batch: 176300, loss: 2.16981, acc: 0.86079, time: Sat Jan 22 11:34:35 2022
epoch 1 training:  90%|████████▉ | 176399/196351 [25:00:47<2:46:36,  2.00it/s]global step 176400, epoch: 1, batch: 176400, loss: 2.09763, acc: 0.86079, time: Sat Jan 22 11:35:25 2022
epoch 1 training:  90%|████████▉ | 176499/196351 [25:01:38<2:47:52,  1.97it/s]global step 176500, epoch: 1, batch: 176500, loss: 0.78202, acc: 0.86082, time: Sat Jan 22 11:36:16 2022
epoch 1 training:  90%|████████▉ | 176599/196351 [25:02:29<2:48:02,  1.96it/s]global step 176600, epoch: 1, batch: 176600, loss: 3.73027, acc: 0.86085, time: Sat Jan 22 11:37:08 2022
epoch 1 training:  90%|████████▉ | 176699/196351 [25:03:20<2:47:17,  1.96it/s]global step 176700, epoch: 1, batch: 176700, loss: 3.42311, acc: 0.86086, time: Sat Jan 22 11:37:58 2022
epoch 1 training:  90%|█████████ | 176799/196351 [25:04:11<2:50:26,  1.91it/s]global step 176800, epoch: 1, batch: 176800, loss: 2.36404, acc: 0.86087, time: Sat Jan 22 11:38:49 2022
epoch 1 training:  90%|█████████ | 176899/196351 [25:05:02<2:45:13,  1.96it/s]global step 176900, epoch: 1, batch: 176900, loss: 3.09058, acc: 0.86090, time: Sat Jan 22 11:39:41 2022
epoch 1 training:  90%|█████████ | 176999/196351 [25:05:53<2:45:59,  1.94it/s]global step 177000, epoch: 1, batch: 177000, loss: 0.62194, acc: 0.86091, time: Sat Jan 22 11:40:32 2022
epoch 1 training:  90%|█████████ | 177099/196351 [25:06:44<2:42:37,  1.97it/s]global step 177100, epoch: 1, batch: 177100, loss: 0.65452, acc: 0.86093, time: Sat Jan 22 11:41:23 2022
epoch 1 training:  90%|█████████ | 177199/196351 [25:07:35<2:42:09,  1.97it/s]global step 177200, epoch: 1, batch: 177200, loss: 2.34941, acc: 0.86096, time: Sat Jan 22 11:42:14 2022
epoch 1 training:  90%|█████████ | 177299/196351 [25:08:26<2:39:24,  1.99it/s]global step 177300, epoch: 1, batch: 177300, loss: 3.94106, acc: 0.86096, time: Sat Jan 22 11:43:05 2022
epoch 1 training:  90%|█████████ | 177399/196351 [25:09:16<2:38:18,  2.00it/s]global step 177400, epoch: 1, batch: 177400, loss: 3.23144, acc: 0.86098, time: Sat Jan 22 11:43:55 2022
epoch 1 training:  90%|█████████ | 177499/196351 [25:10:07<2:39:13,  1.97it/s]global step 177500, epoch: 1, batch: 177500, loss: 3.26251, acc: 0.86097, time: Sat Jan 22 11:44:46 2022
epoch 1 training:  90%|█████████ | 177599/196351 [25:10:59<2:42:00,  1.93it/s]global step 177600, epoch: 1, batch: 177600, loss: 1.39922, acc: 0.86099, time: Sat Jan 22 11:45:37 2022
epoch 1 training:  91%|█████████ | 177699/196351 [25:11:49<2:37:14,  1.98it/s]global step 177700, epoch: 1, batch: 177700, loss: 1.93481, acc: 0.86102, time: Sat Jan 22 11:46:28 2022
epoch 1 training:  91%|█████████ | 177799/196351 [25:12:41<2:38:22,  1.95it/s]global step 177800, epoch: 1, batch: 177800, loss: 3.28627, acc: 0.86105, time: Sat Jan 22 11:47:19 2022
epoch 1 training:  91%|█████████ | 177899/196351 [25:13:31<2:35:27,  1.98it/s]global step 177900, epoch: 1, batch: 177900, loss: 1.73585, acc: 0.86109, time: Sat Jan 22 11:48:10 2022
epoch 1 training:  91%|█████████ | 177999/196351 [25:14:22<2:33:24,  1.99it/s]global step 178000, epoch: 1, batch: 178000, loss: 2.93367, acc: 0.86110, time: Sat Jan 22 11:49:01 2022
epoch 1 training:  91%|█████████ | 178099/196351 [25:15:13<2:32:57,  1.99it/s]global step 178100, epoch: 1, batch: 178100, loss: 3.57717, acc: 0.86114, time: Sat Jan 22 11:49:52 2022
epoch 1 training:  91%|█████████ | 178199/196351 [25:16:03<2:31:33,  2.00it/s]global step 178200, epoch: 1, batch: 178200, loss: 2.77007, acc: 0.86118, time: Sat Jan 22 11:50:42 2022
epoch 1 training:  91%|█████████ | 178299/196351 [25:16:54<2:32:08,  1.98it/s]global step 178300, epoch: 1, batch: 178300, loss: 1.22381, acc: 0.86119, time: Sat Jan 22 11:51:33 2022
epoch 1 training:  91%|█████████ | 178399/196351 [25:17:45<2:32:11,  1.97it/s]global step 178400, epoch: 1, batch: 178400, loss: 3.44606, acc: 0.86122, time: Sat Jan 22 11:52:23 2022
epoch 1 training:  91%|█████████ | 178499/196351 [25:18:35<2:29:07,  2.00it/s]global step 178500, epoch: 1, batch: 178500, loss: 0.95210, acc: 0.86125, time: Sat Jan 22 11:53:14 2022
epoch 1 training:  91%|█████████ | 178599/196351 [25:19:26<2:29:26,  1.98it/s]global step 178600, epoch: 1, batch: 178600, loss: 2.30804, acc: 0.86127, time: Sat Jan 22 11:54:05 2022
epoch 1 training:  91%|█████████ | 178699/196351 [25:20:17<2:31:54,  1.94it/s]global step 178700, epoch: 1, batch: 178700, loss: 2.01024, acc: 0.86131, time: Sat Jan 22 11:54:56 2022
epoch 1 training:  91%|█████████ | 178799/196351 [25:21:08<2:30:20,  1.95it/s]global step 178800, epoch: 1, batch: 178800, loss: 2.42316, acc: 0.86132, time: Sat Jan 22 11:55:47 2022
epoch 1 training:  91%|█████████ | 178899/196351 [25:21:59<2:26:32,  1.98it/s]global step 178900, epoch: 1, batch: 178900, loss: 1.63947, acc: 0.86136, time: Sat Jan 22 11:56:38 2022
epoch 1 training:  91%|█████████ | 178999/196351 [25:22:50<2:26:41,  1.97it/s]global step 179000, epoch: 1, batch: 179000, loss: 0.92398, acc: 0.86136, time: Sat Jan 22 11:57:29 2022
epoch 1 training:  91%|█████████ | 179099/196351 [25:23:41<2:27:23,  1.95it/s]global step 179100, epoch: 1, batch: 179100, loss: 2.93069, acc: 0.86139, time: Sat Jan 22 11:58:19 2022
epoch 1 training:  91%|█████████▏| 179199/196351 [25:24:32<2:24:36,  1.98it/s]global step 179200, epoch: 1, batch: 179200, loss: 1.64612, acc: 0.86142, time: Sat Jan 22 11:59:11 2022
epoch 1 training:  91%|█████████▏| 179299/196351 [25:25:22<2:26:04,  1.95it/s]global step 179300, epoch: 1, batch: 179300, loss: 3.58062, acc: 0.86142, time: Sat Jan 22 12:00:01 2022
epoch 1 training:  91%|█████████▏| 179399/196351 [25:26:13<2:22:12,  1.99it/s]global step 179400, epoch: 1, batch: 179400, loss: 0.58205, acc: 0.86142, time: Sat Jan 22 12:00:52 2022
epoch 1 training:  91%|█████████▏| 179499/196351 [25:27:04<2:19:38,  2.01it/s]global step 179500, epoch: 1, batch: 179500, loss: 3.79479, acc: 0.86144, time: Sat Jan 22 12:01:43 2022
epoch 1 training:  91%|█████████▏| 179599/196351 [25:27:55<2:21:33,  1.97it/s]global step 179600, epoch: 1, batch: 179600, loss: 3.58243, acc: 0.86147, time: Sat Jan 22 12:02:33 2022
epoch 1 training:  92%|█████████▏| 179699/196351 [25:28:46<2:22:12,  1.95it/s]global step 179700, epoch: 1, batch: 179700, loss: 0.57314, acc: 0.86148, time: Sat Jan 22 12:03:25 2022
epoch 1 training:  92%|█████████▏| 179799/196351 [25:29:36<2:19:12,  1.98it/s]global step 179800, epoch: 1, batch: 179800, loss: 3.69271, acc: 0.86150, time: Sat Jan 22 12:04:15 2022
epoch 1 training:  92%|█████████▏| 179899/196351 [25:30:27<2:18:39,  1.98it/s]global step 179900, epoch: 1, batch: 179900, loss: 3.09677, acc: 0.86150, time: Sat Jan 22 12:05:06 2022
epoch 1 training:  92%|█████████▏| 179999/196351 [25:31:17<2:18:49,  1.96it/s]global step 180000, epoch: 1, batch: 180000, loss: 0.85315, acc: 0.86152, time: Sat Jan 22 12:05:56 2022
epoch 1 training:  92%|█████████▏| 180099/196351 [25:32:08<2:18:29,  1.96it/s]global step 180100, epoch: 1, batch: 180100, loss: 1.05354, acc: 0.86154, time: Sat Jan 22 12:06:47 2022
epoch 1 training:  92%|█████████▏| 180199/196351 [25:32:58<2:15:28,  1.99it/s]global step 180200, epoch: 1, batch: 180200, loss: 1.84458, acc: 0.86156, time: Sat Jan 22 12:07:37 2022
epoch 1 training:  92%|█████████▏| 180299/196351 [25:33:49<2:15:45,  1.97it/s]global step 180300, epoch: 1, batch: 180300, loss: 0.78964, acc: 0.86158, time: Sat Jan 22 12:08:28 2022
epoch 1 training:  92%|█████████▏| 180399/196351 [25:34:40<2:14:34,  1.98it/s]global step 180400, epoch: 1, batch: 180400, loss: 1.71111, acc: 0.86160, time: Sat Jan 22 12:09:18 2022
epoch 1 training:  92%|█████████▏| 180499/196351 [25:35:30<2:16:16,  1.94it/s]global step 180500, epoch: 1, batch: 180500, loss: 2.02260, acc: 0.86165, time: Sat Jan 22 12:10:09 2022
epoch 1 training:  92%|█████████▏| 180599/196351 [25:36:21<2:12:13,  1.99it/s]global step 180600, epoch: 1, batch: 180600, loss: 0.72075, acc: 0.86169, time: Sat Jan 22 12:11:00 2022
epoch 1 training:  92%|█████████▏| 180699/196351 [25:37:12<2:14:09,  1.94it/s]global step 180700, epoch: 1, batch: 180700, loss: 0.83506, acc: 0.86171, time: Sat Jan 22 12:11:51 2022
epoch 1 training:  92%|█████████▏| 180799/196351 [25:38:03<2:12:37,  1.95it/s]global step 180800, epoch: 1, batch: 180800, loss: 2.71643, acc: 0.86174, time: Sat Jan 22 12:12:42 2022
epoch 1 training:  92%|█████████▏| 180899/196351 [25:38:54<2:12:38,  1.94it/s]global step 180900, epoch: 1, batch: 180900, loss: 3.77340, acc: 0.86175, time: Sat Jan 22 12:13:33 2022
epoch 1 training:  92%|█████████▏| 180999/196351 [25:39:45<2:08:29,  1.99it/s]global step 181000, epoch: 1, batch: 181000, loss: 1.63795, acc: 0.86178, time: Sat Jan 22 12:14:24 2022
epoch 1 training:  92%|█████████▏| 181099/196351 [25:40:36<2:10:20,  1.95it/s]global step 181100, epoch: 1, batch: 181100, loss: 0.86963, acc: 0.86181, time: Sat Jan 22 12:15:15 2022
epoch 1 training:  92%|█████████▏| 181199/196351 [25:41:27<2:08:02,  1.97it/s]global step 181200, epoch: 1, batch: 181200, loss: 1.76583, acc: 0.86183, time: Sat Jan 22 12:16:06 2022
epoch 1 training:  92%|█████████▏| 181299/196351 [25:42:18<2:14:08,  1.87it/s]global step 181300, epoch: 1, batch: 181300, loss: 3.24328, acc: 0.86187, time: Sat Jan 22 12:16:57 2022
epoch 1 training:  92%|█████████▏| 181399/196351 [25:43:10<2:06:52,  1.96it/s]global step 181400, epoch: 1, batch: 181400, loss: 1.08600, acc: 0.86189, time: Sat Jan 22 12:17:48 2022
epoch 1 training:  92%|█████████▏| 181499/196351 [25:44:01<2:05:44,  1.97it/s]global step 181500, epoch: 1, batch: 181500, loss: 2.09752, acc: 0.86192, time: Sat Jan 22 12:18:40 2022
epoch 1 training:  92%|█████████▏| 181599/196351 [25:44:52<2:04:08,  1.98it/s]global step 181600, epoch: 1, batch: 181600, loss: 1.79555, acc: 0.86194, time: Sat Jan 22 12:19:31 2022
epoch 1 training:  93%|█████████▎| 181699/196351 [25:45:42<2:05:22,  1.95it/s]global step 181700, epoch: 1, batch: 181700, loss: 3.52954, acc: 0.86196, time: Sat Jan 22 12:20:21 2022
epoch 1 training:  93%|█████████▎| 181799/196351 [25:46:33<2:01:18,  2.00it/s]global step 181800, epoch: 1, batch: 181800, loss: 1.04398, acc: 0.86199, time: Sat Jan 22 12:21:12 2022
epoch 1 training:  93%|█████████▎| 181899/196351 [25:47:23<2:02:40,  1.96it/s]global step 181900, epoch: 1, batch: 181900, loss: 2.59486, acc: 0.86200, time: Sat Jan 22 12:22:02 2022
epoch 1 training:  93%|█████████▎| 181999/196351 [25:48:14<1:59:29,  2.00it/s]global step 182000, epoch: 1, batch: 182000, loss: 3.79434, acc: 0.86201, time: Sat Jan 22 12:22:53 2022
epoch 1 training:  93%|█████████▎| 182099/196351 [25:49:04<2:00:24,  1.97it/s]global step 182100, epoch: 1, batch: 182100, loss: 1.41014, acc: 0.86201, time: Sat Jan 22 12:23:43 2022
epoch 1 training:  93%|█████████▎| 182199/196351 [25:49:55<1:59:37,  1.97it/s]global step 182200, epoch: 1, batch: 182200, loss: 2.49168, acc: 0.86202, time: Sat Jan 22 12:24:34 2022
epoch 1 training:  93%|█████████▎| 182299/196351 [25:50:46<1:58:01,  1.98it/s]global step 182300, epoch: 1, batch: 182300, loss: 0.51365, acc: 0.86205, time: Sat Jan 22 12:25:25 2022
epoch 1 training:  93%|█████████▎| 182399/196351 [25:51:37<1:57:24,  1.98it/s]global step 182400, epoch: 1, batch: 182400, loss: 0.93333, acc: 0.86207, time: Sat Jan 22 12:26:15 2022
epoch 1 training:  93%|█████████▎| 182499/196351 [25:52:27<1:57:38,  1.96it/s]global step 182500, epoch: 1, batch: 182500, loss: 3.26917, acc: 0.86210, time: Sat Jan 22 12:27:06 2022
epoch 1 training:  93%|█████████▎| 182599/196351 [25:53:18<1:57:40,  1.95it/s]global step 182600, epoch: 1, batch: 182600, loss: 1.54507, acc: 0.86212, time: Sat Jan 22 12:27:57 2022
epoch 1 training:  93%|█████████▎| 182699/196351 [25:54:09<1:55:04,  1.98it/s]global step 182700, epoch: 1, batch: 182700, loss: 0.15175, acc: 0.86215, time: Sat Jan 22 12:28:48 2022
epoch 1 training:  93%|█████████▎| 182799/196351 [25:55:00<1:55:12,  1.96it/s]global step 182800, epoch: 1, batch: 182800, loss: 0.33113, acc: 0.86216, time: Sat Jan 22 12:29:39 2022
epoch 1 training:  93%|█████████▎| 182899/196351 [25:55:51<1:53:06,  1.98it/s]global step 182900, epoch: 1, batch: 182900, loss: 0.70624, acc: 0.86219, time: Sat Jan 22 12:30:30 2022
epoch 1 training:  93%|█████████▎| 182999/196351 [25:56:42<1:53:23,  1.96it/s]global step 183000, epoch: 1, batch: 183000, loss: 1.49673, acc: 0.86220, time: Sat Jan 22 12:31:21 2022
epoch 1 training:  93%|█████████▎| 183099/196351 [25:57:33<1:51:48,  1.98it/s]global step 183100, epoch: 1, batch: 183100, loss: 2.80225, acc: 0.86223, time: Sat Jan 22 12:32:12 2022
epoch 1 training:  93%|█████████▎| 183199/196351 [25:58:24<1:52:13,  1.95it/s]global step 183200, epoch: 1, batch: 183200, loss: 3.06026, acc: 0.86226, time: Sat Jan 22 12:33:02 2022
epoch 1 training:  93%|█████████▎| 183299/196351 [25:59:14<1:50:01,  1.98it/s]global step 183300, epoch: 1, batch: 183300, loss: 0.43685, acc: 0.86228, time: Sat Jan 22 12:33:53 2022
epoch 1 training:  93%|█████████▎| 183399/196351 [26:00:05<1:49:52,  1.96it/s]global step 183400, epoch: 1, batch: 183400, loss: 2.15289, acc: 0.86231, time: Sat Jan 22 12:34:44 2022
epoch 1 training:  93%|█████████▎| 183499/196351 [26:00:56<1:51:06,  1.93it/s]global step 183500, epoch: 1, batch: 183500, loss: 0.19191, acc: 0.86235, time: Sat Jan 22 12:35:35 2022
epoch 1 training:  94%|█████████▎| 183599/196351 [26:01:47<1:47:47,  1.97it/s]global step 183600, epoch: 1, batch: 183600, loss: 1.84353, acc: 0.86239, time: Sat Jan 22 12:36:26 2022
epoch 1 training:  94%|█████████▎| 183699/196351 [26:02:38<1:47:22,  1.96it/s]global step 183700, epoch: 1, batch: 183700, loss: 0.94997, acc: 0.86238, time: Sat Jan 22 12:37:17 2022
epoch 1 training:  94%|█████████▎| 183799/196351 [26:03:29<1:45:30,  1.98it/s]global step 183800, epoch: 1, batch: 183800, loss: 3.62070, acc: 0.86239, time: Sat Jan 22 12:38:08 2022
epoch 1 training:  94%|█████████▎| 183899/196351 [26:04:20<1:44:32,  1.99it/s]global step 183900, epoch: 1, batch: 183900, loss: 2.12596, acc: 0.86242, time: Sat Jan 22 12:38:59 2022
epoch 1 training:  94%|█████████▎| 183999/196351 [26:05:11<1:45:03,  1.96it/s]global step 184000, epoch: 1, batch: 184000, loss: 1.00519, acc: 0.86245, time: Sat Jan 22 12:39:50 2022
epoch 1 training:  94%|█████████▍| 184099/196351 [26:06:02<1:44:44,  1.95it/s]global step 184100, epoch: 1, batch: 184100, loss: 4.15747, acc: 0.86246, time: Sat Jan 22 12:40:41 2022
epoch 1 training:  94%|█████████▍| 184199/196351 [26:06:53<1:43:40,  1.95it/s]global step 184200, epoch: 1, batch: 184200, loss: 2.69705, acc: 0.86247, time: Sat Jan 22 12:41:32 2022
epoch 1 training:  94%|█████████▍| 184299/196351 [26:07:44<1:41:42,  1.97it/s]global step 184300, epoch: 1, batch: 184300, loss: 0.66674, acc: 0.86250, time: Sat Jan 22 12:42:22 2022
epoch 1 training:  94%|█████████▍| 184399/196351 [26:08:35<1:40:51,  1.97it/s]global step 184400, epoch: 1, batch: 184400, loss: 4.18772, acc: 0.86249, time: Sat Jan 22 12:43:13 2022
epoch 1 training:  94%|█████████▍| 184499/196351 [26:09:26<1:41:29,  1.95it/s]global step 184500, epoch: 1, batch: 184500, loss: 2.57347, acc: 0.86251, time: Sat Jan 22 12:44:04 2022
epoch 1 training:  94%|█████████▍| 184599/196351 [26:10:16<1:39:50,  1.96it/s]global step 184600, epoch: 1, batch: 184600, loss: 2.50752, acc: 0.86252, time: Sat Jan 22 12:44:55 2022
epoch 1 training:  94%|█████████▍| 184699/196351 [26:11:07<1:38:51,  1.96it/s]global step 184700, epoch: 1, batch: 184700, loss: 2.71931, acc: 0.86255, time: Sat Jan 22 12:45:45 2022
epoch 1 training:  94%|█████████▍| 184799/196351 [26:11:57<1:38:54,  1.95it/s]global step 184800, epoch: 1, batch: 184800, loss: 3.46378, acc: 0.86257, time: Sat Jan 22 12:46:36 2022
epoch 1 training:  94%|█████████▍| 184899/196351 [26:12:49<1:36:54,  1.97it/s]global step 184900, epoch: 1, batch: 184900, loss: 0.68808, acc: 0.86259, time: Sat Jan 22 12:47:27 2022
epoch 1 training:  94%|█████████▍| 184999/196351 [26:13:40<1:37:26,  1.94it/s]global step 185000, epoch: 1, batch: 185000, loss: 2.05177, acc: 0.86261, time: Sat Jan 22 12:48:18 2022
epoch 1 training:  94%|█████████▍| 185099/196351 [26:14:31<1:35:41,  1.96it/s]global step 185100, epoch: 1, batch: 185100, loss: 3.14774, acc: 0.86262, time: Sat Jan 22 12:49:09 2022
epoch 1 training:  94%|█████████▍| 185199/196351 [26:15:21<1:34:11,  1.97it/s]global step 185200, epoch: 1, batch: 185200, loss: 0.79899, acc: 0.86266, time: Sat Jan 22 12:50:00 2022
epoch 1 training:  94%|█████████▍| 185299/196351 [26:16:13<1:32:34,  1.99it/s]global step 185300, epoch: 1, batch: 185300, loss: 1.32040, acc: 0.86269, time: Sat Jan 22 12:50:51 2022
epoch 1 training:  94%|█████████▍| 185399/196351 [26:17:04<1:32:50,  1.97it/s]global step 185400, epoch: 1, batch: 185400, loss: 1.40999, acc: 0.86271, time: Sat Jan 22 12:51:43 2022
epoch 1 training:  94%|█████████▍| 185499/196351 [26:17:55<1:31:02,  1.99it/s]global step 185500, epoch: 1, batch: 185500, loss: 1.51652, acc: 0.86273, time: Sat Jan 22 12:52:33 2022
epoch 1 training:  95%|█████████▍| 185599/196351 [26:18:45<1:31:08,  1.97it/s]global step 185600, epoch: 1, batch: 185600, loss: 1.72883, acc: 0.86275, time: Sat Jan 22 12:53:24 2022
epoch 1 training:  95%|█████████▍| 185699/196351 [26:19:36<1:29:56,  1.97it/s]global step 185700, epoch: 1, batch: 185700, loss: 2.50781, acc: 0.86277, time: Sat Jan 22 12:54:15 2022
epoch 1 training:  95%|█████████▍| 185799/196351 [26:20:27<1:30:09,  1.95it/s]global step 185800, epoch: 1, batch: 185800, loss: 0.30955, acc: 0.86277, time: Sat Jan 22 12:55:06 2022
epoch 1 training:  95%|█████████▍| 185899/196351 [26:21:18<1:29:16,  1.95it/s]global step 185900, epoch: 1, batch: 185900, loss: 1.57513, acc: 0.86279, time: Sat Jan 22 12:55:57 2022
epoch 1 training:  95%|█████████▍| 185999/196351 [26:22:09<1:28:37,  1.95it/s]global step 186000, epoch: 1, batch: 186000, loss: 3.50272, acc: 0.86282, time: Sat Jan 22 12:56:48 2022
epoch 1 training:  95%|█████████▍| 186099/196351 [26:23:00<1:26:54,  1.97it/s]global step 186100, epoch: 1, batch: 186100, loss: 2.42366, acc: 0.86284, time: Sat Jan 22 12:57:39 2022
epoch 1 training:  95%|█████████▍| 186199/196351 [26:23:52<1:25:44,  1.97it/s]global step 186200, epoch: 1, batch: 186200, loss: 2.88997, acc: 0.86285, time: Sat Jan 22 12:58:30 2022
epoch 1 training:  95%|█████████▍| 186299/196351 [26:24:43<1:24:43,  1.98it/s]global step 186300, epoch: 1, batch: 186300, loss: 1.33416, acc: 0.86286, time: Sat Jan 22 12:59:21 2022
epoch 1 training:  95%|█████████▍| 186399/196351 [26:25:34<1:23:43,  1.98it/s]global step 186400, epoch: 1, batch: 186400, loss: 3.89699, acc: 0.86288, time: Sat Jan 22 13:00:13 2022
epoch 1 training:  95%|█████████▍| 186499/196351 [26:26:24<1:22:34,  1.99it/s]global step 186500, epoch: 1, batch: 186500, loss: 1.52221, acc: 0.86290, time: Sat Jan 22 13:01:03 2022
epoch 1 training:  95%|█████████▌| 186599/196351 [26:27:15<1:21:35,  1.99it/s]global step 186600, epoch: 1, batch: 186600, loss: 3.70959, acc: 0.86291, time: Sat Jan 22 13:01:54 2022
epoch 1 training:  95%|█████████▌| 186699/196351 [26:28:06<1:21:58,  1.96it/s]global step 186700, epoch: 1, batch: 186700, loss: 2.61383, acc: 0.86292, time: Sat Jan 22 13:02:44 2022
epoch 1 training:  95%|█████████▌| 186799/196351 [26:28:56<1:19:53,  1.99it/s]global step 186800, epoch: 1, batch: 186800, loss: 3.29794, acc: 0.86293, time: Sat Jan 22 13:03:35 2022
epoch 1 training:  95%|█████████▌| 186899/196351 [26:29:47<1:19:35,  1.98it/s]global step 186900, epoch: 1, batch: 186900, loss: 2.32354, acc: 0.86294, time: Sat Jan 22 13:04:26 2022
epoch 1 training:  95%|█████████▌| 186999/196351 [26:30:38<1:18:57,  1.97it/s]global step 187000, epoch: 1, batch: 187000, loss: 2.45539, acc: 0.86297, time: Sat Jan 22 13:05:16 2022
epoch 1 training:  95%|█████████▌| 187099/196351 [26:31:28<1:18:03,  1.98it/s]global step 187100, epoch: 1, batch: 187100, loss: 1.28095, acc: 0.86300, time: Sat Jan 22 13:06:07 2022
epoch 1 training:  95%|█████████▌| 187199/196351 [26:32:19<1:18:10,  1.95it/s]global step 187200, epoch: 1, batch: 187200, loss: 2.42663, acc: 0.86300, time: Sat Jan 22 13:06:58 2022
epoch 1 training:  95%|█████████▌| 187299/196351 [26:33:10<1:16:19,  1.98it/s]global step 187300, epoch: 1, batch: 187300, loss: 4.49474, acc: 0.86301, time: Sat Jan 22 13:07:48 2022
epoch 1 training:  95%|█████████▌| 187399/196351 [26:34:01<1:16:07,  1.96it/s]global step 187400, epoch: 1, batch: 187400, loss: 2.02185, acc: 0.86302, time: Sat Jan 22 13:08:39 2022
epoch 1 training:  95%|█████████▌| 187499/196351 [26:34:51<1:15:27,  1.96it/s]global step 187500, epoch: 1, batch: 187500, loss: 2.68436, acc: 0.86304, time: Sat Jan 22 13:09:30 2022
epoch 1 training:  96%|█████████▌| 187599/196351 [26:35:42<1:13:14,  1.99it/s]global step 187600, epoch: 1, batch: 187600, loss: 0.36355, acc: 0.86307, time: Sat Jan 22 13:10:21 2022
epoch 1 training:  96%|█████████▌| 187699/196351 [26:36:33<1:13:32,  1.96it/s]global step 187700, epoch: 1, batch: 187700, loss: 3.48025, acc: 0.86308, time: Sat Jan 22 13:11:12 2022
epoch 1 training:  96%|█████████▌| 187799/196351 [26:37:24<1:13:29,  1.94it/s]global step 187800, epoch: 1, batch: 187800, loss: 2.67353, acc: 0.86310, time: Sat Jan 22 13:12:03 2022
epoch 1 training:  96%|█████████▌| 187899/196351 [26:38:15<1:11:24,  1.97it/s]global step 187900, epoch: 1, batch: 187900, loss: 2.70571, acc: 0.86311, time: Sat Jan 22 13:12:54 2022
epoch 1 training:  96%|█████████▌| 187999/196351 [26:39:06<1:10:35,  1.97it/s]global step 188000, epoch: 1, batch: 188000, loss: 2.57016, acc: 0.86314, time: Sat Jan 22 13:13:45 2022
epoch 1 training:  96%|█████████▌| 188099/196351 [26:39:57<1:10:31,  1.95it/s]global step 188100, epoch: 1, batch: 188100, loss: 1.74619, acc: 0.86316, time: Sat Jan 22 13:14:36 2022
epoch 1 training:  96%|█████████▌| 188199/196351 [26:40:48<1:08:54,  1.97it/s]global step 188200, epoch: 1, batch: 188200, loss: 3.63265, acc: 0.86319, time: Sat Jan 22 13:15:27 2022
epoch 1 training:  96%|█████████▌| 188299/196351 [26:41:39<1:07:29,  1.99it/s]global step 188300, epoch: 1, batch: 188300, loss: 1.30846, acc: 0.86322, time: Sat Jan 22 13:16:18 2022
epoch 1 training:  96%|█████████▌| 188399/196351 [26:42:30<1:07:03,  1.98it/s]global step 188400, epoch: 1, batch: 188400, loss: 2.23841, acc: 0.86323, time: Sat Jan 22 13:17:09 2022
epoch 1 training:  96%|█████████▌| 188499/196351 [26:43:21<1:06:34,  1.97it/s]global step 188500, epoch: 1, batch: 188500, loss: 2.46350, acc: 0.86323, time: Sat Jan 22 13:17:59 2022
epoch 1 training:  96%|█████████▌| 188599/196351 [26:44:12<1:06:13,  1.95it/s]global step 188600, epoch: 1, batch: 188600, loss: 0.94510, acc: 0.86325, time: Sat Jan 22 13:18:50 2022
epoch 1 training:  96%|█████████▌| 188699/196351 [26:45:02<1:04:20,  1.98it/s]global step 188700, epoch: 1, batch: 188700, loss: 0.65623, acc: 0.86329, time: Sat Jan 22 13:19:41 2022
epoch 1 training:  96%|█████████▌| 188799/196351 [26:45:53<1:03:47,  1.97it/s]global step 188800, epoch: 1, batch: 188800, loss: 0.87828, acc: 0.86331, time: Sat Jan 22 13:20:32 2022
epoch 1 training:  96%|█████████▌| 188899/196351 [26:46:44<1:04:40,  1.92it/s]global step 188900, epoch: 1, batch: 188900, loss: 2.22632, acc: 0.86332, time: Sat Jan 22 13:21:23 2022
epoch 1 training:  96%|█████████▋| 188999/196351 [26:47:35<1:03:08,  1.94it/s]global step 189000, epoch: 1, batch: 189000, loss: 3.88387, acc: 0.86333, time: Sat Jan 22 13:22:14 2022
epoch 1 training:  96%|█████████▋| 189099/196351 [26:48:26<1:01:37,  1.96it/s]global step 189100, epoch: 1, batch: 189100, loss: 0.52150, acc: 0.86335, time: Sat Jan 22 13:23:05 2022
epoch 1 training:  96%|█████████▋| 189199/196351 [26:49:18<1:02:44,  1.90it/s]global step 189200, epoch: 1, batch: 189200, loss: 4.15809, acc: 0.86336, time: Sat Jan 22 13:23:56 2022
epoch 1 training:  96%|█████████▋| 189299/196351 [26:50:09<1:00:32,  1.94it/s]global step 189300, epoch: 1, batch: 189300, loss: 2.37779, acc: 0.86335, time: Sat Jan 22 13:24:47 2022
epoch 1 training:  96%|█████████▋| 189399/196351 [26:51:00<58:46,  1.97it/s]global step 189400, epoch: 1, batch: 189400, loss: 3.09089, acc: 0.86337, time: Sat Jan 22 13:25:39 2022
epoch 1 training:  97%|█████████▋| 189499/196351 [26:51:51<58:37,  1.95it/s]global step 189500, epoch: 1, batch: 189500, loss: 0.42601, acc: 0.86340, time: Sat Jan 22 13:26:29 2022
epoch 1 training:  97%|█████████▋| 189599/196351 [26:52:42<57:36,  1.95it/s]global step 189600, epoch: 1, batch: 189600, loss: 2.99283, acc: 0.86342, time: Sat Jan 22 13:27:21 2022
epoch 1 training:  97%|█████████▋| 189699/196351 [26:53:33<56:24,  1.97it/s]global step 189700, epoch: 1, batch: 189700, loss: 4.02577, acc: 0.86344, time: Sat Jan 22 13:28:11 2022
epoch 1 training:  97%|█████████▋| 189799/196351 [26:54:23<55:03,  1.98it/s]global step 189800, epoch: 1, batch: 189800, loss: 1.05770, acc: 0.86347, time: Sat Jan 22 13:29:02 2022
epoch 1 training:  97%|█████████▋| 189899/196351 [26:55:14<54:20,  1.98it/s]global step 189900, epoch: 1, batch: 189900, loss: 3.29459, acc: 0.86349, time: Sat Jan 22 13:29:53 2022
epoch 1 training:  97%|█████████▋| 189999/196351 [26:56:05<52:58,  2.00it/s]global step 190000, epoch: 1, batch: 190000, loss: 2.81577, acc: 0.86349, time: Sat Jan 22 13:30:43 2022
epoch 1 training:  97%|█████████▋| 190099/196351 [26:56:55<53:10,  1.96it/s]global step 190100, epoch: 1, batch: 190100, loss: 1.75558, acc: 0.86352, time: Sat Jan 22 13:31:34 2022
epoch 1 training:  97%|█████████▋| 190199/196351 [26:57:46<52:04,  1.97it/s]global step 190200, epoch: 1, batch: 190200, loss: 2.20759, acc: 0.86353, time: Sat Jan 22 13:32:25 2022
epoch 1 training:  97%|█████████▋| 190299/196351 [26:58:37<51:11,  1.97it/s]global step 190300, epoch: 1, batch: 190300, loss: 1.12673, acc: 0.86353, time: Sat Jan 22 13:33:16 2022
epoch 1 training:  97%|█████████▋| 190399/196351 [26:59:28<50:18,  1.97it/s]global step 190400, epoch: 1, batch: 190400, loss: 3.17757, acc: 0.86355, time: Sat Jan 22 13:34:07 2022
epoch 1 training:  97%|█████████▋| 190499/196351 [27:00:19<49:41,  1.96it/s]global step 190500, epoch: 1, batch: 190500, loss: 0.69082, acc: 0.86356, time: Sat Jan 22 13:34:58 2022
epoch 1 training:  97%|█████████▋| 190599/196351 [27:01:10<48:45,  1.97it/s]global step 190600, epoch: 1, batch: 190600, loss: 2.43539, acc: 0.86358, time: Sat Jan 22 13:35:49 2022
epoch 1 training:  97%|█████████▋| 190699/196351 [27:02:01<47:08,  2.00it/s]global step 190700, epoch: 1, batch: 190700, loss: 2.03151, acc: 0.86361, time: Sat Jan 22 13:36:40 2022
epoch 1 training:  97%|█████████▋| 190799/196351 [27:02:52<46:25,  1.99it/s]global step 190800, epoch: 1, batch: 190800, loss: 1.60286, acc: 0.86362, time: Sat Jan 22 13:37:30 2022
epoch 1 training:  97%|█████████▋| 190899/196351 [27:03:42<46:14,  1.96it/s]global step 190900, epoch: 1, batch: 190900, loss: 2.10292, acc: 0.86364, time: Sat Jan 22 13:38:21 2022
epoch 1 training:  97%|█████████▋| 190999/196351 [27:04:33<45:37,  1.95it/s]global step 191000, epoch: 1, batch: 191000, loss: 3.64887, acc: 0.86365, time: Sat Jan 22 13:39:12 2022
epoch 1 training:  97%|█████████▋| 191099/196351 [27:05:24<44:24,  1.97it/s]global step 191100, epoch: 1, batch: 191100, loss: 0.66750, acc: 0.86367, time: Sat Jan 22 13:40:03 2022
epoch 1 training:  97%|█████████▋| 191199/196351 [27:06:15<44:39,  1.92it/s]global step 191200, epoch: 1, batch: 191200, loss: 1.55232, acc: 0.86370, time: Sat Jan 22 13:40:54 2022
epoch 1 training:  97%|█████████▋| 191299/196351 [27:07:07<43:10,  1.95it/s]global step 191300, epoch: 1, batch: 191300, loss: 2.56467, acc: 0.86371, time: Sat Jan 22 13:41:45 2022
epoch 1 training:  97%|█████████▋| 191399/196351 [27:07:58<41:33,  1.99it/s]global step 191400, epoch: 1, batch: 191400, loss: 1.53829, acc: 0.86374, time: Sat Jan 22 13:42:36 2022
epoch 1 training:  98%|█████████▊| 191499/196351 [27:08:48<41:09,  1.97it/s]global step 191500, epoch: 1, batch: 191500, loss: 3.96561, acc: 0.86374, time: Sat Jan 22 13:43:27 2022
epoch 1 training:  98%|█████████▊| 191599/196351 [27:09:39<40:21,  1.96it/s]global step 191600, epoch: 1, batch: 191600, loss: 1.30790, acc: 0.86378, time: Sat Jan 22 13:44:18 2022
epoch 1 training:  98%|█████████▊| 191699/196351 [27:10:30<39:32,  1.96it/s]global step 191700, epoch: 1, batch: 191700, loss: 2.86819, acc: 0.86381, time: Sat Jan 22 13:45:08 2022
epoch 1 training:  98%|█████████▊| 191799/196351 [27:11:20<38:16,  1.98it/s]global step 191800, epoch: 1, batch: 191800, loss: 2.12542, acc: 0.86381, time: Sat Jan 22 13:45:59 2022
epoch 1 training:  98%|█████████▊| 191899/196351 [27:12:11<37:26,  1.98it/s]global step 191900, epoch: 1, batch: 191900, loss: 0.95914, acc: 0.86383, time: Sat Jan 22 13:46:50 2022
epoch 1 training:  98%|█████████▊| 191999/196351 [27:13:01<36:46,  1.97it/s]global step 192000, epoch: 1, batch: 192000, loss: 1.24576, acc: 0.86384, time: Sat Jan 22 13:47:40 2022
epoch 1 training:  98%|█████████▊| 192099/196351 [27:13:52<35:56,  1.97it/s]global step 192100, epoch: 1, batch: 192100, loss: 1.09584, acc: 0.86386, time: Sat Jan 22 13:48:31 2022
epoch 1 training:  98%|█████████▊| 192199/196351 [27:14:43<34:47,  1.99it/s]global step 192200, epoch: 1, batch: 192200, loss: 3.26185, acc: 0.86386, time: Sat Jan 22 13:49:21 2022
epoch 1 training:  98%|█████████▊| 192299/196351 [27:15:34<34:38,  1.95it/s]global step 192300, epoch: 1, batch: 192300, loss: 2.54564, acc: 0.86389, time: Sat Jan 22 13:50:12 2022
epoch 1 training:  98%|█████████▊| 192399/196351 [27:16:24<33:41,  1.95it/s]global step 192400, epoch: 1, batch: 192400, loss: 2.44916, acc: 0.86389, time: Sat Jan 22 13:51:03 2022
epoch 1 training:  98%|█████████▊| 192499/196351 [27:17:15<33:31,  1.92it/s]global step 192500, epoch: 1, batch: 192500, loss: 3.71793, acc: 0.86391, time: Sat Jan 22 13:51:54 2022
epoch 1 training:  98%|█████████▊| 192599/196351 [27:18:06<31:58,  1.96it/s]global step 192600, epoch: 1, batch: 192600, loss: 0.58635, acc: 0.86393, time: Sat Jan 22 13:52:45 2022
epoch 1 training:  98%|█████████▊| 192699/196351 [27:18:57<30:56,  1.97it/s]global step 192700, epoch: 1, batch: 192700, loss: 1.09409, acc: 0.86396, time: Sat Jan 22 13:53:36 2022
epoch 1 training:  98%|█████████▊| 192799/196351 [27:19:48<29:52,  1.98it/s]global step 192800, epoch: 1, batch: 192800, loss: 1.97515, acc: 0.86398, time: Sat Jan 22 13:54:27 2022
epoch 1 training:  98%|█████████▊| 192899/196351 [27:20:41<29:18,  1.96it/s]global step 192900, epoch: 1, batch: 192900, loss: 2.22908, acc: 0.86400, time: Sat Jan 22 13:55:20 2022
epoch 1 training:  98%|█████████▊| 192999/196351 [27:21:36<28:29,  1.96it/s]global step 193000, epoch: 1, batch: 193000, loss: 1.39401, acc: 0.86402, time: Sat Jan 22 13:56:14 2022
epoch 1 training:  98%|█████████▊| 193099/196351 [27:22:28<31:00,  1.75it/s]global step 193100, epoch: 1, batch: 193100, loss: 0.91280, acc: 0.86404, time: Sat Jan 22 13:57:07 2022
epoch 1 training:  98%|█████████▊| 193199/196351 [27:23:22<26:35,  1.98it/s]global step 193200, epoch: 1, batch: 193200, loss: 1.93566, acc: 0.86405, time: Sat Jan 22 13:58:01 2022
epoch 1 training:  98%|█████████▊| 193299/196351 [27:24:13<25:48,  1.97it/s]global step 193300, epoch: 1, batch: 193300, loss: 0.84239, acc: 0.86406, time: Sat Jan 22 13:58:52 2022
epoch 1 training:  98%|█████████▊| 193399/196351 [27:25:04<24:53,  1.98it/s]global step 193400, epoch: 1, batch: 193400, loss: 2.21810, acc: 0.86408, time: Sat Jan 22 13:59:43 2022
epoch 1 training:  99%|█████████▊| 193499/196351 [27:25:55<25:27,  1.87it/s]global step 193500, epoch: 1, batch: 193500, loss: 0.48760, acc: 0.86409, time: Sat Jan 22 14:00:34 2022
epoch 1 training:  99%|█████████▊| 193599/196351 [27:26:48<23:17,  1.97it/s]global step 193600, epoch: 1, batch: 193600, loss: 3.09969, acc: 0.86411, time: Sat Jan 22 14:01:27 2022
epoch 1 training:  99%|█████████▊| 193699/196351 [27:27:39<22:11,  1.99it/s]global step 193700, epoch: 1, batch: 193700, loss: 0.83065, acc: 0.86412, time: Sat Jan 22 14:02:18 2022
epoch 1 training:  99%|█████████▊| 193799/196351 [27:28:30<21:36,  1.97it/s]global step 193800, epoch: 1, batch: 193800, loss: 4.06865, acc: 0.86414, time: Sat Jan 22 14:03:08 2022
epoch 1 training:  99%|█████████▉| 193899/196351 [27:29:21<21:09,  1.93it/s]global step 193900, epoch: 1, batch: 193900, loss: 2.72635, acc: 0.86417, time: Sat Jan 22 14:04:00 2022
epoch 1 training:  99%|█████████▉| 193999/196351 [27:30:12<20:13,  1.94it/s]global step 194000, epoch: 1, batch: 194000, loss: 1.38415, acc: 0.86419, time: Sat Jan 22 14:04:51 2022
epoch 1 training:  99%|█████████▉| 194099/196351 [27:31:04<19:03,  1.97it/s]global step 194100, epoch: 1, batch: 194100, loss: 1.35041, acc: 0.86422, time: Sat Jan 22 14:05:43 2022
epoch 1 training:  99%|█████████▉| 194199/196351 [27:31:56<18:12,  1.97it/s]global step 194200, epoch: 1, batch: 194200, loss: 4.74450, acc: 0.86426, time: Sat Jan 22 14:06:34 2022
epoch 1 training:  99%|█████████▉| 194299/196351 [27:32:47<17:21,  1.97it/s]global step 194300, epoch: 1, batch: 194300, loss: 1.43117, acc: 0.86427, time: Sat Jan 22 14:07:25 2022
epoch 1 training:  99%|█████████▉| 194399/196351 [27:33:37<16:27,  1.98it/s]global step 194400, epoch: 1, batch: 194400, loss: 2.23057, acc: 0.86428, time: Sat Jan 22 14:08:16 2022
epoch 1 training:  99%|█████████▉| 194499/196351 [27:34:28<15:35,  1.98it/s]global step 194500, epoch: 1, batch: 194500, loss: 1.32562, acc: 0.86430, time: Sat Jan 22 14:09:07 2022
epoch 1 training:  99%|█████████▉| 194599/196351 [27:35:19<14:46,  1.98it/s]global step 194600, epoch: 1, batch: 194600, loss: 1.92165, acc: 0.86432, time: Sat Jan 22 14:09:57 2022
epoch 1 training:  99%|█████████▉| 194699/196351 [27:36:10<13:59,  1.97it/s]global step 194700, epoch: 1, batch: 194700, loss: 0.60239, acc: 0.86436, time: Sat Jan 22 14:10:48 2022
epoch 1 training:  99%|█████████▉| 194799/196351 [27:37:00<13:19,  1.94it/s]global step 194800, epoch: 1, batch: 194800, loss: 0.74014, acc: 0.86440, time: Sat Jan 22 14:11:39 2022
epoch 1 training:  99%|█████████▉| 194899/196351 [27:37:51<12:10,  1.99it/s]global step 194900, epoch: 1, batch: 194900, loss: 0.54513, acc: 0.86440, time: Sat Jan 22 14:12:30 2022
epoch 1 training:  99%|█████████▉| 194999/196351 [27:38:42<11:23,  1.98it/s]global step 195000, epoch: 1, batch: 195000, loss: 2.42761, acc: 0.86441, time: Sat Jan 22 14:13:21 2022
epoch 1 training:  99%|█████████▉| 195099/196351 [27:39:33<10:38,  1.96it/s]global step 195100, epoch: 1, batch: 195100, loss: 2.65973, acc: 0.86442, time: Sat Jan 22 14:14:12 2022
epoch 1 training:  99%|█████████▉| 195199/196351 [27:40:25<09:56,  1.93it/s]global step 195200, epoch: 1, batch: 195200, loss: 2.83761, acc: 0.86443, time: Sat Jan 22 14:15:04 2022
epoch 1 training:  99%|█████████▉| 195299/196351 [27:41:16<09:02,  1.94it/s]global step 195300, epoch: 1, batch: 195300, loss: 0.13992, acc: 0.86445, time: Sat Jan 22 14:15:55 2022
epoch 1 training: 100%|█████████▉| 195399/196351 [27:42:07<08:03,  1.97it/s]global step 195400, epoch: 1, batch: 195400, loss: 1.72686, acc: 0.86449, time: Sat Jan 22 14:16:46 2022
epoch 1 training: 100%|█████████▉| 195499/196351 [27:42:58<07:17,  1.95it/s]global step 195500, epoch: 1, batch: 195500, loss: 1.17931, acc: 0.86450, time: Sat Jan 22 14:17:37 2022
epoch 1 training: 100%|█████████▉| 195599/196351 [27:43:49<06:26,  1.95it/s]global step 195600, epoch: 1, batch: 195600, loss: 3.31716, acc: 0.86449, time: Sat Jan 22 14:18:28 2022
epoch 1 training: 100%|█████████▉| 195699/196351 [27:44:40<05:29,  1.98it/s]global step 195700, epoch: 1, batch: 195700, loss: 3.28433, acc: 0.86449, time: Sat Jan 22 14:19:19 2022
epoch 1 training: 100%|█████████▉| 195799/196351 [27:45:31<04:43,  1.95it/s]global step 195800, epoch: 1, batch: 195800, loss: 0.76883, acc: 0.86452, time: Sat Jan 22 14:20:10 2022
epoch 1 training: 100%|█████████▉| 195899/196351 [27:46:22<03:52,  1.94it/s]global step 195900, epoch: 1, batch: 195900, loss: 2.03082, acc: 0.86453, time: Sat Jan 22 14:21:01 2022
epoch 1 training: 100%|█████████▉| 195999/196351 [27:47:14<02:58,  1.97it/s]global step 196000, epoch: 1, batch: 196000, loss: 2.38165, acc: 0.86454, time: Sat Jan 22 14:21:53 2022
epoch 1 training: 100%|█████████▉| 196099/196351 [27:48:05<02:09,  1.95it/s]global step 196100, epoch: 1, batch: 196100, loss: 0.99573, acc: 0.86456, time: Sat Jan 22 14:22:44 2022
epoch 1 training: 100%|█████████▉| 196199/196351 [27:48:56<01:17,  1.96it/s]global step 196200, epoch: 1, batch: 196200, loss: 2.12941, acc: 0.86458, time: Sat Jan 22 14:23:35 2022
epoch 1 training: 100%|█████████▉| 196299/196351 [27:49:48<00:26,  1.95it/s]global step 196300, epoch: 1, batch: 196300, loss: 0.42464, acc: 0.86461, time: Sat Jan 22 14:24:27 2022
epoch 1 training: 100%|██████████| 196351/196351 [27:50:15<00:00,  1.96it/s]
epoch 1 dev: 100%|██████████| 307/307 [01:03<00:00,  4.85it/s]
epoch: 1, dev accuracy: 0.90698, time: Sat Jan 22 14:25:57 2022
epoch 2 training:   0%|          | 48/196351 [00:24<27:39:28,  1.97it/s]global step 196400, epoch: 2, batch: 49, loss: 2.29493, acc: 0.89796, time: Sat Jan 22 14:26:22 2022
epoch 2 training:   0%|          | 148/196351 [01:15<27:39:01,  1.97it/s]global step 196500, epoch: 2, batch: 149, loss: 0.72625, acc: 0.93624, time: Sat Jan 22 14:27:13 2022
epoch 2 training:   0%|          | 248/196351 [02:07<28:26:49,  1.91it/s]global step 196600, epoch: 2, batch: 249, loss: 0.27165, acc: 0.93373, time: Sat Jan 22 14:28:04 2022
epoch 2 training:   0%|          | 348/196351 [02:58<27:43:20,  1.96it/s]global step 196700, epoch: 2, batch: 349, loss: 0.88996, acc: 0.92407, time: Sat Jan 22 14:28:55 2022
epoch 2 training:   0%|          | 448/196351 [03:49<28:19:24,  1.92it/s]global step 196800, epoch: 2, batch: 449, loss: 1.95926, acc: 0.92428, time: Sat Jan 22 14:29:46 2022
epoch 2 training:   0%|          | 548/196351 [04:40<27:54:53,  1.95it/s]global step 196900, epoch: 2, batch: 549, loss: 0.14795, acc: 0.92896, time: Sat Jan 22 14:30:37 2022
epoch 2 training:   0%|          | 648/196351 [05:31<27:34:50,  1.97it/s]global step 197000, epoch: 2, batch: 649, loss: 0.80210, acc: 0.92681, time: Sat Jan 22 14:31:28 2022
epoch 2 training:   0%|          | 748/196351 [06:22<27:41:08,  1.96it/s]global step 197100, epoch: 2, batch: 749, loss: 0.81844, acc: 0.92991, time: Sat Jan 22 14:32:19 2022
epoch 2 training:   0%|          | 848/196351 [07:13<27:12:30,  2.00it/s]global step 197200, epoch: 2, batch: 849, loss: 2.76885, acc: 0.93110, time: Sat Jan 22 14:33:10 2022
epoch 2 training:   0%|          | 948/196351 [08:04<27:53:10,  1.95it/s]global step 197300, epoch: 2, batch: 949, loss: 1.61418, acc: 0.93151, time: Sat Jan 22 14:34:01 2022
epoch 2 training:   1%|          | 1048/196351 [08:55<27:16:19,  1.99it/s]global step 197400, epoch: 2, batch: 1049, loss: 1.66789, acc: 0.93184, time: Sat Jan 22 14:34:52 2022
epoch 2 training:   1%|          | 1148/196351 [09:46<27:52:54,  1.94it/s]global step 197500, epoch: 2, batch: 1149, loss: 2.20524, acc: 0.93124, time: Sat Jan 22 14:35:43 2022
epoch 2 training:   1%|          | 1248/196351 [10:37<27:55:48,  1.94it/s]global step 197600, epoch: 2, batch: 1249, loss: 0.97322, acc: 0.92994, time: Sat Jan 22 14:36:34 2022
epoch 2 training:   1%|          | 1348/196351 [11:28<27:27:00,  1.97it/s]global step 197700, epoch: 2, batch: 1349, loss: 4.44350, acc: 0.93032, time: Sat Jan 22 14:37:25 2022
epoch 2 training:   1%|          | 1448/196351 [12:19<27:18:25,  1.98it/s]global step 197800, epoch: 2, batch: 1449, loss: 2.57581, acc: 0.92754, time: Sat Jan 22 14:38:16 2022
epoch 2 training:   1%|          | 1548/196351 [13:10<27:47:46,  1.95it/s]global step 197900, epoch: 2, batch: 1549, loss: 0.36379, acc: 0.92705, time: Sat Jan 22 14:39:07 2022
epoch 2 training:   1%|          | 1648/196351 [14:01<27:28:35,  1.97it/s]global step 198000, epoch: 2, batch: 1649, loss: 1.10613, acc: 0.92905, time: Sat Jan 22 14:39:59 2022
epoch 2 training:   1%|          | 1748/196351 [14:53<27:40:56,  1.95it/s]global step 198100, epoch: 2, batch: 1749, loss: 0.81985, acc: 0.92996, time: Sat Jan 22 14:40:50 2022
epoch 2 training:   1%|          | 1848/196351 [15:44<27:35:22,  1.96it/s]global step 198200, epoch: 2, batch: 1849, loss: 0.95215, acc: 0.92996, time: Sat Jan 22 14:41:41 2022
epoch 2 training:   1%|          | 1948/196351 [16:35<27:41:03,  1.95it/s]global step 198300, epoch: 2, batch: 1949, loss: 2.53167, acc: 0.92817, time: Sat Jan 22 14:42:32 2022
epoch 2 training:   1%|          | 2048/196351 [17:26<27:16:22,  1.98it/s]global step 198400, epoch: 2, batch: 2049, loss: 2.60965, acc: 0.92801, time: Sat Jan 22 14:43:24 2022
epoch 2 training:   1%|          | 2148/196351 [18:17<27:45:12,  1.94it/s]global step 198500, epoch: 2, batch: 2149, loss: 2.75951, acc: 0.92834, time: Sat Jan 22 14:44:15 2022
epoch 2 training:   1%|          | 2248/196351 [19:09<27:45:20,  1.94it/s]global step 198600, epoch: 2, batch: 2249, loss: 1.87950, acc: 0.92863, time: Sat Jan 22 14:45:06 2022
epoch 2 training:   1%|          | 2348/196351 [20:00<27:22:16,  1.97it/s]global step 198700, epoch: 2, batch: 2349, loss: 5.89095, acc: 0.91954, time: Sat Jan 22 14:45:57 2022
epoch 2 training:   1%|          | 2448/196351 [20:51<27:33:05,  1.95it/s]global step 198800, epoch: 2, batch: 2449, loss: 5.79376, acc: 0.90445, time: Sat Jan 22 14:46:48 2022
epoch 2 training:   1%|▏         | 2548/196351 [21:42<27:43:49,  1.94it/s]global step 198900, epoch: 2, batch: 2549, loss: 5.11261, acc: 0.89702, time: Sat Jan 22 14:47:40 2022
epoch 2 training:   1%|▏         | 2648/196351 [22:34<27:54:31,  1.93it/s]global step 199000, epoch: 2, batch: 2649, loss: 3.75325, acc: 0.89581, time: Sat Jan 22 14:48:32 2022
epoch 2 training:   1%|▏         | 2748/196351 [23:25<27:45:53,  1.94it/s]global step 199100, epoch: 2, batch: 2749, loss: 4.23320, acc: 0.89396, time: Sat Jan 22 14:49:23 2022
epoch 2 training:   1%|▏         | 2848/196351 [24:16<27:17:57,  1.97it/s]global step 199200, epoch: 2, batch: 2849, loss: 5.15901, acc: 0.89435, time: Sat Jan 22 14:50:13 2022
epoch 2 training:   2%|▏         | 2948/196351 [25:07<27:03:13,  1.99it/s]global step 199300, epoch: 2, batch: 2949, loss: 4.47615, acc: 0.89285, time: Sat Jan 22 14:51:04 2022
epoch 2 training:   2%|▏         | 3048/196351 [25:57<27:14:27,  1.97it/s]global step 199400, epoch: 2, batch: 3049, loss: 2.80092, acc: 0.89210, time: Sat Jan 22 14:51:55 2022
epoch 2 training:   2%|▏         | 3148/196351 [26:48<27:14:00,  1.97it/s]global step 199500, epoch: 2, batch: 3149, loss: 1.97755, acc: 0.89235, time: Sat Jan 22 14:52:46 2022
epoch 2 training:   2%|▏         | 3248/196351 [27:39<27:34:24,  1.95it/s]global step 199600, epoch: 2, batch: 3249, loss: 1.96535, acc: 0.89320, time: Sat Jan 22 14:53:37 2022
epoch 2 training:   2%|▏         | 3348/196351 [28:30<27:14:35,  1.97it/s]global step 199700, epoch: 2, batch: 3349, loss: 1.30696, acc: 0.89400, time: Sat Jan 22 14:54:28 2022
epoch 2 training:   2%|▏         | 3448/196351 [29:21<27:10:45,  1.97it/s]global step 199800, epoch: 2, batch: 3449, loss: 3.03925, acc: 0.89417, time: Sat Jan 22 14:55:19 2022
epoch 2 training:   2%|▏         | 3548/196351 [30:12<27:04:59,  1.98it/s]global step 199900, epoch: 2, batch: 3549, loss: 2.93507, acc: 0.89490, time: Sat Jan 22 14:56:10 2022
epoch 2 training:   2%|▏         | 3648/196351 [31:03<27:54:53,  1.92it/s]global step 200000, epoch: 2, batch: 3649, loss: 2.55385, acc: 0.89504, time: Sat Jan 22 14:57:00 2022
epoch 2 training:   2%|▏         | 3748/196351 [31:54<27:12:52,  1.97it/s]global step 200100, epoch: 2, batch: 3749, loss: 3.26107, acc: 0.89451, time: Sat Jan 22 14:57:51 2022
epoch 2 training:   2%|▏         | 3848/196351 [32:45<27:01:55,  1.98it/s]global step 200200, epoch: 2, batch: 3849, loss: 2.87722, acc: 0.89309, time: Sat Jan 22 14:58:42 2022
epoch 2 training:   2%|▏         | 3948/196351 [33:36<27:24:56,  1.95it/s]global step 200300, epoch: 2, batch: 3949, loss: 5.75265, acc: 0.88883, time: Sat Jan 22 14:59:33 2022
epoch 2 training:   2%|▏         | 4048/196351 [34:27<27:11:25,  1.96it/s]global step 200400, epoch: 2, batch: 4049, loss: 5.70119, acc: 0.88343, time: Sat Jan 22 15:00:25 2022
epoch 2 training:   2%|▏         | 4148/196351 [35:18<27:03:41,  1.97it/s]global step 200500, epoch: 2, batch: 4149, loss: 5.06794, acc: 0.88045, time: Sat Jan 22 15:01:16 2022
epoch 2 training:   2%|▏         | 4248/196351 [36:10<27:30:05,  1.94it/s]global step 200600, epoch: 2, batch: 4249, loss: 5.42863, acc: 0.87750, time: Sat Jan 22 15:02:07 2022
epoch 2 training:   2%|▏         | 4348/196351 [37:01<27:05:59,  1.97it/s]global step 200700, epoch: 2, batch: 4349, loss: 5.46238, acc: 0.87549, time: Sat Jan 22 15:02:59 2022
epoch 2 training:   2%|▏         | 4448/196351 [37:53<27:50:56,  1.91it/s]global step 200800, epoch: 2, batch: 4449, loss: 4.97952, acc: 0.87087, time: Sat Jan 22 15:03:51 2022
epoch 2 training:   2%|▏         | 4548/196351 [38:45<28:50:12,  1.85it/s]global step 200900, epoch: 2, batch: 4549, loss: 5.84252, acc: 0.86689, time: Sat Jan 22 15:04:43 2022
epoch 2 training:   2%|▏         | 4648/196351 [39:36<27:10:28,  1.96it/s]global step 201000, epoch: 2, batch: 4649, loss: 4.95966, acc: 0.86309, time: Sat Jan 22 15:05:34 2022
epoch 2 training:   2%|▏         | 4748/196351 [40:28<27:09:15,  1.96it/s]global step 201100, epoch: 2, batch: 4749, loss: 4.57072, acc: 0.86134, time: Sat Jan 22 15:06:25 2022
epoch 2 training:   2%|▏         | 4848/196351 [41:19<27:19:00,  1.95it/s]global step 201200, epoch: 2, batch: 4849, loss: 4.53731, acc: 0.85976, time: Sat Jan 22 15:07:16 2022
epoch 2 training:   3%|▎         | 4948/196351 [42:10<27:12:21,  1.95it/s]global step 201300, epoch: 2, batch: 4949, loss: 2.59008, acc: 0.85906, time: Sat Jan 22 15:08:07 2022
epoch 2 training:   3%|▎         | 5048/196351 [43:01<26:53:46,  1.98it/s]global step 201400, epoch: 2, batch: 5049, loss: 2.42477, acc: 0.85908, time: Sat Jan 22 15:08:58 2022
epoch 2 training:   3%|▎         | 5148/196351 [43:52<26:59:49,  1.97it/s]global step 201500, epoch: 2, batch: 5149, loss: 3.17832, acc: 0.86007, time: Sat Jan 22 15:09:49 2022
epoch 2 training:   3%|▎         | 5248/196351 [44:43<27:32:49,  1.93it/s]global step 201600, epoch: 2, batch: 5249, loss: 2.64387, acc: 0.86026, time: Sat Jan 22 15:10:40 2022
epoch 2 training:   3%|▎         | 5348/196351 [45:34<27:04:03,  1.96it/s]global step 201700, epoch: 2, batch: 5349, loss: 3.24349, acc: 0.86091, time: Sat Jan 22 15:11:31 2022
epoch 2 training:   3%|▎         | 5448/196351 [46:25<26:57:33,  1.97it/s]global step 201800, epoch: 2, batch: 5449, loss: 3.02572, acc: 0.86172, time: Sat Jan 22 15:12:22 2022
epoch 2 training:   3%|▎         | 5548/196351 [47:17<27:31:57,  1.93it/s]global step 201900, epoch: 2, batch: 5549, loss: 3.91515, acc: 0.86259, time: Sat Jan 22 15:13:14 2022
epoch 2 training:   3%|▎         | 5648/196351 [48:08<26:40:12,  1.99it/s]global step 202000, epoch: 2, batch: 5649, loss: 2.61989, acc: 0.86396, time: Sat Jan 22 15:14:05 2022
epoch 2 training:   3%|▎         | 5748/196351 [48:59<26:52:23,  1.97it/s]global step 202100, epoch: 2, batch: 5749, loss: 2.37860, acc: 0.86467, time: Sat Jan 22 15:14:56 2022
epoch 2 training:   3%|▎         | 5848/196351 [49:50<27:13:51,  1.94it/s]global step 202200, epoch: 2, batch: 5849, loss: 1.68179, acc: 0.86545, time: Sat Jan 22 15:15:48 2022
epoch 2 training:   3%|▎         | 5948/196351 [50:41<27:23:08,  1.93it/s]global step 202300, epoch: 2, batch: 5949, loss: 1.35440, acc: 0.86603, time: Sat Jan 22 15:16:39 2022
epoch 2 training:   3%|▎         | 6048/196351 [51:33<27:10:40,  1.95it/s]global step 202400, epoch: 2, batch: 6049, loss: 3.30101, acc: 0.86634, time: Sat Jan 22 15:17:30 2022
epoch 2 training:   3%|▎         | 6148/196351 [52:24<27:01:18,  1.96it/s]global step 202500, epoch: 2, batch: 6149, loss: 4.56452, acc: 0.86640, time: Sat Jan 22 15:18:21 2022
epoch 2 training:   3%|▎         | 6248/196351 [53:15<26:43:29,  1.98it/s]global step 202600, epoch: 2, batch: 6249, loss: 1.39413, acc: 0.86742, time: Sat Jan 22 15:19:12 2022
epoch 2 training:   3%|▎         | 6348/196351 [54:05<26:38:49,  1.98it/s]global step 202700, epoch: 2, batch: 6349, loss: 2.38344, acc: 0.86785, time: Sat Jan 22 15:20:02 2022
epoch 2 training:   3%|▎         | 6448/196351 [54:56<26:48:47,  1.97it/s]global step 202800, epoch: 2, batch: 6449, loss: 1.15572, acc: 0.86905, time: Sat Jan 22 15:20:53 2022
epoch 2 training:   3%|▎         | 6548/196351 [55:47<26:43:53,  1.97it/s]global step 202900, epoch: 2, batch: 6549, loss: 4.17492, acc: 0.86945, time: Sat Jan 22 15:21:45 2022
epoch 2 training:   3%|▎         | 6648/196351 [56:38<26:42:01,  1.97it/s]global step 203000, epoch: 2, batch: 6649, loss: 1.25790, acc: 0.87006, time: Sat Jan 22 15:22:35 2022
epoch 2 training:   3%|▎         | 6748/196351 [57:28<26:49:35,  1.96it/s]global step 203100, epoch: 2, batch: 6749, loss: 2.78164, acc: 0.87050, time: Sat Jan 22 15:23:26 2022
epoch 2 training:   3%|▎         | 6848/196351 [58:19<26:35:29,  1.98it/s]global step 203200, epoch: 2, batch: 6849, loss: 2.92008, acc: 0.87064, time: Sat Jan 22 15:24:16 2022
epoch 2 training:   4%|▎         | 6948/196351 [59:10<26:59:52,  1.95it/s]global step 203300, epoch: 2, batch: 6949, loss: 0.36954, acc: 0.87135, time: Sat Jan 22 15:25:07 2022
epoch 2 training:   4%|▎         | 7048/196351 [1:00:00<26:48:24,  1.96it/s]global step 203400, epoch: 2, batch: 7049, loss: 1.63343, acc: 0.87239, time: Sat Jan 22 15:25:57 2022
epoch 2 training:   4%|▎         | 7148/196351 [1:00:50<26:25:38,  1.99it/s]global step 203500, epoch: 2, batch: 7149, loss: 0.82638, acc: 0.87264, time: Sat Jan 22 15:26:48 2022
epoch 2 training:   4%|▎         | 7248/196351 [1:01:41<26:28:41,  1.98it/s]global step 203600, epoch: 2, batch: 7249, loss: 2.56894, acc: 0.87281, time: Sat Jan 22 15:27:38 2022
epoch 2 training:   4%|▎         | 7348/196351 [1:02:31<26:46:32,  1.96it/s]global step 203700, epoch: 2, batch: 7349, loss: 4.41457, acc: 0.87325, time: Sat Jan 22 15:28:29 2022
epoch 2 training:   4%|▍         | 7448/196351 [1:03:22<26:37:48,  1.97it/s]global step 203800, epoch: 2, batch: 7449, loss: 1.24583, acc: 0.87354, time: Sat Jan 22 15:29:20 2022
epoch 2 training:   4%|▍         | 7548/196351 [1:04:13<27:20:34,  1.92it/s]global step 203900, epoch: 2, batch: 7549, loss: 1.61633, acc: 0.87409, time: Sat Jan 22 15:30:10 2022
epoch 2 training:   4%|▍         | 7648/196351 [1:05:04<26:27:21,  1.98it/s]global step 204000, epoch: 2, batch: 7649, loss: 2.47362, acc: 0.87502, time: Sat Jan 22 15:31:01 2022
epoch 2 training:   4%|▍         | 7748/196351 [1:05:54<26:22:36,  1.99it/s]global step 204100, epoch: 2, batch: 7749, loss: 4.61744, acc: 0.87527, time: Sat Jan 22 15:31:52 2022
epoch 2 training:   4%|▍         | 7848/196351 [1:06:45<26:29:28,  1.98it/s]global step 204200, epoch: 2, batch: 7849, loss: 4.01142, acc: 0.87540, time: Sat Jan 22 15:32:42 2022
epoch 2 training:   4%|▍         | 7948/196351 [1:07:36<26:20:46,  1.99it/s]global step 204300, epoch: 2, batch: 7949, loss: 4.17297, acc: 0.87577, time: Sat Jan 22 15:33:33 2022
epoch 2 training:   4%|▍         | 8048/196351 [1:08:26<26:25:43,  1.98it/s]global step 204400, epoch: 2, batch: 8049, loss: 0.91124, acc: 0.87651, time: Sat Jan 22 15:34:23 2022
epoch 2 training:   4%|▍         | 8148/196351 [1:09:17<26:40:09,  1.96it/s]global step 204500, epoch: 2, batch: 8149, loss: 2.25504, acc: 0.87704, time: Sat Jan 22 15:35:14 2022
epoch 2 training:   4%|▍         | 8248/196351 [1:10:08<26:48:56,  1.95it/s]global step 204600, epoch: 2, batch: 8249, loss: 1.91055, acc: 0.87805, time: Sat Jan 22 15:36:05 2022
epoch 2 training:   4%|▍         | 8348/196351 [1:11:00<26:17:16,  1.99it/s]global step 204700, epoch: 2, batch: 8349, loss: 2.35880, acc: 0.87855, time: Sat Jan 22 15:36:57 2022
epoch 2 training:   4%|▍         | 8448/196351 [1:11:51<26:46:50,  1.95it/s]global step 204800, epoch: 2, batch: 8449, loss: 2.34459, acc: 0.87904, time: Sat Jan 22 15:37:48 2022
epoch 2 training:   4%|▍         | 8548/196351 [1:12:42<26:39:33,  1.96it/s]global step 204900, epoch: 2, batch: 8549, loss: 1.57413, acc: 0.87958, time: Sat Jan 22 15:38:39 2022
epoch 2 training:   4%|▍         | 8648/196351 [1:13:33<26:22:46,  1.98it/s]global step 205000, epoch: 2, batch: 8649, loss: 2.25516, acc: 0.88016, time: Sat Jan 22 15:39:30 2022
epoch 2 training:   4%|▍         | 8748/196351 [1:14:24<26:38:19,  1.96it/s]global step 205100, epoch: 2, batch: 8749, loss: 1.92135, acc: 0.88039, time: Sat Jan 22 15:40:22 2022
epoch 2 training:   5%|▍         | 8848/196351 [1:15:15<26:37:05,  1.96it/s]global step 205200, epoch: 2, batch: 8849, loss: 4.47595, acc: 0.88066, time: Sat Jan 22 15:41:13 2022
epoch 2 training:   5%|▍         | 8948/196351 [1:16:07<26:27:50,  1.97it/s]global step 205300, epoch: 2, batch: 8949, loss: 2.02238, acc: 0.88144, time: Sat Jan 22 15:42:04 2022
epoch 2 training:   5%|▍         | 9048/196351 [1:16:58<26:27:15,  1.97it/s]global step 205400, epoch: 2, batch: 9049, loss: 2.46357, acc: 0.88187, time: Sat Jan 22 15:42:55 2022
epoch 2 training:   5%|▍         | 9148/196351 [1:17:49<26:26:57,  1.97it/s]global step 205500, epoch: 2, batch: 9149, loss: 3.22699, acc: 0.88212, time: Sat Jan 22 15:43:46 2022
epoch 2 training:   5%|▍         | 9248/196351 [1:18:40<26:46:12,  1.94it/s]global step 205600, epoch: 2, batch: 9249, loss: 2.42154, acc: 0.88253, time: Sat Jan 22 15:44:38 2022
epoch 2 training:   5%|▍         | 9348/196351 [1:19:31<25:58:27,  2.00it/s]global step 205700, epoch: 2, batch: 9349, loss: 2.63963, acc: 0.88304, time: Sat Jan 22 15:45:29 2022
epoch 2 training:   5%|▍         | 9448/196351 [1:20:23<26:39:18,  1.95it/s]global step 205800, epoch: 2, batch: 9449, loss: 1.79428, acc: 0.88322, time: Sat Jan 22 15:46:20 2022
epoch 2 training:   5%|▍         | 9548/196351 [1:21:14<26:20:11,  1.97it/s]global step 205900, epoch: 2, batch: 9549, loss: 0.71265, acc: 0.88339, time: Sat Jan 22 15:47:11 2022
epoch 2 training:   5%|▍         | 9648/196351 [1:22:05<26:20:45,  1.97it/s]global step 206000, epoch: 2, batch: 9649, loss: 0.88722, acc: 0.88387, time: Sat Jan 22 15:48:02 2022
epoch 2 training:   5%|▍         | 9748/196351 [1:22:55<26:29:51,  1.96it/s]global step 206100, epoch: 2, batch: 9749, loss: 1.75039, acc: 0.88445, time: Sat Jan 22 15:48:53 2022
epoch 2 training:   5%|▌         | 9848/196351 [1:23:46<26:26:12,  1.96it/s]global step 206200, epoch: 2, batch: 9849, loss: 1.78381, acc: 0.88491, time: Sat Jan 22 15:49:44 2022
epoch 2 training:   5%|▌         | 9948/196351 [1:24:38<26:40:34,  1.94it/s]global step 206300, epoch: 2, batch: 9949, loss: 0.74447, acc: 0.88552, time: Sat Jan 22 15:50:35 2022
epoch 2 training:   5%|▌         | 10048/196351 [1:25:29<26:17:17,  1.97it/s]global step 206400, epoch: 2, batch: 10049, loss: 1.55215, acc: 0.88596, time: Sat Jan 22 15:51:26 2022
epoch 2 training:   5%|▌         | 10148/196351 [1:26:20<26:37:47,  1.94it/s]global step 206500, epoch: 2, batch: 10149, loss: 0.88883, acc: 0.88649, time: Sat Jan 22 15:52:17 2022
epoch 2 training:   5%|▌         | 10248/196351 [1:27:11<26:03:51,  1.98it/s]global step 206600, epoch: 2, batch: 10249, loss: 0.56954, acc: 0.88677, time: Sat Jan 22 15:53:08 2022
epoch 2 training:   5%|▌         | 10348/196351 [1:28:02<30:01:33,  1.72it/s]global step 206700, epoch: 2, batch: 10349, loss: 1.48387, acc: 0.88695, time: Sat Jan 22 15:54:00 2022
epoch 2 training:   5%|▌         | 10448/196351 [1:28:53<26:27:55,  1.95it/s]global step 206800, epoch: 2, batch: 10449, loss: 1.51974, acc: 0.88731, time: Sat Jan 22 15:54:51 2022
epoch 2 training:   5%|▌         | 10548/196351 [1:29:44<26:10:18,  1.97it/s]global step 206900, epoch: 2, batch: 10549, loss: 1.83926, acc: 0.88752, time: Sat Jan 22 15:55:42 2022
epoch 2 training:   5%|▌         | 10648/196351 [1:30:35<26:06:35,  1.98it/s]global step 207000, epoch: 2, batch: 10649, loss: 2.27255, acc: 0.88736, time: Sat Jan 22 15:56:33 2022
epoch 2 training:   5%|▌         | 10748/196351 [1:31:26<26:26:14,  1.95it/s]global step 207100, epoch: 2, batch: 10749, loss: 1.01841, acc: 0.88780, time: Sat Jan 22 15:57:24 2022
epoch 2 training:   6%|▌         | 10848/196351 [1:32:18<26:09:17,  1.97it/s]global step 207200, epoch: 2, batch: 10849, loss: 1.19746, acc: 0.88815, time: Sat Jan 22 15:58:15 2022
epoch 2 training:   6%|▌         | 10948/196351 [1:33:09<26:08:48,  1.97it/s]global step 207300, epoch: 2, batch: 10949, loss: 4.40589, acc: 0.88848, time: Sat Jan 22 15:59:06 2022
epoch 2 training:   6%|▌         | 11048/196351 [1:34:00<26:33:27,  1.94it/s]global step 207400, epoch: 2, batch: 11049, loss: 3.53507, acc: 0.88868, time: Sat Jan 22 15:59:57 2022
epoch 2 training:   6%|▌         | 11148/196351 [1:34:51<26:18:38,  1.96it/s]global step 207500, epoch: 2, batch: 11149, loss: 2.53026, acc: 0.88900, time: Sat Jan 22 16:00:48 2022
epoch 2 training:   6%|▌         | 11248/196351 [1:35:42<25:56:05,  1.98it/s]global step 207600, epoch: 2, batch: 11249, loss: 2.89901, acc: 0.88946, time: Sat Jan 22 16:01:39 2022
epoch 2 training:   6%|▌         | 11348/196351 [1:36:32<25:57:25,  1.98it/s]global step 207700, epoch: 2, batch: 11349, loss: 0.90674, acc: 0.88951, time: Sat Jan 22 16:02:30 2022
epoch 2 training:   6%|▌         | 11448/196351 [1:37:23<26:38:00,  1.93it/s]global step 207800, epoch: 2, batch: 11449, loss: 1.97570, acc: 0.88986, time: Sat Jan 22 16:03:21 2022
epoch 2 training:   6%|▌         | 11548/196351 [1:38:14<26:15:40,  1.95it/s]global step 207900, epoch: 2, batch: 11549, loss: 1.50543, acc: 0.89008, time: Sat Jan 22 16:04:12 2022
epoch 2 training:   6%|▌         | 11648/196351 [1:39:05<25:49:35,  1.99it/s]global step 208000, epoch: 2, batch: 11649, loss: 1.86721, acc: 0.89042, time: Sat Jan 22 16:05:03 2022
epoch 2 training:   6%|▌         | 11748/196351 [1:39:56<26:35:04,  1.93it/s]global step 208100, epoch: 2, batch: 11749, loss: 1.61245, acc: 0.89080, time: Sat Jan 22 16:05:54 2022
epoch 2 training:   6%|▌         | 11848/196351 [1:40:47<25:56:13,  1.98it/s]global step 208200, epoch: 2, batch: 11849, loss: 0.71147, acc: 0.89088, time: Sat Jan 22 16:06:45 2022
epoch 2 training:   6%|▌         | 11948/196351 [1:41:38<26:02:39,  1.97it/s]global step 208300, epoch: 2, batch: 11949, loss: 1.57443, acc: 0.89116, time: Sat Jan 22 16:07:35 2022
epoch 2 training:   6%|▌         | 12048/196351 [1:42:29<26:24:38,  1.94it/s]global step 208400, epoch: 2, batch: 12049, loss: 1.15102, acc: 0.89136, time: Sat Jan 22 16:08:27 2022
epoch 2 training:   6%|▌         | 12148/196351 [1:43:20<25:54:24,  1.98it/s]global step 208500, epoch: 2, batch: 12149, loss: 0.57832, acc: 0.89176, time: Sat Jan 22 16:09:18 2022
epoch 2 training:   6%|▌         | 12248/196351 [1:44:12<26:09:55,  1.95it/s]global step 208600, epoch: 2, batch: 12249, loss: 1.97372, acc: 0.89183, time: Sat Jan 22 16:10:09 2022
epoch 2 training:   6%|▋         | 12348/196351 [1:45:03<26:09:55,  1.95it/s]global step 208700, epoch: 2, batch: 12349, loss: 1.68232, acc: 0.89206, time: Sat Jan 22 16:11:01 2022
epoch 2 training:   6%|▋         | 12448/196351 [1:45:54<26:18:25,  1.94it/s]global step 208800, epoch: 2, batch: 12449, loss: 1.36103, acc: 0.89228, time: Sat Jan 22 16:11:51 2022
epoch 2 training:   6%|▋         | 12548/196351 [1:46:45<26:06:45,  1.96it/s]global step 208900, epoch: 2, batch: 12549, loss: 1.47887, acc: 0.89238, time: Sat Jan 22 16:12:42 2022
epoch 2 training:   6%|▋         | 12648/196351 [1:47:36<25:45:49,  1.98it/s]global step 209000, epoch: 2, batch: 12649, loss: 1.36823, acc: 0.89280, time: Sat Jan 22 16:13:33 2022
epoch 2 training:   6%|▋         | 12748/196351 [1:48:27<25:59:34,  1.96it/s]global step 209100, epoch: 2, batch: 12749, loss: 2.49828, acc: 0.89321, time: Sat Jan 22 16:14:24 2022
epoch 2 training:   7%|▋         | 12848/196351 [1:49:17<26:10:39,  1.95it/s]global step 209200, epoch: 2, batch: 12849, loss: 0.47948, acc: 0.89342, time: Sat Jan 22 16:15:15 2022
epoch 2 training:   7%|▋         | 12948/196351 [1:50:08<25:46:26,  1.98it/s]global step 209300, epoch: 2, batch: 12949, loss: 2.22757, acc: 0.89358, time: Sat Jan 22 16:16:06 2022
epoch 2 training:   7%|▋         | 13048/196351 [1:50:59<25:59:02,  1.96it/s]global step 209400, epoch: 2, batch: 13049, loss: 0.39613, acc: 0.89382, time: Sat Jan 22 16:16:57 2022
epoch 2 training:   7%|▋         | 13148/196351 [1:51:50<25:59:49,  1.96it/s]global step 209500, epoch: 2, batch: 13149, loss: 4.70710, acc: 0.89398, time: Sat Jan 22 16:17:47 2022
epoch 2 training:   7%|▋         | 13248/196351 [1:52:42<25:49:49,  1.97it/s]global step 209600, epoch: 2, batch: 13249, loss: 2.46336, acc: 0.89411, time: Sat Jan 22 16:18:39 2022
epoch 2 training:   7%|▋         | 13348/196351 [1:53:32<25:48:59,  1.97it/s]global step 209700, epoch: 2, batch: 13349, loss: 0.46927, acc: 0.89449, time: Sat Jan 22 16:19:30 2022
epoch 2 training:   7%|▋         | 13448/196351 [1:54:23<26:47:21,  1.90it/s]global step 209800, epoch: 2, batch: 13449, loss: 0.94851, acc: 0.89456, time: Sat Jan 22 16:20:21 2022
epoch 2 training:   7%|▋         | 13548/196351 [1:55:14<25:59:48,  1.95it/s]global step 209900, epoch: 2, batch: 13549, loss: 0.37188, acc: 0.89475, time: Sat Jan 22 16:21:12 2022
epoch 2 training:   7%|▋         | 13648/196351 [1:56:06<25:46:25,  1.97it/s]global step 210000, epoch: 2, batch: 13649, loss: 2.78136, acc: 0.89512, time: Sat Jan 22 16:22:03 2022
epoch 2 training:   7%|▋         | 13748/196351 [1:56:57<25:52:55,  1.96it/s]global step 210100, epoch: 2, batch: 13749, loss: 3.08337, acc: 0.89530, time: Sat Jan 22 16:22:54 2022
epoch 2 training:   7%|▋         | 13848/196351 [1:57:48<25:42:37,  1.97it/s]global step 210200, epoch: 2, batch: 13849, loss: 1.70786, acc: 0.89537, time: Sat Jan 22 16:23:45 2022
epoch 2 training:   7%|▋         | 13948/196351 [1:58:39<25:39:37,  1.97it/s]global step 210300, epoch: 2, batch: 13949, loss: 1.09562, acc: 0.89573, time: Sat Jan 22 16:24:36 2022
epoch 2 training:   7%|▋         | 14048/196351 [1:59:30<26:11:01,  1.93it/s]global step 210400, epoch: 2, batch: 14049, loss: 0.40455, acc: 0.89579, time: Sat Jan 22 16:25:27 2022
epoch 2 training:   7%|▋         | 14148/196351 [2:00:21<26:03:08,  1.94it/s]global step 210500, epoch: 2, batch: 14149, loss: 1.36600, acc: 0.89593, time: Sat Jan 22 16:26:18 2022
epoch 2 training:   7%|▋         | 14248/196351 [2:01:12<25:37:51,  1.97it/s]global step 210600, epoch: 2, batch: 14249, loss: 0.76791, acc: 0.89603, time: Sat Jan 22 16:27:10 2022
epoch 2 training:   7%|▋         | 14348/196351 [2:02:04<26:35:39,  1.90it/s]global step 210700, epoch: 2, batch: 14349, loss: 3.27170, acc: 0.89616, time: Sat Jan 22 16:28:01 2022
epoch 2 training:   7%|▋         | 14448/196351 [2:02:55<25:43:31,  1.96it/s]global step 210800, epoch: 2, batch: 14449, loss: 1.96985, acc: 0.89643, time: Sat Jan 22 16:28:52 2022
epoch 2 training:   7%|▋         | 14548/196351 [2:03:45<25:39:58,  1.97it/s]global step 210900, epoch: 2, batch: 14549, loss: 0.17491, acc: 0.89666, time: Sat Jan 22 16:29:43 2022
epoch 2 training:   7%|▋         | 14648/196351 [2:04:36<25:23:12,  1.99it/s]global step 211000, epoch: 2, batch: 14649, loss: 2.63173, acc: 0.89689, time: Sat Jan 22 16:30:34 2022
epoch 2 training:   8%|▊         | 14748/196351 [2:05:27<25:26:15,  1.98it/s]global step 211100, epoch: 2, batch: 14749, loss: 0.81241, acc: 0.89711, time: Sat Jan 22 16:31:25 2022
epoch 2 training:   8%|▊         | 14848/196351 [2:06:18<25:36:37,  1.97it/s]global step 211200, epoch: 2, batch: 14849, loss: 0.44700, acc: 0.89743, time: Sat Jan 22 16:32:16 2022
epoch 2 training:   8%|▊         | 14948/196351 [2:07:09<25:26:21,  1.98it/s]global step 211300, epoch: 2, batch: 14949, loss: 1.16534, acc: 0.89755, time: Sat Jan 22 16:33:07 2022
epoch 2 training:   8%|▊         | 15048/196351 [2:08:00<25:46:48,  1.95it/s]global step 211400, epoch: 2, batch: 15049, loss: 0.68326, acc: 0.89790, time: Sat Jan 22 16:33:58 2022
epoch 2 training:   8%|▊         | 15148/196351 [2:08:51<25:21:06,  1.99it/s]global step 211500, epoch: 2, batch: 15149, loss: 4.77048, acc: 0.89815, time: Sat Jan 22 16:34:48 2022
epoch 2 training:   8%|▊         | 15248/196351 [2:09:42<25:28:22,  1.97it/s]global step 211600, epoch: 2, batch: 15249, loss: 0.99358, acc: 0.89855, time: Sat Jan 22 16:35:40 2022
epoch 2 training:   8%|▊         | 15348/196351 [2:10:33<25:38:56,  1.96it/s]global step 211700, epoch: 2, batch: 15349, loss: 2.04083, acc: 0.89885, time: Sat Jan 22 16:36:31 2022
epoch 2 training:   8%|▊         | 15448/196351 [2:11:24<25:38:27,  1.96it/s]global step 211800, epoch: 2, batch: 15449, loss: 2.57334, acc: 0.89893, time: Sat Jan 22 16:37:22 2022
epoch 2 training:   8%|▊         | 15548/196351 [2:12:15<25:23:46,  1.98it/s]global step 211900, epoch: 2, batch: 15549, loss: 1.11522, acc: 0.89900, time: Sat Jan 22 16:38:13 2022
epoch 2 training:   8%|▊         | 15648/196351 [2:13:06<25:52:47,  1.94it/s]global step 212000, epoch: 2, batch: 15649, loss: 0.77328, acc: 0.89919, time: Sat Jan 22 16:39:04 2022
epoch 2 training:   8%|▊         | 15748/196351 [2:13:57<25:35:28,  1.96it/s]global step 212100, epoch: 2, batch: 15749, loss: 3.09709, acc: 0.89939, time: Sat Jan 22 16:39:55 2022
epoch 2 training:   8%|▊         | 15848/196351 [2:14:48<25:51:14,  1.94it/s]global step 212200, epoch: 2, batch: 15849, loss: 2.54637, acc: 0.89936, time: Sat Jan 22 16:40:46 2022
epoch 2 training:   8%|▊         | 15948/196351 [2:15:39<25:35:30,  1.96it/s]global step 212300, epoch: 2, batch: 15949, loss: 2.34017, acc: 0.89946, time: Sat Jan 22 16:41:37 2022
epoch 2 training:   8%|▊         | 16048/196351 [2:16:31<25:27:49,  1.97it/s]global step 212400, epoch: 2, batch: 16049, loss: 1.67499, acc: 0.89965, time: Sat Jan 22 16:42:29 2022
epoch 2 training:   8%|▊         | 16148/196351 [2:17:22<25:08:13,  1.99it/s]global step 212500, epoch: 2, batch: 16149, loss: 0.71143, acc: 0.89981, time: Sat Jan 22 16:43:20 2022
epoch 2 training:   8%|▊         | 16248/196351 [2:18:13<25:54:26,  1.93it/s]global step 212600, epoch: 2, batch: 16249, loss: 2.40493, acc: 0.89987, time: Sat Jan 22 16:44:10 2022
epoch 2 training:   8%|▊         | 16348/196351 [2:19:04<25:37:17,  1.95it/s]global step 212700, epoch: 2, batch: 16349, loss: 2.61729, acc: 0.90002, time: Sat Jan 22 16:45:01 2022
epoch 2 training:   8%|▊         | 16448/196351 [2:19:55<25:02:29,  2.00it/s]global step 212800, epoch: 2, batch: 16449, loss: 0.21732, acc: 0.90024, time: Sat Jan 22 16:45:52 2022
epoch 2 training:   8%|▊         | 16548/196351 [2:20:46<25:04:37,  1.99it/s]global step 212900, epoch: 2, batch: 16549, loss: 4.91553, acc: 0.90036, time: Sat Jan 22 16:46:43 2022
epoch 2 training:   8%|▊         | 16648/196351 [2:21:37<25:22:11,  1.97it/s]global step 213000, epoch: 2, batch: 16649, loss: 0.37153, acc: 0.90062, time: Sat Jan 22 16:47:34 2022
epoch 2 training:   9%|▊         | 16748/196351 [2:22:28<25:16:26,  1.97it/s]global step 213100, epoch: 2, batch: 16749, loss: 0.58969, acc: 0.90074, time: Sat Jan 22 16:48:26 2022
epoch 2 training:   9%|▊         | 16848/196351 [2:23:19<25:36:36,  1.95it/s]global step 213200, epoch: 2, batch: 16849, loss: 0.87326, acc: 0.90103, time: Sat Jan 22 16:49:16 2022
epoch 2 training:   9%|▊         | 16948/196351 [2:24:10<25:16:43,  1.97it/s]global step 213300, epoch: 2, batch: 16949, loss: 0.61331, acc: 0.90126, time: Sat Jan 22 16:50:08 2022
epoch 2 training:   9%|▊         | 17048/196351 [2:25:02<25:34:09,  1.95it/s]global step 213400, epoch: 2, batch: 17049, loss: 2.17340, acc: 0.90143, time: Sat Jan 22 16:50:59 2022
epoch 2 training:   9%|▊         | 17148/196351 [2:25:53<25:19:36,  1.97it/s]global step 213500, epoch: 2, batch: 17149, loss: 1.91104, acc: 0.90160, time: Sat Jan 22 16:51:50 2022
epoch 2 training:   9%|▉         | 17248/196351 [2:26:44<25:21:49,  1.96it/s]global step 213600, epoch: 2, batch: 17249, loss: 1.66156, acc: 0.90176, time: Sat Jan 22 16:52:41 2022
epoch 2 training:   9%|▉         | 17348/196351 [2:27:35<24:59:45,  1.99it/s]global step 213700, epoch: 2, batch: 17349, loss: 2.39937, acc: 0.90190, time: Sat Jan 22 16:53:32 2022
epoch 2 training:   9%|▉         | 17448/196351 [2:28:26<25:27:23,  1.95it/s]global step 213800, epoch: 2, batch: 17449, loss: 3.67607, acc: 0.90206, time: Sat Jan 22 16:54:23 2022
epoch 2 training:   9%|▉         | 17548/196351 [2:29:16<25:02:56,  1.98it/s]global step 213900, epoch: 2, batch: 17549, loss: 2.03388, acc: 0.90227, time: Sat Jan 22 16:55:14 2022
epoch 2 training:   9%|▉         | 17648/196351 [2:30:07<25:13:39,  1.97it/s]global step 214000, epoch: 2, batch: 17649, loss: 2.71517, acc: 0.90226, time: Sat Jan 22 16:56:05 2022
epoch 2 training:   9%|▉         | 17748/196351 [2:30:58<25:17:43,  1.96it/s]global step 214100, epoch: 2, batch: 17749, loss: 0.45994, acc: 0.90233, time: Sat Jan 22 16:56:56 2022
epoch 2 training:   9%|▉         | 17848/196351 [2:31:49<25:06:14,  1.98it/s]global step 214200, epoch: 2, batch: 17849, loss: 0.93555, acc: 0.90240, time: Sat Jan 22 16:57:47 2022
epoch 2 training:   9%|▉         | 17948/196351 [2:32:40<24:53:26,  1.99it/s]global step 214300, epoch: 2, batch: 17949, loss: 1.89405, acc: 0.90253, time: Sat Jan 22 16:58:38 2022
epoch 2 training:   9%|▉         | 18048/196351 [2:33:31<25:05:53,  1.97it/s]global step 214400, epoch: 2, batch: 18049, loss: 3.70811, acc: 0.90252, time: Sat Jan 22 16:59:28 2022
epoch 2 training:   9%|▉         | 18148/196351 [2:34:22<24:42:45,  2.00it/s]global step 214500, epoch: 2, batch: 18149, loss: 3.39024, acc: 0.90253, time: Sat Jan 22 17:00:20 2022
epoch 2 training:   9%|▉         | 18248/196351 [2:35:13<25:33:58,  1.94it/s]global step 214600, epoch: 2, batch: 18249, loss: 2.03640, acc: 0.90284, time: Sat Jan 22 17:01:10 2022
epoch 2 training:   9%|▉         | 18348/196351 [2:36:04<25:13:04,  1.96it/s]global step 214700, epoch: 2, batch: 18349, loss: 2.17846, acc: 0.90296, time: Sat Jan 22 17:02:01 2022
epoch 2 training:   9%|▉         | 18448/196351 [2:36:55<25:35:51,  1.93it/s]global step 214800, epoch: 2, batch: 18449, loss: 1.13477, acc: 0.90292, time: Sat Jan 22 17:02:52 2022
epoch 2 training:   9%|▉         | 18548/196351 [2:37:46<25:00:07,  1.98it/s]global step 214900, epoch: 2, batch: 18549, loss: 3.22703, acc: 0.90320, time: Sat Jan 22 17:03:43 2022
epoch 2 training:   9%|▉         | 18648/196351 [2:38:37<25:02:24,  1.97it/s]global step 215000, epoch: 2, batch: 18649, loss: 0.28682, acc: 0.90340, time: Sat Jan 22 17:04:34 2022
epoch 2 training:  10%|▉         | 18748/196351 [2:39:27<25:09:04,  1.96it/s]global step 215100, epoch: 2, batch: 18749, loss: 1.90126, acc: 0.90367, time: Sat Jan 22 17:05:25 2022
epoch 2 training:  10%|▉         | 18848/196351 [2:40:18<25:07:12,  1.96it/s]global step 215200, epoch: 2, batch: 18849, loss: 3.32780, acc: 0.90379, time: Sat Jan 22 17:06:16 2022
epoch 2 training:  10%|▉         | 18948/196351 [2:41:09<24:53:35,  1.98it/s]global step 215300, epoch: 2, batch: 18949, loss: 1.97559, acc: 0.90382, time: Sat Jan 22 17:07:07 2022
epoch 2 training:  10%|▉         | 19048/196351 [2:42:00<25:32:41,  1.93it/s]global step 215400, epoch: 2, batch: 19049, loss: 4.09726, acc: 0.90391, time: Sat Jan 22 17:07:58 2022
epoch 2 training:  10%|▉         | 19148/196351 [2:42:51<24:47:17,  1.99it/s]global step 215500, epoch: 2, batch: 19149, loss: 0.89114, acc: 0.90378, time: Sat Jan 22 17:08:48 2022
epoch 2 training:  10%|▉         | 19248/196351 [2:43:42<25:32:14,  1.93it/s]global step 215600, epoch: 2, batch: 19249, loss: 0.91328, acc: 0.90387, time: Sat Jan 22 17:09:39 2022
epoch 2 training:  10%|▉         | 19348/196351 [2:44:33<25:00:58,  1.97it/s]global step 215700, epoch: 2, batch: 19349, loss: 2.03662, acc: 0.90397, time: Sat Jan 22 17:10:30 2022
epoch 2 training:  10%|▉         | 19448/196351 [2:45:24<24:52:46,  1.98it/s]global step 215800, epoch: 2, batch: 19449, loss: 0.87689, acc: 0.90393, time: Sat Jan 22 17:11:21 2022
epoch 2 training:  10%|▉         | 19548/196351 [2:46:15<24:44:07,  1.99it/s]global step 215900, epoch: 2, batch: 19549, loss: 4.56644, acc: 0.90414, time: Sat Jan 22 17:12:12 2022
epoch 2 training:  10%|█         | 19648/196351 [2:47:05<24:30:58,  2.00it/s]global step 216000, epoch: 2, batch: 19649, loss: 0.60010, acc: 0.90427, time: Sat Jan 22 17:13:03 2022
epoch 2 training:  10%|█         | 19748/196351 [2:47:56<24:38:08,  1.99it/s]global step 216100, epoch: 2, batch: 19749, loss: 0.53648, acc: 0.90443, time: Sat Jan 22 17:13:53 2022
epoch 2 training:  10%|█         | 19848/196351 [2:48:47<24:41:06,  1.99it/s]global step 216200, epoch: 2, batch: 19849, loss: 0.28696, acc: 0.90473, time: Sat Jan 22 17:14:44 2022
epoch 2 training:  10%|█         | 19948/196351 [2:49:38<24:59:42,  1.96it/s]global step 216300, epoch: 2, batch: 19949, loss: 0.81409, acc: 0.90491, time: Sat Jan 22 17:15:35 2022
epoch 2 training:  10%|█         | 20048/196351 [2:50:29<24:41:58,  1.98it/s]global step 216400, epoch: 2, batch: 20049, loss: 2.25274, acc: 0.90491, time: Sat Jan 22 17:16:26 2022
epoch 2 training:  10%|█         | 20148/196351 [2:51:19<25:02:09,  1.95it/s]global step 216500, epoch: 2, batch: 20149, loss: 3.62417, acc: 0.90498, time: Sat Jan 22 17:17:17 2022
epoch 2 training:  10%|█         | 20248/196351 [2:52:10<24:56:12,  1.96it/s]global step 216600, epoch: 2, batch: 20249, loss: 4.33578, acc: 0.90503, time: Sat Jan 22 17:18:08 2022
epoch 2 training:  10%|█         | 20348/196351 [2:53:01<25:02:03,  1.95it/s]global step 216700, epoch: 2, batch: 20349, loss: 1.25651, acc: 0.90508, time: Sat Jan 22 17:18:59 2022
epoch 2 training:  10%|█         | 20448/196351 [2:53:52<25:04:30,  1.95it/s]global step 216800, epoch: 2, batch: 20449, loss: 0.96428, acc: 0.90525, time: Sat Jan 22 17:19:50 2022
epoch 2 training:  10%|█         | 20548/196351 [2:54:43<25:06:38,  1.94it/s]global step 216900, epoch: 2, batch: 20549, loss: 2.25512, acc: 0.90530, time: Sat Jan 22 17:20:41 2022
epoch 2 training:  11%|█         | 20648/196351 [2:55:34<24:43:07,  1.97it/s]global step 217000, epoch: 2, batch: 20649, loss: 1.64576, acc: 0.90539, time: Sat Jan 22 17:21:32 2022
epoch 2 training:  11%|█         | 20748/196351 [2:56:25<24:31:44,  1.99it/s]global step 217100, epoch: 2, batch: 20749, loss: 0.68901, acc: 0.90547, time: Sat Jan 22 17:22:22 2022
epoch 2 training:  11%|█         | 20848/196351 [2:57:15<24:39:06,  1.98it/s]global step 217200, epoch: 2, batch: 20849, loss: 1.69788, acc: 0.90563, time: Sat Jan 22 17:23:13 2022
epoch 2 training:  11%|█         | 20948/196351 [2:58:06<24:44:47,  1.97it/s]global step 217300, epoch: 2, batch: 20949, loss: 1.39586, acc: 0.90577, time: Sat Jan 22 17:24:04 2022
epoch 2 training:  11%|█         | 21048/196351 [2:58:57<24:15:06,  2.01it/s]global step 217400, epoch: 2, batch: 21049, loss: 2.32114, acc: 0.90586, time: Sat Jan 22 17:24:54 2022
epoch 2 training:  11%|█         | 21148/196351 [2:59:48<24:37:53,  1.98it/s]global step 217500, epoch: 2, batch: 21149, loss: 2.13527, acc: 0.90602, time: Sat Jan 22 17:25:45 2022
epoch 2 training:  11%|█         | 21248/196351 [3:00:38<24:17:53,  2.00it/s]global step 217600, epoch: 2, batch: 21249, loss: 2.24564, acc: 0.90628, time: Sat Jan 22 17:26:36 2022
epoch 2 training:  11%|█         | 21348/196351 [3:01:29<24:46:07,  1.96it/s]global step 217700, epoch: 2, batch: 21349, loss: 3.11324, acc: 0.90639, time: Sat Jan 22 17:27:27 2022
epoch 2 training:  11%|█         | 21448/196351 [3:02:20<24:20:32,  2.00it/s]global step 217800, epoch: 2, batch: 21449, loss: 0.71795, acc: 0.90655, time: Sat Jan 22 17:28:17 2022
epoch 2 training:  11%|█         | 21548/196351 [3:03:11<24:44:09,  1.96it/s]global step 217900, epoch: 2, batch: 21549, loss: 0.94380, acc: 0.90661, time: Sat Jan 22 17:29:08 2022
epoch 2 training:  11%|█         | 21648/196351 [3:04:02<24:37:27,  1.97it/s]global step 218000, epoch: 2, batch: 21649, loss: 2.52120, acc: 0.90669, time: Sat Jan 22 17:29:59 2022
epoch 2 training:  11%|█         | 21748/196351 [3:04:53<24:41:31,  1.96it/s]global step 218100, epoch: 2, batch: 21749, loss: 0.41389, acc: 0.90692, time: Sat Jan 22 17:30:50 2022
epoch 2 training:  11%|█         | 21848/196351 [3:05:44<24:42:31,  1.96it/s]global step 218200, epoch: 2, batch: 21849, loss: 2.90927, acc: 0.90693, time: Sat Jan 22 17:31:41 2022
epoch 2 training:  11%|█         | 21948/196351 [3:06:35<25:06:31,  1.93it/s]global step 218300, epoch: 2, batch: 21949, loss: 0.80822, acc: 0.90703, time: Sat Jan 22 17:32:33 2022
epoch 2 training:  11%|█         | 22048/196351 [3:07:26<24:47:59,  1.95it/s]global step 218400, epoch: 2, batch: 22049, loss: 2.29924, acc: 0.90723, time: Sat Jan 22 17:33:24 2022
epoch 2 training:  11%|█▏        | 22148/196351 [3:08:17<24:29:45,  1.98it/s]global step 218500, epoch: 2, batch: 22149, loss: 2.25012, acc: 0.90724, time: Sat Jan 22 17:34:15 2022
epoch 2 training:  11%|█▏        | 22248/196351 [3:09:10<26:31:06,  1.82it/s]global step 218600, epoch: 2, batch: 22249, loss: 3.62339, acc: 0.90732, time: Sat Jan 22 17:35:08 2022
epoch 2 training:  11%|█▏        | 22348/196351 [3:10:07<27:14:23,  1.77it/s]global step 218700, epoch: 2, batch: 22349, loss: 1.71606, acc: 0.90742, time: Sat Jan 22 17:36:04 2022
epoch 2 training:  11%|█▏        | 22448/196351 [3:10:59<24:49:47,  1.95it/s]global step 218800, epoch: 2, batch: 22449, loss: 1.35057, acc: 0.90739, time: Sat Jan 22 17:36:57 2022
epoch 2 training:  11%|█▏        | 22548/196351 [3:11:51<24:38:40,  1.96it/s]global step 218900, epoch: 2, batch: 22549, loss: 1.94239, acc: 0.90749, time: Sat Jan 22 17:37:48 2022
epoch 2 training:  12%|█▏        | 22648/196351 [3:12:42<25:20:05,  1.90it/s]global step 219000, epoch: 2, batch: 22649, loss: 0.85708, acc: 0.90766, time: Sat Jan 22 17:38:39 2022
epoch 2 training:  12%|█▏        | 22748/196351 [3:13:33<24:56:40,  1.93it/s]global step 219100, epoch: 2, batch: 22749, loss: 1.23679, acc: 0.90775, time: Sat Jan 22 17:39:30 2022
epoch 2 training:  12%|█▏        | 22848/196351 [3:14:24<24:32:39,  1.96it/s]global step 219200, epoch: 2, batch: 22849, loss: 0.49297, acc: 0.90787, time: Sat Jan 22 17:40:22 2022
epoch 2 training:  12%|█▏        | 22948/196351 [3:15:15<24:19:28,  1.98it/s]global step 219300, epoch: 2, batch: 22949, loss: 1.37874, acc: 0.90790, time: Sat Jan 22 17:41:13 2022
epoch 2 training:  12%|█▏        | 23048/196351 [3:16:06<24:41:12,  1.95it/s]global step 219400, epoch: 2, batch: 23049, loss: 0.83651, acc: 0.90807, time: Sat Jan 22 17:42:04 2022
epoch 2 training:  12%|█▏        | 23148/196351 [3:16:57<24:44:20,  1.94it/s]global step 219500, epoch: 2, batch: 23149, loss: 2.83991, acc: 0.90803, time: Sat Jan 22 17:42:55 2022
epoch 2 training:  12%|█▏        | 23248/196351 [3:17:49<25:15:22,  1.90it/s]global step 219600, epoch: 2, batch: 23249, loss: 2.53425, acc: 0.90813, time: Sat Jan 22 17:43:46 2022
epoch 2 training:  12%|█▏        | 23348/196351 [3:18:40<24:25:56,  1.97it/s]global step 219700, epoch: 2, batch: 23349, loss: 0.38806, acc: 0.90830, time: Sat Jan 22 17:44:37 2022
epoch 2 training:  12%|█▏        | 23448/196351 [3:19:31<24:14:35,  1.98it/s]global step 219800, epoch: 2, batch: 23449, loss: 1.09812, acc: 0.90844, time: Sat Jan 22 17:45:28 2022
epoch 2 training:  12%|█▏        | 23548/196351 [3:20:22<24:33:31,  1.95it/s]global step 219900, epoch: 2, batch: 23549, loss: 2.35784, acc: 0.90855, time: Sat Jan 22 17:46:19 2022
epoch 2 training:  12%|█▏        | 23648/196351 [3:21:13<24:28:51,  1.96it/s]global step 220000, epoch: 2, batch: 23649, loss: 0.42152, acc: 0.90854, time: Sat Jan 22 17:47:10 2022
epoch 2 training:  12%|█▏        | 23748/196351 [3:22:04<24:29:50,  1.96it/s]global step 220100, epoch: 2, batch: 23749, loss: 1.12923, acc: 0.90856, time: Sat Jan 22 17:48:02 2022
epoch 2 training:  12%|█▏        | 23848/196351 [3:22:56<24:38:16,  1.94it/s]global step 220200, epoch: 2, batch: 23849, loss: 0.51418, acc: 0.90876, time: Sat Jan 22 17:48:53 2022
epoch 2 training:  12%|█▏        | 23948/196351 [3:23:47<24:39:52,  1.94it/s]global step 220300, epoch: 2, batch: 23949, loss: 0.67566, acc: 0.90879, time: Sat Jan 22 17:49:45 2022
epoch 2 training:  12%|█▏        | 24048/196351 [3:24:38<24:17:06,  1.97it/s]global step 220400, epoch: 2, batch: 24049, loss: 0.95940, acc: 0.90887, time: Sat Jan 22 17:50:36 2022
epoch 2 training:  12%|█▏        | 24148/196351 [3:25:29<24:56:05,  1.92it/s]global step 220500, epoch: 2, batch: 24149, loss: 0.48413, acc: 0.90898, time: Sat Jan 22 17:51:27 2022
epoch 2 training:  12%|█▏        | 24248/196351 [3:26:20<24:09:17,  1.98it/s]global step 220600, epoch: 2, batch: 24249, loss: 1.74795, acc: 0.90907, time: Sat Jan 22 17:52:18 2022
epoch 2 training:  12%|█▏        | 24348/196351 [3:27:11<24:20:37,  1.96it/s]global step 220700, epoch: 2, batch: 24349, loss: 1.48101, acc: 0.90915, time: Sat Jan 22 17:53:09 2022
epoch 2 training:  12%|█▏        | 24448/196351 [3:28:02<24:33:42,  1.94it/s]global step 220800, epoch: 2, batch: 24449, loss: 3.22269, acc: 0.90920, time: Sat Jan 22 17:54:00 2022
epoch 2 training:  13%|█▎        | 24548/196351 [3:28:54<23:57:43,  1.99it/s]global step 220900, epoch: 2, batch: 24549, loss: 0.42275, acc: 0.90920, time: Sat Jan 22 17:54:51 2022
epoch 2 training:  13%|█▎        | 24648/196351 [3:29:45<24:20:30,  1.96it/s]global step 221000, epoch: 2, batch: 24649, loss: 2.50384, acc: 0.90918, time: Sat Jan 22 17:55:42 2022
epoch 2 training:  13%|█▎        | 24748/196351 [3:30:36<24:20:55,  1.96it/s]global step 221100, epoch: 2, batch: 24749, loss: 2.70346, acc: 0.90935, time: Sat Jan 22 17:56:34 2022
epoch 2 training:  13%|█▎        | 24848/196351 [3:31:28<24:45:10,  1.92it/s]global step 221200, epoch: 2, batch: 24849, loss: 2.11573, acc: 0.90941, time: Sat Jan 22 17:57:25 2022
epoch 2 training:  13%|█▎        | 24948/196351 [3:32:19<24:20:46,  1.96it/s]global step 221300, epoch: 2, batch: 24949, loss: 0.52813, acc: 0.90952, time: Sat Jan 22 17:58:17 2022
epoch 2 training:  13%|█▎        | 25048/196351 [3:33:11<24:31:52,  1.94it/s]global step 221400, epoch: 2, batch: 25049, loss: 1.39856, acc: 0.90970, time: Sat Jan 22 17:59:08 2022
epoch 2 training:  13%|█▎        | 25148/196351 [3:34:02<24:06:28,  1.97it/s]global step 221500, epoch: 2, batch: 25149, loss: 0.62146, acc: 0.90974, time: Sat Jan 22 17:59:59 2022
epoch 2 training:  13%|█▎        | 25248/196351 [3:34:52<24:16:02,  1.96it/s]global step 221600, epoch: 2, batch: 25249, loss: 0.13152, acc: 0.90994, time: Sat Jan 22 18:00:50 2022
epoch 2 training:  13%|█▎        | 25348/196351 [3:35:43<23:59:53,  1.98it/s]global step 221700, epoch: 2, batch: 25349, loss: 1.03404, acc: 0.91004, time: Sat Jan 22 18:01:41 2022
epoch 2 training:  13%|█▎        | 25448/196351 [3:36:35<24:09:06,  1.97it/s]global step 221800, epoch: 2, batch: 25449, loss: 0.53123, acc: 0.91015, time: Sat Jan 22 18:02:32 2022
epoch 2 training:  13%|█▎        | 25548/196351 [3:37:26<24:00:12,  1.98it/s]global step 221900, epoch: 2, batch: 25549, loss: 0.44109, acc: 0.91017, time: Sat Jan 22 18:03:23 2022
epoch 2 training:  13%|█▎        | 25648/196351 [3:38:16<24:10:03,  1.96it/s]global step 222000, epoch: 2, batch: 25649, loss: 1.31531, acc: 0.91023, time: Sat Jan 22 18:04:14 2022
epoch 2 training:  13%|█▎        | 25748/196351 [3:39:07<23:58:29,  1.98it/s]global step 222100, epoch: 2, batch: 25749, loss: 3.76805, acc: 0.91015, time: Sat Jan 22 18:05:05 2022
epoch 2 training:  13%|█▎        | 25848/196351 [3:39:58<24:20:53,  1.95it/s]global step 222200, epoch: 2, batch: 25849, loss: 3.58785, acc: 0.91017, time: Sat Jan 22 18:05:56 2022
epoch 2 training:  13%|█▎        | 25948/196351 [3:40:49<23:57:51,  1.98it/s]global step 222300, epoch: 2, batch: 25949, loss: 0.83426, acc: 0.91023, time: Sat Jan 22 18:06:47 2022
epoch 2 training:  13%|█▎        | 26048/196351 [3:41:40<24:13:13,  1.95it/s]global step 222400, epoch: 2, batch: 26049, loss: 1.14768, acc: 0.91030, time: Sat Jan 22 18:07:38 2022
epoch 2 training:  13%|█▎        | 26148/196351 [3:42:31<24:02:47,  1.97it/s]global step 222500, epoch: 2, batch: 26149, loss: 0.63256, acc: 0.91046, time: Sat Jan 22 18:08:29 2022
epoch 2 training:  13%|█▎        | 26248/196351 [3:43:23<24:19:27,  1.94it/s]global step 222600, epoch: 2, batch: 26249, loss: 0.77723, acc: 0.91057, time: Sat Jan 22 18:09:20 2022
epoch 2 training:  13%|█▎        | 26348/196351 [3:44:14<24:08:45,  1.96it/s]global step 222700, epoch: 2, batch: 26349, loss: 1.99882, acc: 0.91064, time: Sat Jan 22 18:10:11 2022
epoch 2 training:  13%|█▎        | 26448/196351 [3:45:05<24:14:14,  1.95it/s]global step 222800, epoch: 2, batch: 26449, loss: 1.85160, acc: 0.91066, time: Sat Jan 22 18:11:02 2022
epoch 2 training:  14%|█▎        | 26548/196351 [3:45:56<24:22:52,  1.93it/s]global step 222900, epoch: 2, batch: 26549, loss: 3.34103, acc: 0.91069, time: Sat Jan 22 18:11:53 2022
epoch 2 training:  14%|█▎        | 26648/196351 [3:46:47<24:06:45,  1.95it/s]global step 223000, epoch: 2, batch: 26649, loss: 1.08795, acc: 0.91073, time: Sat Jan 22 18:12:45 2022
epoch 2 training:  14%|█▎        | 26748/196351 [3:47:39<24:12:24,  1.95it/s]global step 223100, epoch: 2, batch: 26749, loss: 2.78118, acc: 0.91084, time: Sat Jan 22 18:13:36 2022
epoch 2 training:  14%|█▎        | 26848/196351 [3:48:30<24:28:08,  1.92it/s]global step 223200, epoch: 2, batch: 26849, loss: 0.33459, acc: 0.91091, time: Sat Jan 22 18:14:27 2022
epoch 2 training:  14%|█▎        | 26948/196351 [3:49:21<24:03:58,  1.96it/s]global step 223300, epoch: 2, batch: 26949, loss: 2.04477, acc: 0.91091, time: Sat Jan 22 18:15:18 2022
epoch 2 training:  14%|█▍        | 27048/196351 [3:50:12<24:01:18,  1.96it/s]global step 223400, epoch: 2, batch: 27049, loss: 0.47401, acc: 0.91099, time: Sat Jan 22 18:16:09 2022
epoch 2 training:  14%|█▍        | 27148/196351 [3:51:03<26:14:18,  1.79it/s]global step 223500, epoch: 2, batch: 27149, loss: 0.30350, acc: 0.91106, time: Sat Jan 22 18:17:01 2022
epoch 2 training:  14%|█▍        | 27248/196351 [3:51:56<23:55:18,  1.96it/s]global step 223600, epoch: 2, batch: 27249, loss: 0.26796, acc: 0.91124, time: Sat Jan 22 18:17:53 2022
epoch 2 training:  14%|█▍        | 27348/196351 [3:52:47<23:59:10,  1.96it/s]global step 223700, epoch: 2, batch: 27349, loss: 1.92056, acc: 0.91133, time: Sat Jan 22 18:18:45 2022
epoch 2 training:  14%|█▍        | 27448/196351 [3:53:40<24:56:49,  1.88it/s]global step 223800, epoch: 2, batch: 27449, loss: 3.67680, acc: 0.91147, time: Sat Jan 22 18:19:38 2022
epoch 2 training:  14%|█▍        | 27548/196351 [3:54:32<23:50:05,  1.97it/s]global step 223900, epoch: 2, batch: 27549, loss: 0.41347, acc: 0.91167, time: Sat Jan 22 18:20:29 2022
epoch 2 training:  14%|█▍        | 27648/196351 [3:55:24<23:56:01,  1.96it/s]global step 224000, epoch: 2, batch: 27649, loss: 1.01495, acc: 0.91179, time: Sat Jan 22 18:21:21 2022
epoch 2 training:  14%|█▍        | 27748/196351 [3:56:15<24:27:50,  1.91it/s]global step 224100, epoch: 2, batch: 27749, loss: 2.60698, acc: 0.91192, time: Sat Jan 22 18:22:13 2022
epoch 2 training:  14%|█▍        | 27848/196351 [3:57:07<24:04:20,  1.94it/s]global step 224200, epoch: 2, batch: 27849, loss: 3.64002, acc: 0.91201, time: Sat Jan 22 18:23:04 2022
epoch 2 training:  14%|█▍        | 27948/196351 [3:57:58<24:03:49,  1.94it/s]global step 224300, epoch: 2, batch: 27949, loss: 2.54738, acc: 0.91205, time: Sat Jan 22 18:23:56 2022
epoch 2 training:  14%|█▍        | 28048/196351 [3:58:49<24:04:47,  1.94it/s]global step 224400, epoch: 2, batch: 28049, loss: 3.23800, acc: 0.91214, time: Sat Jan 22 18:24:47 2022
epoch 2 training:  14%|█▍        | 28148/196351 [3:59:41<24:11:40,  1.93it/s]global step 224500, epoch: 2, batch: 28149, loss: 2.77788, acc: 0.91216, time: Sat Jan 22 18:25:38 2022
epoch 2 training:  14%|█▍        | 28248/196351 [4:00:32<24:01:05,  1.94it/s]global step 224600, epoch: 2, batch: 28249, loss: 1.98430, acc: 0.91224, time: Sat Jan 22 18:26:29 2022
epoch 2 training:  14%|█▍        | 28348/196351 [4:01:23<23:56:18,  1.95it/s]global step 224700, epoch: 2, batch: 28349, loss: 0.88071, acc: 0.91227, time: Sat Jan 22 18:27:20 2022
epoch 2 training:  14%|█▍        | 28448/196351 [4:02:14<23:53:57,  1.95it/s]global step 224800, epoch: 2, batch: 28449, loss: 0.52315, acc: 0.91221, time: Sat Jan 22 18:28:12 2022
epoch 2 training:  15%|█▍        | 28548/196351 [4:03:06<23:46:02,  1.96it/s]global step 224900, epoch: 2, batch: 28549, loss: 1.10923, acc: 0.91236, time: Sat Jan 22 18:29:03 2022
epoch 2 training:  15%|█▍        | 28648/196351 [4:03:57<24:06:29,  1.93it/s]global step 225000, epoch: 2, batch: 28649, loss: 5.80048, acc: 0.91237, time: Sat Jan 22 18:29:55 2022
epoch 2 training:  15%|█▍        | 28748/196351 [4:04:49<23:43:17,  1.96it/s]global step 225100, epoch: 2, batch: 28749, loss: 1.30491, acc: 0.91245, time: Sat Jan 22 18:30:47 2022
epoch 2 training:  15%|█▍        | 28848/196351 [4:05:40<23:39:58,  1.97it/s]global step 225200, epoch: 2, batch: 28849, loss: 0.95888, acc: 0.91237, time: Sat Jan 22 18:31:38 2022
epoch 2 training:  15%|█▍        | 28948/196351 [4:06:31<24:46:27,  1.88it/s]global step 225300, epoch: 2, batch: 28949, loss: 1.62343, acc: 0.91250, time: Sat Jan 22 18:32:29 2022
epoch 2 training:  15%|█▍        | 29048/196351 [4:07:23<25:50:22,  1.80it/s]global step 225400, epoch: 2, batch: 29049, loss: 0.54354, acc: 0.91253, time: Sat Jan 22 18:33:20 2022
epoch 2 training:  15%|█▍        | 29148/196351 [4:08:20<26:02:15,  1.78it/s]global step 225500, epoch: 2, batch: 29149, loss: 0.56335, acc: 0.91259, time: Sat Jan 22 18:34:17 2022
epoch 2 training:  15%|█▍        | 29248/196351 [4:09:11<23:43:51,  1.96it/s]global step 225600, epoch: 2, batch: 29249, loss: 1.87532, acc: 0.91266, time: Sat Jan 22 18:35:09 2022
epoch 2 training:  15%|█▍        | 29348/196351 [4:10:02<23:40:35,  1.96it/s]global step 225700, epoch: 2, batch: 29349, loss: 3.53913, acc: 0.91276, time: Sat Jan 22 18:36:00 2022
epoch 2 training:  15%|█▍        | 29448/196351 [4:10:54<23:42:55,  1.95it/s]global step 225800, epoch: 2, batch: 29449, loss: 4.19071, acc: 0.91285, time: Sat Jan 22 18:36:51 2022
epoch 2 training:  15%|█▌        | 29548/196351 [4:11:45<23:48:32,  1.95it/s]global step 225900, epoch: 2, batch: 29549, loss: 0.38904, acc: 0.91301, time: Sat Jan 22 18:37:42 2022
epoch 2 training:  15%|█▌        | 29648/196351 [4:12:36<24:11:07,  1.91it/s]global step 226000, epoch: 2, batch: 29649, loss: 0.89443, acc: 0.91307, time: Sat Jan 22 18:38:33 2022
epoch 2 training:  15%|█▌        | 29748/196351 [4:13:27<23:28:10,  1.97it/s]global step 226100, epoch: 2, batch: 29749, loss: 0.35535, acc: 0.91322, time: Sat Jan 22 18:39:25 2022
epoch 2 training:  15%|█▌        | 29848/196351 [4:14:19<23:56:41,  1.93it/s]global step 226200, epoch: 2, batch: 29849, loss: 1.08829, acc: 0.91335, time: Sat Jan 22 18:40:16 2022
epoch 2 training:  15%|█▌        | 29948/196351 [4:15:10<23:14:20,  1.99it/s]global step 226300, epoch: 2, batch: 29949, loss: 1.76508, acc: 0.91342, time: Sat Jan 22 18:41:07 2022
epoch 2 training:  15%|█▌        | 30048/196351 [4:16:01<23:42:40,  1.95it/s]global step 226400, epoch: 2, batch: 30049, loss: 5.18242, acc: 0.91327, time: Sat Jan 22 18:41:58 2022
epoch 2 training:  15%|█▌        | 30148/196351 [4:16:52<23:34:48,  1.96it/s]global step 226500, epoch: 2, batch: 30149, loss: 3.63152, acc: 0.91331, time: Sat Jan 22 18:42:49 2022
epoch 2 training:  15%|█▌        | 30248/196351 [4:17:43<23:24:39,  1.97it/s]global step 226600, epoch: 2, batch: 30249, loss: 1.50494, acc: 0.91337, time: Sat Jan 22 18:43:40 2022
epoch 2 training:  15%|█▌        | 30348/196351 [4:18:34<23:32:52,  1.96it/s]global step 226700, epoch: 2, batch: 30349, loss: 1.01767, acc: 0.91341, time: Sat Jan 22 18:44:31 2022
epoch 2 training:  16%|█▌        | 30448/196351 [4:19:25<23:22:02,  1.97it/s]global step 226800, epoch: 2, batch: 30449, loss: 3.27882, acc: 0.91338, time: Sat Jan 22 18:45:22 2022
epoch 2 training:  16%|█▌        | 30548/196351 [4:20:16<23:32:55,  1.96it/s]global step 226900, epoch: 2, batch: 30549, loss: 3.49890, acc: 0.91342, time: Sat Jan 22 18:46:13 2022
epoch 2 training:  16%|█▌        | 30648/196351 [4:21:07<23:15:40,  1.98it/s]global step 227000, epoch: 2, batch: 30649, loss: 1.83637, acc: 0.91336, time: Sat Jan 22 18:47:04 2022
epoch 2 training:  16%|█▌        | 30748/196351 [4:21:58<23:32:19,  1.95it/s]global step 227100, epoch: 2, batch: 30749, loss: 1.44051, acc: 0.91340, time: Sat Jan 22 18:47:55 2022
epoch 2 training:  16%|█▌        | 30848/196351 [4:22:49<23:19:20,  1.97it/s]global step 227200, epoch: 2, batch: 30849, loss: 2.09341, acc: 0.91347, time: Sat Jan 22 18:48:46 2022
epoch 2 training:  16%|█▌        | 30948/196351 [4:23:40<23:37:14,  1.95it/s]global step 227300, epoch: 2, batch: 30949, loss: 0.86276, acc: 0.91358, time: Sat Jan 22 18:49:37 2022
epoch 2 training:  16%|█▌        | 31048/196351 [4:24:30<23:15:54,  1.97it/s]global step 227400, epoch: 2, batch: 31049, loss: 1.34496, acc: 0.91364, time: Sat Jan 22 18:50:28 2022
epoch 2 training:  16%|█▌        | 31148/196351 [4:25:21<23:01:33,  1.99it/s]global step 227500, epoch: 2, batch: 31149, loss: 2.75292, acc: 0.91362, time: Sat Jan 22 18:51:19 2022
epoch 2 training:  16%|█▌        | 31248/196351 [4:26:12<23:15:19,  1.97it/s]global step 227600, epoch: 2, batch: 31249, loss: 0.40980, acc: 0.91361, time: Sat Jan 22 18:52:09 2022
epoch 2 training:  16%|█▌        | 31348/196351 [4:27:03<23:15:51,  1.97it/s]global step 227700, epoch: 2, batch: 31349, loss: 4.51708, acc: 0.91363, time: Sat Jan 22 18:53:00 2022
epoch 2 training:  16%|█▌        | 31448/196351 [4:27:54<23:06:51,  1.98it/s]global step 227800, epoch: 2, batch: 31449, loss: 2.11610, acc: 0.91375, time: Sat Jan 22 18:53:51 2022
epoch 2 training:  16%|█▌        | 31548/196351 [4:28:45<23:24:39,  1.96it/s]global step 227900, epoch: 2, batch: 31549, loss: 1.84833, acc: 0.91378, time: Sat Jan 22 18:54:43 2022
epoch 2 training:  16%|█▌        | 31648/196351 [4:29:36<23:29:20,  1.95it/s]global step 228000, epoch: 2, batch: 31649, loss: 2.31905, acc: 0.91393, time: Sat Jan 22 18:55:33 2022
epoch 2 training:  16%|█▌        | 31748/196351 [4:30:27<23:24:37,  1.95it/s]global step 228100, epoch: 2, batch: 31749, loss: 3.44574, acc: 0.91397, time: Sat Jan 22 18:56:25 2022
epoch 2 training:  16%|█▌        | 31848/196351 [4:31:18<23:14:58,  1.97it/s]global step 228200, epoch: 2, batch: 31849, loss: 3.07296, acc: 0.91391, time: Sat Jan 22 18:57:15 2022
epoch 2 training:  16%|█▋        | 31948/196351 [4:32:09<23:09:12,  1.97it/s]global step 228300, epoch: 2, batch: 31949, loss: 0.79153, acc: 0.91402, time: Sat Jan 22 18:58:06 2022
epoch 2 training:  16%|█▋        | 32048/196351 [4:33:00<23:14:31,  1.96it/s]global step 228400, epoch: 2, batch: 32049, loss: 4.26672, acc: 0.91404, time: Sat Jan 22 18:58:57 2022
epoch 2 training:  16%|█▋        | 32148/196351 [4:33:51<23:30:41,  1.94it/s]global step 228500, epoch: 2, batch: 32149, loss: 1.17230, acc: 0.91410, time: Sat Jan 22 18:59:48 2022
epoch 2 training:  16%|█▋        | 32248/196351 [4:34:42<23:11:53,  1.96it/s]global step 228600, epoch: 2, batch: 32249, loss: 1.63891, acc: 0.91411, time: Sat Jan 22 19:00:39 2022
epoch 2 training:  16%|█▋        | 32348/196351 [4:35:33<23:34:07,  1.93it/s]global step 228700, epoch: 2, batch: 32349, loss: 3.43730, acc: 0.91417, time: Sat Jan 22 19:01:30 2022
epoch 2 training:  17%|█▋        | 32448/196351 [4:36:23<23:04:29,  1.97it/s]global step 228800, epoch: 2, batch: 32449, loss: 0.99626, acc: 0.91423, time: Sat Jan 22 19:02:21 2022
epoch 2 training:  17%|█▋        | 32548/196351 [4:37:15<27:52:51,  1.63it/s]global step 228900, epoch: 2, batch: 32549, loss: 2.53473, acc: 0.91431, time: Sat Jan 22 19:03:12 2022
epoch 2 training:  17%|█▋        | 32648/196351 [4:38:05<23:13:04,  1.96it/s]global step 229000, epoch: 2, batch: 32649, loss: 2.49392, acc: 0.91432, time: Sat Jan 22 19:04:03 2022
epoch 2 training:  17%|█▋        | 32748/196351 [4:38:56<23:04:58,  1.97it/s]global step 229100, epoch: 2, batch: 32749, loss: 1.99458, acc: 0.91439, time: Sat Jan 22 19:04:54 2022
epoch 2 training:  17%|█▋        | 32848/196351 [4:39:47<23:10:43,  1.96it/s]global step 229200, epoch: 2, batch: 32849, loss: 2.30643, acc: 0.91435, time: Sat Jan 22 19:05:45 2022
epoch 2 training:  17%|█▋        | 32948/196351 [4:40:39<23:15:19,  1.95it/s]global step 229300, epoch: 2, batch: 32949, loss: 0.25727, acc: 0.91444, time: Sat Jan 22 19:06:36 2022
epoch 2 training:  17%|█▋        | 33048/196351 [4:41:30<23:15:49,  1.95it/s]global step 229400, epoch: 2, batch: 33049, loss: 2.71512, acc: 0.91440, time: Sat Jan 22 19:07:27 2022
epoch 2 training:  17%|█▋        | 33148/196351 [4:42:20<23:17:00,  1.95it/s]global step 229500, epoch: 2, batch: 33149, loss: 5.79436, acc: 0.91437, time: Sat Jan 22 19:08:18 2022
epoch 2 training:  17%|█▋        | 33248/196351 [4:43:11<23:04:04,  1.96it/s]global step 229600, epoch: 2, batch: 33249, loss: 1.66608, acc: 0.91440, time: Sat Jan 22 19:09:08 2022
epoch 2 training:  17%|█▋        | 33348/196351 [4:44:02<23:03:23,  1.96it/s]global step 229700, epoch: 2, batch: 33349, loss: 0.95760, acc: 0.91436, time: Sat Jan 22 19:09:59 2022
epoch 2 training:  17%|█▋        | 33448/196351 [4:44:53<22:55:10,  1.97it/s]global step 229800, epoch: 2, batch: 33449, loss: 1.21268, acc: 0.91444, time: Sat Jan 22 19:10:50 2022
epoch 2 training:  17%|█▋        | 33548/196351 [4:45:44<23:34:14,  1.92it/s]global step 229900, epoch: 2, batch: 33549, loss: 0.78201, acc: 0.91445, time: Sat Jan 22 19:11:41 2022
epoch 2 training:  17%|█▋        | 33648/196351 [4:46:35<23:05:17,  1.96it/s]global step 230000, epoch: 2, batch: 33649, loss: 0.40105, acc: 0.91454, time: Sat Jan 22 19:12:32 2022
epoch 2 training:  17%|█▋        | 33748/196351 [4:47:26<23:03:04,  1.96it/s]global step 230100, epoch: 2, batch: 33749, loss: 2.39712, acc: 0.91462, time: Sat Jan 22 19:13:24 2022
epoch 2 training:  17%|█▋        | 33848/196351 [4:48:17<22:54:21,  1.97it/s]global step 230200, epoch: 2, batch: 33849, loss: 2.29100, acc: 0.91469, time: Sat Jan 22 19:14:15 2022
epoch 2 training:  17%|█▋        | 33948/196351 [4:49:08<23:00:45,  1.96it/s]global step 230300, epoch: 2, batch: 33949, loss: 0.99673, acc: 0.91475, time: Sat Jan 22 19:15:06 2022
epoch 2 training:  17%|█▋        | 34048/196351 [4:50:00<23:00:06,  1.96it/s]global step 230400, epoch: 2, batch: 34049, loss: 2.23247, acc: 0.91473, time: Sat Jan 22 19:15:57 2022
epoch 2 training:  17%|█▋        | 34148/196351 [4:50:51<23:08:12,  1.95it/s]global step 230500, epoch: 2, batch: 34149, loss: 1.47113, acc: 0.91476, time: Sat Jan 22 19:16:48 2022
epoch 2 training:  17%|█▋        | 34248/196351 [4:51:42<23:05:39,  1.95it/s]global step 230600, epoch: 2, batch: 34249, loss: 0.29977, acc: 0.91476, time: Sat Jan 22 19:17:40 2022
epoch 2 training:  17%|█▋        | 34348/196351 [4:52:33<23:00:19,  1.96it/s]global step 230700, epoch: 2, batch: 34349, loss: 2.35283, acc: 0.91489, time: Sat Jan 22 19:18:31 2022
epoch 2 training:  18%|█▊        | 34448/196351 [4:53:25<23:00:28,  1.95it/s]global step 230800, epoch: 2, batch: 34449, loss: 0.49791, acc: 0.91496, time: Sat Jan 22 19:19:22 2022
epoch 2 training:  18%|█▊        | 34548/196351 [4:54:16<22:49:59,  1.97it/s]global step 230900, epoch: 2, batch: 34549, loss: 1.55882, acc: 0.91506, time: Sat Jan 22 19:20:14 2022
epoch 2 training:  18%|█▊        | 34648/196351 [4:55:07<23:06:58,  1.94it/s]global step 231000, epoch: 2, batch: 34649, loss: 2.73112, acc: 0.91509, time: Sat Jan 22 19:21:04 2022
epoch 2 training:  18%|█▊        | 34748/196351 [4:55:58<22:41:03,  1.98it/s]global step 231100, epoch: 2, batch: 34749, loss: 2.43362, acc: 0.91518, time: Sat Jan 22 19:21:55 2022
epoch 2 training:  18%|█▊        | 34848/196351 [4:56:49<22:43:15,  1.97it/s]global step 231200, epoch: 2, batch: 34849, loss: 3.33150, acc: 0.91521, time: Sat Jan 22 19:22:46 2022
epoch 2 training:  18%|█▊        | 34948/196351 [4:57:40<23:04:38,  1.94it/s]global step 231300, epoch: 2, batch: 34949, loss: 2.41441, acc: 0.91522, time: Sat Jan 22 19:23:37 2022
epoch 2 training:  18%|█▊        | 35048/196351 [4:58:31<22:55:06,  1.96it/s]global step 231400, epoch: 2, batch: 35049, loss: 2.46210, acc: 0.91518, time: Sat Jan 22 19:24:28 2022
epoch 2 training:  18%|█▊        | 35148/196351 [4:59:21<22:48:27,  1.96it/s]global step 231500, epoch: 2, batch: 35149, loss: 1.82090, acc: 0.91520, time: Sat Jan 22 19:25:19 2022
epoch 2 training:  18%|█▊        | 35248/196351 [5:00:12<22:36:38,  1.98it/s]global step 231600, epoch: 2, batch: 35249, loss: 3.30941, acc: 0.91509, time: Sat Jan 22 19:26:10 2022
epoch 2 training:  18%|█▊        | 35348/196351 [5:01:04<22:36:40,  1.98it/s]global step 231700, epoch: 2, batch: 35349, loss: 1.50022, acc: 0.91503, time: Sat Jan 22 19:27:01 2022
epoch 2 training:  18%|█▊        | 35448/196351 [5:01:55<22:45:27,  1.96it/s]global step 231800, epoch: 2, batch: 35449, loss: 1.30263, acc: 0.91495, time: Sat Jan 22 19:27:52 2022
epoch 2 training:  18%|█▊        | 35548/196351 [5:02:46<22:33:08,  1.98it/s]global step 231900, epoch: 2, batch: 35549, loss: 0.58428, acc: 0.91502, time: Sat Jan 22 19:28:43 2022
epoch 2 training:  18%|█▊        | 35648/196351 [5:03:37<22:41:15,  1.97it/s]global step 232000, epoch: 2, batch: 35649, loss: 1.75456, acc: 0.91502, time: Sat Jan 22 19:29:34 2022
epoch 2 training:  18%|█▊        | 35748/196351 [5:04:28<22:55:03,  1.95it/s]global step 232100, epoch: 2, batch: 35749, loss: 3.53618, acc: 0.91510, time: Sat Jan 22 19:30:25 2022
epoch 2 training:  18%|█▊        | 35848/196351 [5:05:19<22:29:35,  1.98it/s]global step 232200, epoch: 2, batch: 35849, loss: 0.56821, acc: 0.91514, time: Sat Jan 22 19:31:16 2022
epoch 2 training:  18%|█▊        | 35948/196351 [5:06:10<24:31:16,  1.82it/s]global step 232300, epoch: 2, batch: 35949, loss: 1.78233, acc: 0.91525, time: Sat Jan 22 19:32:08 2022
epoch 2 training:  18%|█▊        | 36048/196351 [5:07:02<22:50:26,  1.95it/s]global step 232400, epoch: 2, batch: 36049, loss: 0.60393, acc: 0.91535, time: Sat Jan 22 19:33:00 2022
epoch 2 training:  18%|█▊        | 36148/196351 [5:07:56<24:59:05,  1.78it/s]global step 232500, epoch: 2, batch: 36149, loss: 2.12611, acc: 0.91534, time: Sat Jan 22 19:33:54 2022
epoch 2 training:  18%|█▊        | 36248/196351 [5:08:47<22:28:01,  1.98it/s]global step 232600, epoch: 2, batch: 36249, loss: 1.83572, acc: 0.91543, time: Sat Jan 22 19:34:45 2022
epoch 2 training:  19%|█▊        | 36348/196351 [5:09:38<22:39:49,  1.96it/s]global step 232700, epoch: 2, batch: 36349, loss: 1.43280, acc: 0.91555, time: Sat Jan 22 19:35:36 2022
epoch 2 training:  19%|█▊        | 36448/196351 [5:10:29<22:39:29,  1.96it/s]global step 232800, epoch: 2, batch: 36449, loss: 2.19332, acc: 0.91568, time: Sat Jan 22 19:36:27 2022
epoch 2 training:  19%|█▊        | 36548/196351 [5:11:20<22:25:46,  1.98it/s]global step 232900, epoch: 2, batch: 36549, loss: 3.17738, acc: 0.91566, time: Sat Jan 22 19:37:18 2022
epoch 2 training:  19%|█▊        | 36648/196351 [5:12:11<22:28:27,  1.97it/s]global step 233000, epoch: 2, batch: 36649, loss: 0.75870, acc: 0.91575, time: Sat Jan 22 19:38:09 2022
epoch 2 training:  19%|█▊        | 36748/196351 [5:13:02<22:24:50,  1.98it/s]global step 233100, epoch: 2, batch: 36749, loss: 0.81344, acc: 0.91579, time: Sat Jan 22 19:39:00 2022
epoch 2 training:  19%|█▉        | 36848/196351 [5:13:53<22:37:07,  1.96it/s]global step 233200, epoch: 2, batch: 36849, loss: 1.19526, acc: 0.91581, time: Sat Jan 22 19:39:50 2022
epoch 2 training:  19%|█▉        | 36948/196351 [5:14:44<22:46:02,  1.94it/s]global step 233300, epoch: 2, batch: 36949, loss: 0.79310, acc: 0.91582, time: Sat Jan 22 19:40:41 2022
epoch 2 training:  19%|█▉        | 37048/196351 [5:15:35<22:41:15,  1.95it/s]global step 233400, epoch: 2, batch: 37049, loss: 3.24188, acc: 0.91576, time: Sat Jan 22 19:41:32 2022
epoch 2 training:  19%|█▉        | 37148/196351 [5:16:26<22:36:36,  1.96it/s]global step 233500, epoch: 2, batch: 37149, loss: 0.15538, acc: 0.91585, time: Sat Jan 22 19:42:23 2022
epoch 2 training:  19%|█▉        | 37248/196351 [5:17:17<22:18:02,  1.98it/s]global step 233600, epoch: 2, batch: 37249, loss: 0.97634, acc: 0.91580, time: Sat Jan 22 19:43:14 2022
epoch 2 training:  19%|█▉        | 37348/196351 [5:18:08<22:20:07,  1.98it/s]global step 233700, epoch: 2, batch: 37349, loss: 1.05307, acc: 0.91575, time: Sat Jan 22 19:44:05 2022
epoch 2 training:  19%|█▉        | 37448/196351 [5:18:59<22:49:24,  1.93it/s]global step 233800, epoch: 2, batch: 37449, loss: 0.42638, acc: 0.91574, time: Sat Jan 22 19:44:57 2022
epoch 2 training:  19%|█▉        | 37548/196351 [5:19:50<22:27:06,  1.96it/s]global step 233900, epoch: 2, batch: 37549, loss: 2.24041, acc: 0.91576, time: Sat Jan 22 19:45:47 2022
epoch 2 training:  19%|█▉        | 37648/196351 [5:20:41<22:45:08,  1.94it/s]global step 234000, epoch: 2, batch: 37649, loss: 1.38937, acc: 0.91583, time: Sat Jan 22 19:46:39 2022
epoch 2 training:  19%|█▉        | 37748/196351 [5:21:32<22:52:25,  1.93it/s]global step 234100, epoch: 2, batch: 37749, loss: 1.70120, acc: 0.91596, time: Sat Jan 22 19:47:30 2022
epoch 2 training:  19%|█▉        | 37848/196351 [5:22:23<22:30:23,  1.96it/s]global step 234200, epoch: 2, batch: 37849, loss: 0.87321, acc: 0.91603, time: Sat Jan 22 19:48:21 2022
epoch 2 training:  19%|█▉        | 37948/196351 [5:23:15<22:33:42,  1.95it/s]global step 234300, epoch: 2, batch: 37949, loss: 1.72313, acc: 0.91603, time: Sat Jan 22 19:49:12 2022
epoch 2 training:  19%|█▉        | 38048/196351 [5:24:06<22:23:47,  1.96it/s]global step 234400, epoch: 2, batch: 38049, loss: 2.81800, acc: 0.91608, time: Sat Jan 22 19:50:03 2022
epoch 2 training:  19%|█▉        | 38148/196351 [5:24:56<22:11:35,  1.98it/s]global step 234500, epoch: 2, batch: 38149, loss: 0.35470, acc: 0.91613, time: Sat Jan 22 19:50:54 2022
epoch 2 training:  19%|█▉        | 38248/196351 [5:25:47<22:32:45,  1.95it/s]global step 234600, epoch: 2, batch: 38249, loss: 0.96689, acc: 0.91615, time: Sat Jan 22 19:51:45 2022
epoch 2 training:  20%|█▉        | 38348/196351 [5:26:38<22:21:02,  1.96it/s]global step 234700, epoch: 2, batch: 38349, loss: 2.67514, acc: 0.91610, time: Sat Jan 22 19:52:36 2022
epoch 2 training:  20%|█▉        | 38448/196351 [5:27:30<22:33:17,  1.94it/s]global step 234800, epoch: 2, batch: 38449, loss: 1.16908, acc: 0.91614, time: Sat Jan 22 19:53:27 2022
epoch 2 training:  20%|█▉        | 38548/196351 [5:28:21<22:34:42,  1.94it/s]global step 234900, epoch: 2, batch: 38549, loss: 2.03104, acc: 0.91615, time: Sat Jan 22 19:54:19 2022
epoch 2 training:  20%|█▉        | 38648/196351 [5:29:12<21:55:50,  2.00it/s]global step 235000, epoch: 2, batch: 38649, loss: 0.76890, acc: 0.91627, time: Sat Jan 22 19:55:10 2022
epoch 2 training:  20%|█▉        | 38748/196351 [5:30:03<22:26:04,  1.95it/s]global step 235100, epoch: 2, batch: 38749, loss: 0.20940, acc: 0.91633, time: Sat Jan 22 19:56:01 2022
epoch 2 training:  20%|█▉        | 38848/196351 [5:30:54<22:08:04,  1.98it/s]global step 235200, epoch: 2, batch: 38849, loss: 2.63302, acc: 0.91639, time: Sat Jan 22 19:56:52 2022
epoch 2 training:  20%|█▉        | 38948/196351 [5:31:45<22:16:46,  1.96it/s]global step 235300, epoch: 2, batch: 38949, loss: 1.64691, acc: 0.91642, time: Sat Jan 22 19:57:43 2022
epoch 2 training:  20%|█▉        | 39048/196351 [5:32:37<22:22:35,  1.95it/s]global step 235400, epoch: 2, batch: 39049, loss: 0.50816, acc: 0.91639, time: Sat Jan 22 19:58:34 2022
epoch 2 training:  20%|█▉        | 39148/196351 [5:33:28<22:48:09,  1.92it/s]global step 235500, epoch: 2, batch: 39149, loss: 1.66639, acc: 0.91642, time: Sat Jan 22 19:59:25 2022
epoch 2 training:  20%|█▉        | 39248/196351 [5:34:20<22:24:02,  1.95it/s]global step 235600, epoch: 2, batch: 39249, loss: 1.70477, acc: 0.91637, time: Sat Jan 22 20:00:17 2022
epoch 2 training:  20%|██        | 39348/196351 [5:35:11<22:35:26,  1.93it/s]global step 235700, epoch: 2, batch: 39349, loss: 0.78230, acc: 0.91644, time: Sat Jan 22 20:01:09 2022
epoch 2 training:  20%|██        | 39448/196351 [5:36:03<22:10:35,  1.97it/s]global step 235800, epoch: 2, batch: 39449, loss: 2.14554, acc: 0.91649, time: Sat Jan 22 20:02:00 2022
epoch 2 training:  20%|██        | 39548/196351 [5:36:54<22:21:47,  1.95it/s]global step 235900, epoch: 2, batch: 39549, loss: 3.36284, acc: 0.91646, time: Sat Jan 22 20:02:52 2022
epoch 2 training:  20%|██        | 39648/196351 [5:37:46<22:07:46,  1.97it/s]global step 236000, epoch: 2, batch: 39649, loss: 1.93239, acc: 0.91653, time: Sat Jan 22 20:03:43 2022
epoch 2 training:  20%|██        | 39748/196351 [5:38:37<22:19:58,  1.95it/s]global step 236100, epoch: 2, batch: 39749, loss: 0.72941, acc: 0.91660, time: Sat Jan 22 20:04:35 2022
epoch 2 training:  20%|██        | 39848/196351 [5:39:29<22:25:19,  1.94it/s]global step 236200, epoch: 2, batch: 39849, loss: 0.95344, acc: 0.91667, time: Sat Jan 22 20:05:26 2022
epoch 2 training:  20%|██        | 39948/196351 [5:40:20<22:03:23,  1.97it/s]global step 236300, epoch: 2, batch: 39949, loss: 2.77998, acc: 0.91668, time: Sat Jan 22 20:06:17 2022
epoch 2 training:  20%|██        | 40048/196351 [5:41:11<22:00:17,  1.97it/s]global step 236400, epoch: 2, batch: 40049, loss: 0.42386, acc: 0.91678, time: Sat Jan 22 20:07:08 2022
epoch 2 training:  20%|██        | 40148/196351 [5:42:01<22:10:45,  1.96it/s]global step 236500, epoch: 2, batch: 40149, loss: 1.12590, acc: 0.91682, time: Sat Jan 22 20:07:59 2022
epoch 2 training:  20%|██        | 40248/196351 [5:42:53<22:05:06,  1.96it/s]global step 236600, epoch: 2, batch: 40249, loss: 2.17262, acc: 0.91692, time: Sat Jan 22 20:08:50 2022
epoch 2 training:  21%|██        | 40348/196351 [5:43:44<22:22:29,  1.94it/s]global step 236700, epoch: 2, batch: 40349, loss: 3.08318, acc: 0.91689, time: Sat Jan 22 20:09:42 2022
epoch 2 training:  21%|██        | 40448/196351 [5:44:36<23:15:48,  1.86it/s]global step 236800, epoch: 2, batch: 40449, loss: 2.63498, acc: 0.91691, time: Sat Jan 22 20:10:34 2022
epoch 2 training:  21%|██        | 40548/196351 [5:45:28<21:47:14,  1.99it/s]global step 236900, epoch: 2, batch: 40549, loss: 0.60686, acc: 0.91703, time: Sat Jan 22 20:11:26 2022
epoch 2 training:  21%|██        | 40648/196351 [5:46:20<21:52:31,  1.98it/s]global step 237000, epoch: 2, batch: 40649, loss: 1.34290, acc: 0.91703, time: Sat Jan 22 20:12:17 2022
epoch 2 training:  21%|██        | 40748/196351 [5:47:11<22:01:31,  1.96it/s]global step 237100, epoch: 2, batch: 40749, loss: 0.86934, acc: 0.91700, time: Sat Jan 22 20:13:08 2022
epoch 2 training:  21%|██        | 40848/196351 [5:48:02<22:26:01,  1.93it/s]global step 237200, epoch: 2, batch: 40849, loss: 0.07751, acc: 0.91701, time: Sat Jan 22 20:14:00 2022
epoch 2 training:  21%|██        | 40948/196351 [5:48:54<22:06:31,  1.95it/s]global step 237300, epoch: 2, batch: 40949, loss: 1.11245, acc: 0.91706, time: Sat Jan 22 20:14:51 2022
epoch 2 training:  21%|██        | 41048/196351 [5:49:45<22:17:08,  1.94it/s]global step 237400, epoch: 2, batch: 41049, loss: 0.65850, acc: 0.91707, time: Sat Jan 22 20:15:43 2022
epoch 2 training:  21%|██        | 41148/196351 [5:50:37<22:07:53,  1.95it/s]global step 237500, epoch: 2, batch: 41149, loss: 1.01443, acc: 0.91718, time: Sat Jan 22 20:16:34 2022
epoch 2 training:  21%|██        | 41248/196351 [5:51:29<22:01:23,  1.96it/s]global step 237600, epoch: 2, batch: 41249, loss: 1.08035, acc: 0.91720, time: Sat Jan 22 20:17:26 2022
epoch 2 training:  21%|██        | 41348/196351 [5:52:21<22:09:56,  1.94it/s]global step 237700, epoch: 2, batch: 41349, loss: 3.43599, acc: 0.91718, time: Sat Jan 22 20:18:18 2022
epoch 2 training:  21%|██        | 41448/196351 [5:53:12<22:10:45,  1.94it/s]global step 237800, epoch: 2, batch: 41449, loss: 1.20987, acc: 0.91721, time: Sat Jan 22 20:19:10 2022
epoch 2 training:  21%|██        | 41548/196351 [5:54:03<21:52:22,  1.97it/s]global step 237900, epoch: 2, batch: 41549, loss: 2.91569, acc: 0.91722, time: Sat Jan 22 20:20:01 2022
epoch 2 training:  21%|██        | 41648/196351 [5:54:55<21:52:15,  1.96it/s]global step 238000, epoch: 2, batch: 41649, loss: 0.53164, acc: 0.91728, time: Sat Jan 22 20:20:52 2022
epoch 2 training:  21%|██▏       | 41748/196351 [5:55:46<22:06:22,  1.94it/s]global step 238100, epoch: 2, batch: 41749, loss: 2.71381, acc: 0.91720, time: Sat Jan 22 20:21:44 2022
epoch 2 training:  21%|██▏       | 41848/196351 [5:56:38<22:08:54,  1.94it/s]global step 238200, epoch: 2, batch: 41849, loss: 0.65832, acc: 0.91727, time: Sat Jan 22 20:22:35 2022
epoch 2 training:  21%|██▏       | 41948/196351 [5:57:29<22:23:22,  1.92it/s]global step 238300, epoch: 2, batch: 41949, loss: 1.10790, acc: 0.91732, time: Sat Jan 22 20:23:27 2022
epoch 2 training:  21%|██▏       | 42048/196351 [5:58:21<21:57:00,  1.95it/s]global step 238400, epoch: 2, batch: 42049, loss: 3.61444, acc: 0.91735, time: Sat Jan 22 20:24:18 2022
epoch 2 training:  21%|██▏       | 42148/196351 [5:59:13<22:07:52,  1.94it/s]global step 238500, epoch: 2, batch: 42149, loss: 0.81581, acc: 0.91738, time: Sat Jan 22 20:25:10 2022
epoch 2 training:  22%|██▏       | 42248/196351 [6:00:04<21:59:35,  1.95it/s]global step 238600, epoch: 2, batch: 42249, loss: 1.97388, acc: 0.91743, time: Sat Jan 22 20:26:01 2022
epoch 2 training:  22%|██▏       | 42348/196351 [6:00:57<24:12:20,  1.77it/s]global step 238700, epoch: 2, batch: 42349, loss: 2.82077, acc: 0.91746, time: Sat Jan 22 20:26:55 2022
epoch 2 training:  22%|██▏       | 42448/196351 [6:01:49<21:37:15,  1.98it/s]global step 238800, epoch: 2, batch: 42449, loss: 2.37837, acc: 0.91747, time: Sat Jan 22 20:27:46 2022
epoch 2 training:  22%|██▏       | 42548/196351 [6:02:40<21:52:49,  1.95it/s]global step 238900, epoch: 2, batch: 42549, loss: 0.39064, acc: 0.91750, time: Sat Jan 22 20:28:38 2022
epoch 2 training:  22%|██▏       | 42648/196351 [6:03:32<22:15:07,  1.92it/s]global step 239000, epoch: 2, batch: 42649, loss: 0.96772, acc: 0.91743, time: Sat Jan 22 20:29:30 2022
epoch 2 training:  22%|██▏       | 42748/196351 [6:04:24<22:07:30,  1.93it/s]global step 239100, epoch: 2, batch: 42749, loss: 0.34132, acc: 0.91748, time: Sat Jan 22 20:30:22 2022
epoch 2 training:  22%|██▏       | 42848/196351 [6:05:17<22:12:27,  1.92it/s]global step 239200, epoch: 2, batch: 42849, loss: 1.61943, acc: 0.91741, time: Sat Jan 22 20:31:14 2022
epoch 2 training:  22%|██▏       | 42948/196351 [6:06:08<22:09:52,  1.92it/s]global step 239300, epoch: 2, batch: 42949, loss: 1.60844, acc: 0.91741, time: Sat Jan 22 20:32:06 2022
epoch 2 training:  22%|██▏       | 43048/196351 [6:07:01<22:50:32,  1.86it/s]global step 239400, epoch: 2, batch: 43049, loss: 0.29628, acc: 0.91747, time: Sat Jan 22 20:32:58 2022
epoch 2 training:  22%|██▏       | 43148/196351 [6:07:53<22:01:43,  1.93it/s]global step 239500, epoch: 2, batch: 43149, loss: 0.18552, acc: 0.91754, time: Sat Jan 22 20:33:50 2022
epoch 2 training:  22%|██▏       | 43248/196351 [6:08:45<21:44:17,  1.96it/s]global step 239600, epoch: 2, batch: 43249, loss: 0.70592, acc: 0.91754, time: Sat Jan 22 20:34:42 2022
epoch 2 training:  22%|██▏       | 43348/196351 [6:09:36<21:46:53,  1.95it/s]global step 239700, epoch: 2, batch: 43349, loss: 1.60459, acc: 0.91753, time: Sat Jan 22 20:35:34 2022
epoch 2 training:  22%|██▏       | 43448/196351 [6:10:27<22:01:30,  1.93it/s]global step 239800, epoch: 2, batch: 43449, loss: 3.98619, acc: 0.91742, time: Sat Jan 22 20:36:25 2022
epoch 2 training:  22%|██▏       | 43548/196351 [6:11:19<21:33:16,  1.97it/s]global step 239900, epoch: 2, batch: 43549, loss: 1.15715, acc: 0.91745, time: Sat Jan 22 20:37:16 2022
epoch 2 training:  22%|██▏       | 43648/196351 [6:12:11<22:16:09,  1.90it/s]global step 240000, epoch: 2, batch: 43649, loss: 2.39600, acc: 0.91743, time: Sat Jan 22 20:38:08 2022
epoch 2 training:  22%|██▏       | 43748/196351 [6:13:03<22:07:14,  1.92it/s]global step 240100, epoch: 2, batch: 43749, loss: 1.17756, acc: 0.91743, time: Sat Jan 22 20:39:00 2022
epoch 2 training:  22%|██▏       | 43848/196351 [6:13:55<21:52:19,  1.94it/s]global step 240200, epoch: 2, batch: 43849, loss: 4.19087, acc: 0.91750, time: Sat Jan 22 20:39:52 2022
epoch 2 training:  22%|██▏       | 43948/196351 [6:14:46<21:59:11,  1.93it/s]global step 240300, epoch: 2, batch: 43949, loss: 2.81402, acc: 0.91747, time: Sat Jan 22 20:40:44 2022
epoch 2 training:  22%|██▏       | 44048/196351 [6:15:38<21:45:21,  1.94it/s]global step 240400, epoch: 2, batch: 44049, loss: 0.88670, acc: 0.91748, time: Sat Jan 22 20:41:35 2022
epoch 2 training:  22%|██▏       | 44148/196351 [6:16:30<22:47:28,  1.86it/s]global step 240500, epoch: 2, batch: 44149, loss: 2.20020, acc: 0.91748, time: Sat Jan 22 20:42:28 2022
epoch 2 training:  23%|██▎       | 44248/196351 [6:17:22<21:30:47,  1.96it/s]global step 240600, epoch: 2, batch: 44249, loss: 2.87604, acc: 0.91749, time: Sat Jan 22 20:43:20 2022
epoch 2 training:  23%|██▎       | 44348/196351 [6:18:14<22:04:15,  1.91it/s]global step 240700, epoch: 2, batch: 44349, loss: 0.84076, acc: 0.91752, time: Sat Jan 22 20:44:11 2022
epoch 2 training:  23%|██▎       | 44448/196351 [6:19:05<21:59:33,  1.92it/s]global step 240800, epoch: 2, batch: 44449, loss: 2.17092, acc: 0.91762, time: Sat Jan 22 20:45:03 2022
epoch 2 training:  23%|██▎       | 44548/196351 [6:19:57<21:36:12,  1.95it/s]global step 240900, epoch: 2, batch: 44549, loss: 2.18038, acc: 0.91771, time: Sat Jan 22 20:45:54 2022
epoch 2 training:  23%|██▎       | 44648/196351 [6:20:48<21:45:19,  1.94it/s]global step 241000, epoch: 2, batch: 44649, loss: 2.08508, acc: 0.91779, time: Sat Jan 22 20:46:46 2022
epoch 2 training:  23%|██▎       | 44748/196351 [6:21:40<21:45:00,  1.94it/s]global step 241100, epoch: 2, batch: 44749, loss: 3.12703, acc: 0.91784, time: Sat Jan 22 20:47:37 2022
epoch 2 training:  23%|██▎       | 44848/196351 [6:22:32<22:16:31,  1.89it/s]global step 241200, epoch: 2, batch: 44849, loss: 3.44307, acc: 0.91789, time: Sat Jan 22 20:48:29 2022
epoch 2 training:  23%|██▎       | 44948/196351 [6:23:23<21:29:39,  1.96it/s]global step 241300, epoch: 2, batch: 44949, loss: 0.58979, acc: 0.91797, time: Sat Jan 22 20:49:20 2022
epoch 2 training:  23%|██▎       | 45048/196351 [6:24:14<21:52:48,  1.92it/s]global step 241400, epoch: 2, batch: 45049, loss: 0.12267, acc: 0.91804, time: Sat Jan 22 20:50:12 2022
epoch 2 training:  23%|██▎       | 45148/196351 [6:25:06<21:36:53,  1.94it/s]global step 241500, epoch: 2, batch: 45149, loss: 1.97597, acc: 0.91807, time: Sat Jan 22 20:51:04 2022
epoch 2 training:  23%|██▎       | 45248/196351 [6:25:58<21:31:51,  1.95it/s]global step 241600, epoch: 2, batch: 45249, loss: 2.82502, acc: 0.91810, time: Sat Jan 22 20:51:55 2022
epoch 2 training:  23%|██▎       | 45348/196351 [6:26:49<21:53:55,  1.92it/s]global step 241700, epoch: 2, batch: 45349, loss: 1.51846, acc: 0.91807, time: Sat Jan 22 20:52:46 2022
epoch 2 training:  23%|██▎       | 45448/196351 [6:27:41<21:57:33,  1.91it/s]global step 241800, epoch: 2, batch: 45449, loss: 1.39512, acc: 0.91810, time: Sat Jan 22 20:53:38 2022
epoch 2 training:  23%|██▎       | 45548/196351 [6:28:33<21:24:03,  1.96it/s]global step 241900, epoch: 2, batch: 45549, loss: 2.76865, acc: 0.91818, time: Sat Jan 22 20:54:30 2022
epoch 2 training:  23%|██▎       | 45648/196351 [6:29:25<21:27:04,  1.95it/s]global step 242000, epoch: 2, batch: 45649, loss: 0.82220, acc: 0.91827, time: Sat Jan 22 20:55:22 2022
epoch 2 training:  23%|██▎       | 45748/196351 [6:30:17<21:30:07,  1.95it/s]global step 242100, epoch: 2, batch: 45749, loss: 0.37019, acc: 0.91829, time: Sat Jan 22 20:56:14 2022
epoch 2 training:  23%|██▎       | 45848/196351 [6:31:09<21:26:18,  1.95it/s]global step 242200, epoch: 2, batch: 45849, loss: 3.23118, acc: 0.91830, time: Sat Jan 22 20:57:06 2022
epoch 2 training:  23%|██▎       | 45948/196351 [6:32:00<21:26:33,  1.95it/s]global step 242300, epoch: 2, batch: 45949, loss: 1.31798, acc: 0.91829, time: Sat Jan 22 20:57:58 2022
epoch 2 training:  23%|██▎       | 46048/196351 [6:32:52<21:25:28,  1.95it/s]global step 242400, epoch: 2, batch: 46049, loss: 1.93824, acc: 0.91835, time: Sat Jan 22 20:58:50 2022
epoch 2 training:  24%|██▎       | 46148/196351 [6:33:44<21:12:02,  1.97it/s]global step 242500, epoch: 2, batch: 46149, loss: 4.22525, acc: 0.91844, time: Sat Jan 22 20:59:41 2022
epoch 2 training:  24%|██▎       | 46248/196351 [6:34:35<21:32:00,  1.94it/s]global step 242600, epoch: 2, batch: 46249, loss: 0.68625, acc: 0.91842, time: Sat Jan 22 21:00:32 2022
epoch 2 training:  24%|██▎       | 46348/196351 [6:35:26<21:17:27,  1.96it/s]global step 242700, epoch: 2, batch: 46349, loss: 0.37774, acc: 0.91848, time: Sat Jan 22 21:01:24 2022
epoch 2 training:  24%|██▎       | 46448/196351 [6:36:18<21:20:18,  1.95it/s]global step 242800, epoch: 2, batch: 46449, loss: 1.19744, acc: 0.91855, time: Sat Jan 22 21:02:15 2022
epoch 2 training:  24%|██▎       | 46548/196351 [6:37:09<21:08:38,  1.97it/s]global step 242900, epoch: 2, batch: 46549, loss: 1.98041, acc: 0.91852, time: Sat Jan 22 21:03:07 2022
epoch 2 training:  24%|██▍       | 46648/196351 [6:38:00<21:07:11,  1.97it/s]global step 243000, epoch: 2, batch: 46649, loss: 1.61743, acc: 0.91857, time: Sat Jan 22 21:03:58 2022
epoch 2 training:  24%|██▍       | 46748/196351 [6:38:52<21:06:02,  1.97it/s]global step 243100, epoch: 2, batch: 46749, loss: 2.02144, acc: 0.91862, time: Sat Jan 22 21:04:49 2022
epoch 2 training:  24%|██▍       | 46848/196351 [6:39:43<21:25:05,  1.94it/s]global step 243200, epoch: 2, batch: 46849, loss: 0.58448, acc: 0.91873, time: Sat Jan 22 21:05:40 2022
epoch 2 training:  24%|██▍       | 46948/196351 [6:40:35<25:13:30,  1.65it/s]global step 243300, epoch: 2, batch: 46949, loss: 2.84525, acc: 0.91874, time: Sat Jan 22 21:06:32 2022
epoch 2 training:  24%|██▍       | 47048/196351 [6:41:26<21:13:48,  1.95it/s]global step 243400, epoch: 2, batch: 47049, loss: 2.86272, acc: 0.91874, time: Sat Jan 22 21:07:24 2022
epoch 2 training:  24%|██▍       | 47148/196351 [6:42:18<21:17:06,  1.95it/s]global step 243500, epoch: 2, batch: 47149, loss: 1.69179, acc: 0.91879, time: Sat Jan 22 21:08:15 2022
epoch 2 training:  24%|██▍       | 47248/196351 [6:43:09<21:18:14,  1.94it/s]global step 243600, epoch: 2, batch: 47249, loss: 0.78628, acc: 0.91880, time: Sat Jan 22 21:09:07 2022
epoch 2 training:  24%|██▍       | 47348/196351 [6:44:01<21:09:39,  1.96it/s]global step 243700, epoch: 2, batch: 47349, loss: 3.16530, acc: 0.91887, time: Sat Jan 22 21:09:58 2022
epoch 2 training:  24%|██▍       | 47448/196351 [6:44:52<21:15:04,  1.95it/s]global step 243800, epoch: 2, batch: 47449, loss: 1.76596, acc: 0.91893, time: Sat Jan 22 21:10:50 2022
epoch 2 training:  24%|██▍       | 47548/196351 [6:45:43<21:26:40,  1.93it/s]global step 243900, epoch: 2, batch: 47549, loss: 1.76717, acc: 0.91902, time: Sat Jan 22 21:11:41 2022
epoch 2 training:  24%|██▍       | 47648/196351 [6:46:35<21:27:43,  1.92it/s]global step 244000, epoch: 2, batch: 47649, loss: 3.78233, acc: 0.91909, time: Sat Jan 22 21:12:32 2022
epoch 2 training:  24%|██▍       | 47748/196351 [6:47:27<21:27:54,  1.92it/s]global step 244100, epoch: 2, batch: 47749, loss: 2.30566, acc: 0.91909, time: Sat Jan 22 21:13:24 2022
epoch 2 training:  24%|██▍       | 47848/196351 [6:48:18<21:26:21,  1.92it/s]global step 244200, epoch: 2, batch: 47849, loss: 0.96198, acc: 0.91912, time: Sat Jan 22 21:14:16 2022
epoch 2 training:  24%|██▍       | 47948/196351 [6:49:10<21:23:08,  1.93it/s]global step 244300, epoch: 2, batch: 47949, loss: 2.53725, acc: 0.91914, time: Sat Jan 22 21:15:07 2022
epoch 2 training:  24%|██▍       | 48048/196351 [6:50:02<21:08:53,  1.95it/s]global step 244400, epoch: 2, batch: 48049, loss: 2.03706, acc: 0.91913, time: Sat Jan 22 21:15:59 2022
epoch 2 training:  25%|██▍       | 48148/196351 [6:50:53<21:13:47,  1.94it/s]global step 244500, epoch: 2, batch: 48149, loss: 1.99419, acc: 0.91914, time: Sat Jan 22 21:16:50 2022
epoch 2 training:  25%|██▍       | 48248/196351 [6:51:44<20:53:07,  1.97it/s]global step 244600, epoch: 2, batch: 48249, loss: 1.37001, acc: 0.91922, time: Sat Jan 22 21:17:41 2022
epoch 2 training:  25%|██▍       | 48348/196351 [6:52:35<21:02:37,  1.95it/s]global step 244700, epoch: 2, batch: 48349, loss: 0.98861, acc: 0.91922, time: Sat Jan 22 21:18:32 2022
epoch 2 training:  25%|██▍       | 48448/196351 [6:53:26<21:05:17,  1.95it/s]global step 244800, epoch: 2, batch: 48449, loss: 1.47707, acc: 0.91924, time: Sat Jan 22 21:19:23 2022
epoch 2 training:  25%|██▍       | 48548/196351 [6:54:17<21:04:29,  1.95it/s]global step 244900, epoch: 2, batch: 48549, loss: 0.72901, acc: 0.91921, time: Sat Jan 22 21:20:15 2022
epoch 2 training:  25%|██▍       | 48648/196351 [6:55:08<20:52:44,  1.97it/s]global step 245000, epoch: 2, batch: 48649, loss: 3.01997, acc: 0.91921, time: Sat Jan 22 21:21:05 2022
epoch 2 training:  25%|██▍       | 48748/196351 [6:55:59<20:43:07,  1.98it/s]global step 245100, epoch: 2, batch: 48749, loss: 2.28358, acc: 0.91921, time: Sat Jan 22 21:21:56 2022
epoch 2 training:  25%|██▍       | 48848/196351 [6:56:50<20:55:29,  1.96it/s]global step 245200, epoch: 2, batch: 48849, loss: 0.32221, acc: 0.91921, time: Sat Jan 22 21:22:48 2022
epoch 2 training:  25%|██▍       | 48948/196351 [6:57:42<21:06:53,  1.94it/s]global step 245300, epoch: 2, batch: 48949, loss: 0.55390, acc: 0.91921, time: Sat Jan 22 21:23:40 2022
epoch 2 training:  25%|██▍       | 49048/196351 [6:58:34<20:57:21,  1.95it/s]global step 245400, epoch: 2, batch: 49049, loss: 1.95708, acc: 0.91915, time: Sat Jan 22 21:24:31 2022
epoch 2 training:  25%|██▌       | 49148/196351 [6:59:25<21:02:10,  1.94it/s]global step 245500, epoch: 2, batch: 49149, loss: 2.58258, acc: 0.91917, time: Sat Jan 22 21:25:22 2022
epoch 2 training:  25%|██▌       | 49248/196351 [7:00:17<21:10:22,  1.93it/s]global step 245600, epoch: 2, batch: 49249, loss: 2.27916, acc: 0.91920, time: Sat Jan 22 21:26:14 2022
epoch 2 training:  25%|██▌       | 49348/196351 [7:01:08<21:15:45,  1.92it/s]global step 245700, epoch: 2, batch: 49349, loss: 0.71686, acc: 0.91918, time: Sat Jan 22 21:27:06 2022
epoch 2 training:  25%|██▌       | 49448/196351 [7:02:00<21:21:25,  1.91it/s]global step 245800, epoch: 2, batch: 49449, loss: 2.74437, acc: 0.91924, time: Sat Jan 22 21:27:57 2022
epoch 2 training:  25%|██▌       | 49548/196351 [7:02:51<20:35:14,  1.98it/s]global step 245900, epoch: 2, batch: 49549, loss: 1.69459, acc: 0.91924, time: Sat Jan 22 21:28:49 2022
epoch 2 training:  25%|██▌       | 49648/196351 [7:03:43<20:44:25,  1.96it/s]global step 246000, epoch: 2, batch: 49649, loss: 0.26616, acc: 0.91929, time: Sat Jan 22 21:29:40 2022
epoch 2 training:  25%|██▌       | 49748/196351 [7:04:34<20:46:13,  1.96it/s]global step 246100, epoch: 2, batch: 49749, loss: 0.20615, acc: 0.91930, time: Sat Jan 22 21:30:31 2022
epoch 2 training:  25%|██▌       | 49848/196351 [7:05:25<20:42:20,  1.97it/s]global step 246200, epoch: 2, batch: 49849, loss: 2.76522, acc: 0.91932, time: Sat Jan 22 21:31:22 2022
epoch 2 training:  25%|██▌       | 49948/196351 [7:06:16<20:56:55,  1.94it/s]global step 246300, epoch: 2, batch: 49949, loss: 0.33044, acc: 0.91937, time: Sat Jan 22 21:32:13 2022
epoch 2 training:  25%|██▌       | 50048/196351 [7:07:07<20:55:38,  1.94it/s]global step 246400, epoch: 2, batch: 50049, loss: 2.46733, acc: 0.91939, time: Sat Jan 22 21:33:04 2022
epoch 2 training:  26%|██▌       | 50148/196351 [7:07:58<20:45:21,  1.96it/s]global step 246500, epoch: 2, batch: 50149, loss: 2.43730, acc: 0.91941, time: Sat Jan 22 21:33:55 2022
epoch 2 training:  26%|██▌       | 50248/196351 [7:08:50<20:56:26,  1.94it/s]global step 246600, epoch: 2, batch: 50249, loss: 1.83481, acc: 0.91945, time: Sat Jan 22 21:34:47 2022
epoch 2 training:  26%|██▌       | 50348/196351 [7:09:41<20:34:01,  1.97it/s]global step 246700, epoch: 2, batch: 50349, loss: 0.81667, acc: 0.91945, time: Sat Jan 22 21:35:38 2022
epoch 2 training:  26%|██▌       | 50448/196351 [7:10:32<21:16:37,  1.90it/s]global step 246800, epoch: 2, batch: 50449, loss: 0.83109, acc: 0.91945, time: Sat Jan 22 21:36:30 2022
epoch 2 training:  26%|██▌       | 50548/196351 [7:11:24<20:25:46,  1.98it/s]global step 246900, epoch: 2, batch: 50549, loss: 0.60942, acc: 0.91949, time: Sat Jan 22 21:37:21 2022
epoch 2 training:  26%|██▌       | 50648/196351 [7:12:15<20:48:17,  1.95it/s]global step 247000, epoch: 2, batch: 50649, loss: 0.54219, acc: 0.91949, time: Sat Jan 22 21:38:12 2022
epoch 2 training:  26%|██▌       | 50748/196351 [7:13:06<20:35:45,  1.96it/s]global step 247100, epoch: 2, batch: 50749, loss: 1.96502, acc: 0.91953, time: Sat Jan 22 21:39:03 2022
epoch 2 training:  26%|██▌       | 50848/196351 [7:13:58<20:42:06,  1.95it/s]global step 247200, epoch: 2, batch: 50849, loss: 3.40458, acc: 0.91953, time: Sat Jan 22 21:39:55 2022
epoch 2 training:  26%|██▌       | 50948/196351 [7:14:49<20:45:50,  1.95it/s]global step 247300, epoch: 2, batch: 50949, loss: 4.48207, acc: 0.91954, time: Sat Jan 22 21:40:46 2022
epoch 2 training:  26%|██▌       | 51048/196351 [7:15:40<20:43:25,  1.95it/s]global step 247400, epoch: 2, batch: 51049, loss: 0.85673, acc: 0.91957, time: Sat Jan 22 21:41:38 2022
epoch 2 training:  26%|██▌       | 51148/196351 [7:16:31<20:34:39,  1.96it/s]global step 247500, epoch: 2, batch: 51149, loss: 1.57542, acc: 0.91963, time: Sat Jan 22 21:42:29 2022
epoch 2 training:  26%|██▌       | 51248/196351 [7:17:22<20:28:52,  1.97it/s]global step 247600, epoch: 2, batch: 51249, loss: 1.19390, acc: 0.91968, time: Sat Jan 22 21:43:20 2022
epoch 2 training:  26%|██▌       | 51348/196351 [7:18:13<20:30:21,  1.96it/s]global step 247700, epoch: 2, batch: 51349, loss: 1.07211, acc: 0.91969, time: Sat Jan 22 21:44:11 2022
epoch 2 training:  26%|██▌       | 51448/196351 [7:19:05<20:33:59,  1.96it/s]global step 247800, epoch: 2, batch: 51449, loss: 2.10337, acc: 0.91972, time: Sat Jan 22 21:45:02 2022
epoch 2 training:  26%|██▋       | 51548/196351 [7:19:56<20:22:52,  1.97it/s]global step 247900, epoch: 2, batch: 51549, loss: 2.24717, acc: 0.91972, time: Sat Jan 22 21:45:53 2022
epoch 2 training:  26%|██▋       | 51648/196351 [7:20:47<20:39:31,  1.95it/s]global step 248000, epoch: 2, batch: 51649, loss: 2.04667, acc: 0.91973, time: Sat Jan 22 21:46:45 2022
epoch 2 training:  26%|██▋       | 51748/196351 [7:21:38<20:27:39,  1.96it/s]global step 248100, epoch: 2, batch: 51749, loss: 0.56176, acc: 0.91972, time: Sat Jan 22 21:47:36 2022
epoch 2 training:  26%|██▋       | 51848/196351 [7:22:30<20:40:26,  1.94it/s]global step 248200, epoch: 2, batch: 51849, loss: 1.56304, acc: 0.91978, time: Sat Jan 22 21:48:27 2022
epoch 2 training:  26%|██▋       | 51948/196351 [7:23:22<20:30:16,  1.96it/s]global step 248300, epoch: 2, batch: 51949, loss: 2.94218, acc: 0.91977, time: Sat Jan 22 21:49:19 2022
epoch 2 training:  27%|██▋       | 52048/196351 [7:24:13<20:43:20,  1.93it/s]global step 248400, epoch: 2, batch: 52049, loss: 1.71721, acc: 0.91981, time: Sat Jan 22 21:50:11 2022
epoch 2 training:  27%|██▋       | 52148/196351 [7:25:04<20:27:12,  1.96it/s]global step 248500, epoch: 2, batch: 52149, loss: 0.84110, acc: 0.91988, time: Sat Jan 22 21:51:02 2022
epoch 2 training:  27%|██▋       | 52248/196351 [7:25:56<20:36:32,  1.94it/s]global step 248600, epoch: 2, batch: 52249, loss: 3.19979, acc: 0.91981, time: Sat Jan 22 21:51:53 2022
epoch 2 training:  27%|██▋       | 52348/196351 [7:26:47<20:32:16,  1.95it/s]global step 248700, epoch: 2, batch: 52349, loss: 1.81219, acc: 0.91986, time: Sat Jan 22 21:52:44 2022
epoch 2 training:  27%|██▋       | 52448/196351 [7:27:38<20:36:40,  1.94it/s]global step 248800, epoch: 2, batch: 52449, loss: 3.84494, acc: 0.91990, time: Sat Jan 22 21:53:36 2022
epoch 2 training:  27%|██▋       | 52548/196351 [7:28:30<20:49:37,  1.92it/s]global step 248900, epoch: 2, batch: 52549, loss: 1.83217, acc: 0.91995, time: Sat Jan 22 21:54:27 2022
epoch 2 training:  27%|██▋       | 52648/196351 [7:29:21<20:18:41,  1.97it/s]global step 249000, epoch: 2, batch: 52649, loss: 3.04544, acc: 0.91994, time: Sat Jan 22 21:55:18 2022
epoch 2 training:  27%|██▋       | 52748/196351 [7:30:12<20:29:37,  1.95it/s]global step 249100, epoch: 2, batch: 52749, loss: 0.65409, acc: 0.91993, time: Sat Jan 22 21:56:10 2022
epoch 2 training:  27%|██▋       | 52848/196351 [7:31:04<20:13:32,  1.97it/s]global step 249200, epoch: 2, batch: 52849, loss: 1.89672, acc: 0.91994, time: Sat Jan 22 21:57:02 2022
epoch 2 training:  27%|██▋       | 52948/196351 [7:31:56<20:18:03,  1.96it/s]global step 249300, epoch: 2, batch: 52949, loss: 2.40763, acc: 0.91987, time: Sat Jan 22 21:57:54 2022
epoch 2 training:  27%|██▋       | 53048/196351 [7:32:48<20:30:30,  1.94it/s]global step 249400, epoch: 2, batch: 53049, loss: 1.09731, acc: 0.91982, time: Sat Jan 22 21:58:45 2022
epoch 2 training:  27%|██▋       | 53148/196351 [7:33:39<20:17:36,  1.96it/s]global step 249500, epoch: 2, batch: 53149, loss: 2.99371, acc: 0.91986, time: Sat Jan 22 21:59:36 2022
epoch 2 training:  27%|██▋       | 53248/196351 [7:34:30<20:23:28,  1.95it/s]global step 249600, epoch: 2, batch: 53249, loss: 3.01783, acc: 0.91989, time: Sat Jan 22 22:00:27 2022
epoch 2 training:  27%|██▋       | 53348/196351 [7:35:21<20:30:45,  1.94it/s]global step 249700, epoch: 2, batch: 53349, loss: 2.05693, acc: 0.91993, time: Sat Jan 22 22:01:19 2022
epoch 2 training:  27%|██▋       | 53448/196351 [7:36:13<20:24:56,  1.94it/s]global step 249800, epoch: 2, batch: 53449, loss: 1.79990, acc: 0.91991, time: Sat Jan 22 22:02:10 2022
epoch 2 training:  27%|██▋       | 53548/196351 [7:37:04<20:11:33,  1.96it/s]global step 249900, epoch: 2, batch: 53549, loss: 0.47676, acc: 0.91992, time: Sat Jan 22 22:03:02 2022
epoch 2 training:  27%|██▋       | 53648/196351 [7:37:55<20:31:56,  1.93it/s]global step 250000, epoch: 2, batch: 53649, loss: 0.68633, acc: 0.92000, time: Sat Jan 22 22:03:53 2022
epoch 2 training:  27%|██▋       | 53748/196351 [7:38:47<20:29:43,  1.93it/s]global step 250100, epoch: 2, batch: 53749, loss: 2.24869, acc: 0.91998, time: Sat Jan 22 22:04:44 2022
epoch 2 training:  27%|██▋       | 53848/196351 [7:39:38<20:23:37,  1.94it/s]global step 250200, epoch: 2, batch: 53849, loss: 1.18807, acc: 0.91997, time: Sat Jan 22 22:05:36 2022
epoch 2 training:  27%|██▋       | 53948/196351 [7:40:29<19:59:28,  1.98it/s]global step 250300, epoch: 2, batch: 53949, loss: 2.17669, acc: 0.91994, time: Sat Jan 22 22:06:27 2022
epoch 2 training:  28%|██▊       | 54048/196351 [7:41:21<20:17:13,  1.95it/s]global step 250400, epoch: 2, batch: 54049, loss: 0.22500, acc: 0.91992, time: Sat Jan 22 22:07:18 2022
epoch 2 training:  28%|██▊       | 54148/196351 [7:42:12<20:17:26,  1.95it/s]global step 250500, epoch: 2, batch: 54149, loss: 0.70429, acc: 0.91999, time: Sat Jan 22 22:08:09 2022
epoch 2 training:  28%|██▊       | 54248/196351 [7:43:03<20:38:32,  1.91it/s]global step 250600, epoch: 2, batch: 54249, loss: 0.30754, acc: 0.92000, time: Sat Jan 22 22:09:01 2022
epoch 2 training:  28%|██▊       | 54348/196351 [7:43:55<20:21:35,  1.94it/s]global step 250700, epoch: 2, batch: 54349, loss: 1.97978, acc: 0.92004, time: Sat Jan 22 22:09:52 2022
epoch 2 training:  28%|██▊       | 54448/196351 [7:44:46<20:11:13,  1.95it/s]global step 250800, epoch: 2, batch: 54449, loss: 0.90162, acc: 0.92006, time: Sat Jan 22 22:10:43 2022
epoch 2 training:  28%|██▊       | 54548/196351 [7:45:37<20:10:45,  1.95it/s]global step 250900, epoch: 2, batch: 54549, loss: 2.34603, acc: 0.92001, time: Sat Jan 22 22:11:35 2022
epoch 2 training:  28%|██▊       | 54648/196351 [7:46:29<20:12:09,  1.95it/s]global step 251000, epoch: 2, batch: 54649, loss: 0.60564, acc: 0.92003, time: Sat Jan 22 22:12:26 2022
epoch 2 training:  28%|██▊       | 54748/196351 [7:47:21<19:54:18,  1.98it/s]global step 251100, epoch: 2, batch: 54749, loss: 2.14219, acc: 0.92004, time: Sat Jan 22 22:13:18 2022
epoch 2 training:  28%|██▊       | 54848/196351 [7:48:12<19:50:36,  1.98it/s]global step 251200, epoch: 2, batch: 54849, loss: 2.09714, acc: 0.92011, time: Sat Jan 22 22:14:09 2022
epoch 2 training:  28%|██▊       | 54948/196351 [7:49:03<19:55:46,  1.97it/s]global step 251300, epoch: 2, batch: 54949, loss: 0.72304, acc: 0.92005, time: Sat Jan 22 22:15:00 2022
epoch 2 training:  28%|██▊       | 55048/196351 [7:49:54<20:11:28,  1.94it/s]global step 251400, epoch: 2, batch: 55049, loss: 3.42674, acc: 0.92012, time: Sat Jan 22 22:15:51 2022
epoch 2 training:  28%|██▊       | 55148/196351 [7:50:45<19:51:36,  1.97it/s]global step 251500, epoch: 2, batch: 55149, loss: 0.35027, acc: 0.92015, time: Sat Jan 22 22:16:43 2022
epoch 2 training:  28%|██▊       | 55248/196351 [7:51:36<20:07:28,  1.95it/s]global step 251600, epoch: 2, batch: 55249, loss: 2.93415, acc: 0.92022, time: Sat Jan 22 22:17:34 2022
epoch 2 training:  28%|██▊       | 55348/196351 [7:52:27<19:55:38,  1.97it/s]global step 251700, epoch: 2, batch: 55349, loss: 2.56202, acc: 0.92016, time: Sat Jan 22 22:18:25 2022
epoch 2 training:  28%|██▊       | 55448/196351 [7:53:19<20:04:03,  1.95it/s]global step 251800, epoch: 2, batch: 55449, loss: 5.01141, acc: 0.92013, time: Sat Jan 22 22:19:16 2022
epoch 2 training:  28%|██▊       | 55548/196351 [7:54:10<20:24:46,  1.92it/s]global step 251900, epoch: 2, batch: 55549, loss: 1.74451, acc: 0.92015, time: Sat Jan 22 22:20:07 2022
epoch 2 training:  28%|██▊       | 55648/196351 [7:55:01<20:04:11,  1.95it/s]global step 252000, epoch: 2, batch: 55649, loss: 2.28519, acc: 0.92021, time: Sat Jan 22 22:20:59 2022
epoch 2 training:  28%|██▊       | 55748/196351 [7:55:54<20:11:04,  1.93it/s]global step 252100, epoch: 2, batch: 55749, loss: 0.93213, acc: 0.92020, time: Sat Jan 22 22:21:51 2022
epoch 2 training:  28%|██▊       | 55848/196351 [7:56:45<19:58:47,  1.95it/s]global step 252200, epoch: 2, batch: 55849, loss: 0.58954, acc: 0.92021, time: Sat Jan 22 22:22:42 2022
epoch 2 training:  28%|██▊       | 55948/196351 [7:57:36<19:44:58,  1.97it/s]global step 252300, epoch: 2, batch: 55949, loss: 0.66290, acc: 0.92023, time: Sat Jan 22 22:23:33 2022
epoch 2 training:  29%|██▊       | 56048/196351 [7:58:27<19:43:31,  1.98it/s]global step 252400, epoch: 2, batch: 56049, loss: 1.93561, acc: 0.92028, time: Sat Jan 22 22:24:24 2022
epoch 2 training:  29%|██▊       | 56148/196351 [7:59:18<19:55:02,  1.96it/s]global step 252500, epoch: 2, batch: 56149, loss: 1.93047, acc: 0.92030, time: Sat Jan 22 22:25:16 2022
epoch 2 training:  29%|██▊       | 56248/196351 [8:00:10<19:51:18,  1.96it/s]global step 252600, epoch: 2, batch: 56249, loss: 1.02995, acc: 0.92026, time: Sat Jan 22 22:26:07 2022
epoch 2 training:  29%|██▊       | 56348/196351 [8:01:01<20:05:40,  1.94it/s]global step 252700, epoch: 2, batch: 56349, loss: 1.07544, acc: 0.92032, time: Sat Jan 22 22:26:58 2022
epoch 2 training:  29%|██▊       | 56448/196351 [8:01:52<20:09:38,  1.93it/s]global step 252800, epoch: 2, batch: 56449, loss: 0.56214, acc: 0.92032, time: Sat Jan 22 22:27:50 2022
epoch 2 training:  29%|██▉       | 56548/196351 [8:02:44<19:50:08,  1.96it/s]global step 252900, epoch: 2, batch: 56549, loss: 1.08093, acc: 0.92037, time: Sat Jan 22 22:28:41 2022
epoch 2 training:  29%|██▉       | 56648/196351 [8:03:35<19:53:19,  1.95it/s]global step 253000, epoch: 2, batch: 56649, loss: 1.19736, acc: 0.92036, time: Sat Jan 22 22:29:33 2022
epoch 2 training:  29%|██▉       | 56748/196351 [8:04:27<20:04:10,  1.93it/s]global step 253100, epoch: 2, batch: 56749, loss: 0.92111, acc: 0.92042, time: Sat Jan 22 22:30:24 2022
epoch 2 training:  29%|██▉       | 56848/196351 [8:05:18<19:42:36,  1.97it/s]global step 253200, epoch: 2, batch: 56849, loss: 1.78707, acc: 0.92046, time: Sat Jan 22 22:31:16 2022
epoch 2 training:  29%|██▉       | 56948/196351 [8:06:09<20:08:23,  1.92it/s]global step 253300, epoch: 2, batch: 56949, loss: 1.00545, acc: 0.92049, time: Sat Jan 22 22:32:07 2022
epoch 2 training:  29%|██▉       | 57048/196351 [8:07:00<19:49:41,  1.95it/s]global step 253400, epoch: 2, batch: 57049, loss: 1.00657, acc: 0.92048, time: Sat Jan 22 22:32:58 2022
epoch 2 training:  29%|██▉       | 57148/196351 [8:07:52<19:46:00,  1.96it/s]global step 253500, epoch: 2, batch: 57149, loss: 1.83831, acc: 0.92050, time: Sat Jan 22 22:33:49 2022
epoch 2 training:  29%|██▉       | 57248/196351 [8:08:43<20:02:55,  1.93it/s]global step 253600, epoch: 2, batch: 57249, loss: 0.90733, acc: 0.92051, time: Sat Jan 22 22:34:41 2022
epoch 2 training:  29%|██▉       | 57348/196351 [8:09:34<19:50:27,  1.95it/s]global step 253700, epoch: 2, batch: 57349, loss: 2.03434, acc: 0.92048, time: Sat Jan 22 22:35:32 2022
epoch 2 training:  29%|██▉       | 57448/196351 [8:10:26<20:07:39,  1.92it/s]global step 253800, epoch: 2, batch: 57449, loss: 2.37279, acc: 0.92053, time: Sat Jan 22 22:36:23 2022
epoch 2 training:  29%|██▉       | 57548/196351 [8:11:17<19:44:57,  1.95it/s]global step 253900, epoch: 2, batch: 57549, loss: 0.90343, acc: 0.92052, time: Sat Jan 22 22:37:14 2022
epoch 2 training:  29%|██▉       | 57648/196351 [8:12:08<19:47:03,  1.95it/s]global step 254000, epoch: 2, batch: 57649, loss: 1.47123, acc: 0.92054, time: Sat Jan 22 22:38:05 2022
epoch 2 training:  29%|██▉       | 57748/196351 [8:12:59<19:51:53,  1.94it/s]global step 254100, epoch: 2, batch: 57749, loss: 0.45834, acc: 0.92060, time: Sat Jan 22 22:38:57 2022
epoch 2 training:  29%|██▉       | 57848/196351 [8:13:50<19:50:44,  1.94it/s]global step 254200, epoch: 2, batch: 57849, loss: 0.74055, acc: 0.92063, time: Sat Jan 22 22:39:48 2022
epoch 2 training:  30%|██▉       | 57948/196351 [8:14:42<19:35:59,  1.96it/s]global step 254300, epoch: 2, batch: 57949, loss: 0.64046, acc: 0.92064, time: Sat Jan 22 22:40:39 2022
epoch 2 training:  30%|██▉       | 58048/196351 [8:15:33<19:34:12,  1.96it/s]global step 254400, epoch: 2, batch: 58049, loss: 4.61138, acc: 0.92064, time: Sat Jan 22 22:41:30 2022
epoch 2 training:  30%|██▉       | 58148/196351 [8:16:24<19:53:47,  1.93it/s]global step 254500, epoch: 2, batch: 58149, loss: 0.57063, acc: 0.92063, time: Sat Jan 22 22:42:22 2022
epoch 2 training:  30%|██▉       | 58248/196351 [8:17:16<19:39:42,  1.95it/s]global step 254600, epoch: 2, batch: 58249, loss: 2.29579, acc: 0.92063, time: Sat Jan 22 22:43:13 2022
epoch 2 training:  30%|██▉       | 58348/196351 [8:18:08<19:56:05,  1.92it/s]global step 254700, epoch: 2, batch: 58349, loss: 2.81158, acc: 0.92062, time: Sat Jan 22 22:44:05 2022
epoch 2 training:  30%|██▉       | 58448/196351 [8:18:59<19:36:34,  1.95it/s]global step 254800, epoch: 2, batch: 58449, loss: 0.50514, acc: 0.92061, time: Sat Jan 22 22:44:57 2022
epoch 2 training:  30%|██▉       | 58548/196351 [8:19:51<20:01:33,  1.91it/s]global step 254900, epoch: 2, batch: 58549, loss: 0.81084, acc: 0.92064, time: Sat Jan 22 22:45:48 2022
epoch 2 training:  30%|██▉       | 58648/196351 [8:20:42<19:09:52,  2.00it/s]global step 255000, epoch: 2, batch: 58649, loss: 3.28663, acc: 0.92063, time: Sat Jan 22 22:46:39 2022
epoch 2 training:  30%|██▉       | 58748/196351 [8:21:33<19:25:57,  1.97it/s]global step 255100, epoch: 2, batch: 58749, loss: 1.05090, acc: 0.92066, time: Sat Jan 22 22:47:30 2022
epoch 2 training:  30%|██▉       | 58848/196351 [8:22:24<19:17:16,  1.98it/s]global step 255200, epoch: 2, batch: 58849, loss: 1.32281, acc: 0.92067, time: Sat Jan 22 22:48:22 2022
epoch 2 training:  30%|███       | 58948/196351 [8:23:16<19:31:39,  1.95it/s]global step 255300, epoch: 2, batch: 58949, loss: 0.97179, acc: 0.92068, time: Sat Jan 22 22:49:13 2022
epoch 2 training:  30%|███       | 59048/196351 [8:24:07<19:23:55,  1.97it/s]global step 255400, epoch: 2, batch: 59049, loss: 2.26802, acc: 0.92068, time: Sat Jan 22 22:50:04 2022
epoch 2 training:  30%|███       | 59148/196351 [8:24:58<19:38:50,  1.94it/s]global step 255500, epoch: 2, batch: 59149, loss: 3.38742, acc: 0.92071, time: Sat Jan 22 22:50:56 2022
epoch 2 training:  30%|███       | 59248/196351 [8:25:50<19:23:49,  1.96it/s]global step 255600, epoch: 2, batch: 59249, loss: 0.22305, acc: 0.92075, time: Sat Jan 22 22:51:47 2022
epoch 2 training:  30%|███       | 59348/196351 [8:26:40<19:10:06,  1.99it/s]global step 255700, epoch: 2, batch: 59349, loss: 1.96677, acc: 0.92079, time: Sat Jan 22 22:52:38 2022
epoch 2 training:  30%|███       | 59448/196351 [8:27:31<19:38:01,  1.94it/s]global step 255800, epoch: 2, batch: 59449, loss: 1.43450, acc: 0.92081, time: Sat Jan 22 22:53:29 2022
epoch 2 training:  30%|███       | 59548/196351 [8:28:23<19:55:13,  1.91it/s]global step 255900, epoch: 2, batch: 59549, loss: 3.83869, acc: 0.92083, time: Sat Jan 22 22:54:20 2022
epoch 2 training:  30%|███       | 59648/196351 [8:29:14<19:22:00,  1.96it/s]global step 256000, epoch: 2, batch: 59649, loss: 0.61971, acc: 0.92085, time: Sat Jan 22 22:55:12 2022
epoch 2 training:  30%|███       | 59748/196351 [8:30:06<19:48:42,  1.92it/s]global step 256100, epoch: 2, batch: 59749, loss: 2.71643, acc: 0.92089, time: Sat Jan 22 22:56:03 2022
epoch 2 training:  30%|███       | 59848/196351 [8:30:57<19:27:45,  1.95it/s]global step 256200, epoch: 2, batch: 59849, loss: 0.50897, acc: 0.92093, time: Sat Jan 22 22:56:55 2022
epoch 2 training:  31%|███       | 59948/196351 [8:31:49<19:18:58,  1.96it/s]global step 256300, epoch: 2, batch: 59949, loss: 3.00447, acc: 0.92093, time: Sat Jan 22 22:57:46 2022
epoch 2 training:  31%|███       | 60048/196351 [8:32:40<20:02:11,  1.89it/s]global step 256400, epoch: 2, batch: 60049, loss: 0.70250, acc: 0.92089, time: Sat Jan 22 22:58:37 2022
epoch 2 training:  31%|███       | 60148/196351 [8:33:31<19:13:15,  1.97it/s]global step 256500, epoch: 2, batch: 60149, loss: 1.01988, acc: 0.92088, time: Sat Jan 22 22:59:29 2022
epoch 2 training:  31%|███       | 60248/196351 [8:34:22<19:11:28,  1.97it/s]global step 256600, epoch: 2, batch: 60249, loss: 2.21779, acc: 0.92089, time: Sat Jan 22 23:00:19 2022
epoch 2 training:  31%|███       | 60348/196351 [8:35:13<19:15:34,  1.96it/s]global step 256700, epoch: 2, batch: 60349, loss: 0.37062, acc: 0.92093, time: Sat Jan 22 23:01:11 2022
epoch 2 training:  31%|███       | 60448/196351 [8:36:04<19:13:18,  1.96it/s]global step 256800, epoch: 2, batch: 60449, loss: 4.77261, acc: 0.92088, time: Sat Jan 22 23:02:02 2022
epoch 2 training:  31%|███       | 60548/196351 [8:36:55<19:15:11,  1.96it/s]global step 256900, epoch: 2, batch: 60549, loss: 2.86779, acc: 0.92090, time: Sat Jan 22 23:02:53 2022
epoch 2 training:  31%|███       | 60648/196351 [8:37:47<19:54:11,  1.89it/s]global step 257000, epoch: 2, batch: 60649, loss: 0.82352, acc: 0.92091, time: Sat Jan 22 23:03:45 2022
epoch 2 training:  31%|███       | 60748/196351 [8:38:39<19:18:17,  1.95it/s]global step 257100, epoch: 2, batch: 60749, loss: 0.63619, acc: 0.92094, time: Sat Jan 22 23:04:36 2022
epoch 2 training:  31%|███       | 60848/196351 [8:39:30<19:23:39,  1.94it/s]global step 257200, epoch: 2, batch: 60849, loss: 0.97638, acc: 0.92094, time: Sat Jan 22 23:05:27 2022
epoch 2 training:  31%|███       | 60948/196351 [8:40:21<18:51:26,  1.99it/s]global step 257300, epoch: 2, batch: 60949, loss: 1.93724, acc: 0.92096, time: Sat Jan 22 23:06:19 2022
epoch 2 training:  31%|███       | 61048/196351 [8:41:12<18:57:40,  1.98it/s]global step 257400, epoch: 2, batch: 61049, loss: 3.20341, acc: 0.92098, time: Sat Jan 22 23:07:09 2022
epoch 2 training:  31%|███       | 61148/196351 [8:42:03<19:22:57,  1.94it/s]global step 257500, epoch: 2, batch: 61149, loss: 1.21840, acc: 0.92100, time: Sat Jan 22 23:08:01 2022
epoch 2 training:  31%|███       | 61248/196351 [8:42:55<18:52:56,  1.99it/s]global step 257600, epoch: 2, batch: 61249, loss: 3.78614, acc: 0.92104, time: Sat Jan 22 23:08:52 2022
epoch 2 training:  31%|███       | 61348/196351 [8:43:46<19:12:47,  1.95it/s]global step 257700, epoch: 2, batch: 61349, loss: 0.09284, acc: 0.92106, time: Sat Jan 22 23:09:43 2022
epoch 2 training:  31%|███▏      | 61448/196351 [8:44:37<19:26:16,  1.93it/s]global step 257800, epoch: 2, batch: 61449, loss: 0.37400, acc: 0.92110, time: Sat Jan 22 23:10:35 2022
epoch 2 training:  31%|███▏      | 61548/196351 [8:45:29<19:24:25,  1.93it/s]global step 257900, epoch: 2, batch: 61549, loss: 1.06411, acc: 0.92110, time: Sat Jan 22 23:11:26 2022
epoch 2 training:  31%|███▏      | 61648/196351 [8:46:20<19:18:57,  1.94it/s]global step 258000, epoch: 2, batch: 61649, loss: 0.84712, acc: 0.92113, time: Sat Jan 22 23:12:18 2022
epoch 2 training:  31%|███▏      | 61748/196351 [8:47:12<19:19:21,  1.94it/s]global step 258100, epoch: 2, batch: 61749, loss: 0.73239, acc: 0.92114, time: Sat Jan 22 23:13:09 2022
epoch 2 training:  31%|███▏      | 61848/196351 [8:48:03<18:52:32,  1.98it/s]global step 258200, epoch: 2, batch: 61849, loss: 0.44316, acc: 0.92113, time: Sat Jan 22 23:14:01 2022
epoch 2 training:  32%|███▏      | 61948/196351 [8:48:55<19:08:50,  1.95it/s]global step 258300, epoch: 2, batch: 61949, loss: 1.14527, acc: 0.92116, time: Sat Jan 22 23:14:52 2022
epoch 2 training:  32%|███▏      | 62048/196351 [8:49:46<19:09:42,  1.95it/s]global step 258400, epoch: 2, batch: 62049, loss: 0.61333, acc: 0.92118, time: Sat Jan 22 23:15:43 2022
epoch 2 training:  32%|███▏      | 62148/196351 [8:50:37<18:57:17,  1.97it/s]global step 258500, epoch: 2, batch: 62149, loss: 2.06711, acc: 0.92120, time: Sat Jan 22 23:16:35 2022
epoch 2 training:  32%|███▏      | 62248/196351 [8:51:29<18:52:08,  1.97it/s]global step 258600, epoch: 2, batch: 62249, loss: 1.22851, acc: 0.92119, time: Sat Jan 22 23:17:26 2022
epoch 2 training:  32%|███▏      | 62348/196351 [8:52:20<18:55:58,  1.97it/s]global step 258700, epoch: 2, batch: 62349, loss: 0.06618, acc: 0.92121, time: Sat Jan 22 23:18:18 2022
epoch 2 training:  32%|███▏      | 62448/196351 [8:53:12<18:36:08,  2.00it/s]global step 258800, epoch: 2, batch: 62449, loss: 0.81306, acc: 0.92122, time: Sat Jan 22 23:19:09 2022
epoch 2 training:  32%|███▏      | 62548/196351 [8:54:03<18:58:03,  1.96it/s]global step 258900, epoch: 2, batch: 62549, loss: 1.57164, acc: 0.92128, time: Sat Jan 22 23:20:01 2022
epoch 2 training:  32%|███▏      | 62648/196351 [8:54:54<18:56:32,  1.96it/s]global step 259000, epoch: 2, batch: 62649, loss: 3.11623, acc: 0.92128, time: Sat Jan 22 23:20:52 2022
epoch 2 training:  32%|███▏      | 62748/196351 [8:55:46<18:53:42,  1.96it/s]global step 259100, epoch: 2, batch: 62749, loss: 2.86847, acc: 0.92129, time: Sat Jan 22 23:21:43 2022
epoch 2 training:  32%|███▏      | 62848/196351 [8:56:37<18:57:12,  1.96it/s]global step 259200, epoch: 2, batch: 62849, loss: 1.44051, acc: 0.92131, time: Sat Jan 22 23:22:34 2022
epoch 2 training:  32%|███▏      | 62948/196351 [8:57:28<18:46:38,  1.97it/s]global step 259300, epoch: 2, batch: 62949, loss: 1.44196, acc: 0.92136, time: Sat Jan 22 23:23:25 2022
epoch 2 training:  32%|███▏      | 63048/196351 [8:58:19<18:56:55,  1.95it/s]global step 259400, epoch: 2, batch: 63049, loss: 2.71599, acc: 0.92139, time: Sat Jan 22 23:24:16 2022
epoch 2 training:  32%|███▏      | 63148/196351 [8:59:10<18:59:37,  1.95it/s]global step 259500, epoch: 2, batch: 63149, loss: 2.42834, acc: 0.92145, time: Sat Jan 22 23:25:07 2022
epoch 2 training:  32%|███▏      | 63248/196351 [9:00:02<19:07:15,  1.93it/s]global step 259600, epoch: 2, batch: 63249, loss: 1.20309, acc: 0.92141, time: Sat Jan 22 23:25:59 2022
epoch 2 training:  32%|███▏      | 63348/196351 [9:00:53<18:50:20,  1.96it/s]global step 259700, epoch: 2, batch: 63349, loss: 2.14836, acc: 0.92142, time: Sat Jan 22 23:26:51 2022
epoch 2 training:  32%|███▏      | 63448/196351 [9:01:45<19:10:44,  1.92it/s]global step 259800, epoch: 2, batch: 63449, loss: 0.25728, acc: 0.92144, time: Sat Jan 22 23:27:42 2022
epoch 2 training:  32%|███▏      | 63548/196351 [9:02:37<19:12:29,  1.92it/s]global step 259900, epoch: 2, batch: 63549, loss: 2.16996, acc: 0.92150, time: Sat Jan 22 23:28:34 2022
epoch 2 training:  32%|███▏      | 63648/196351 [9:03:28<18:47:30,  1.96it/s]global step 260000, epoch: 2, batch: 63649, loss: 1.82340, acc: 0.92151, time: Sat Jan 22 23:29:26 2022
epoch 2 training:  32%|███▏      | 63748/196351 [9:04:20<18:44:56,  1.96it/s]global step 260100, epoch: 2, batch: 63749, loss: 0.77848, acc: 0.92156, time: Sat Jan 22 23:30:17 2022
epoch 2 training:  33%|███▎      | 63848/196351 [9:05:11<18:41:31,  1.97it/s]global step 260200, epoch: 2, batch: 63849, loss: 1.39030, acc: 0.92161, time: Sat Jan 22 23:31:09 2022
epoch 2 training:  33%|███▎      | 63948/196351 [9:06:02<18:53:59,  1.95it/s]global step 260300, epoch: 2, batch: 63949, loss: 0.98039, acc: 0.92162, time: Sat Jan 22 23:32:00 2022
epoch 2 training:  33%|███▎      | 64048/196351 [9:06:54<18:50:00,  1.95it/s]global step 260400, epoch: 2, batch: 64049, loss: 1.37544, acc: 0.92163, time: Sat Jan 22 23:32:51 2022
epoch 2 training:  33%|███▎      | 64148/196351 [9:07:45<18:54:26,  1.94it/s]global step 260500, epoch: 2, batch: 64149, loss: 0.59984, acc: 0.92164, time: Sat Jan 22 23:33:42 2022
epoch 2 training:  33%|███▎      | 64248/196351 [9:08:36<19:06:11,  1.92it/s]global step 260600, epoch: 2, batch: 64249, loss: 1.47841, acc: 0.92163, time: Sat Jan 22 23:34:34 2022
epoch 2 training:  33%|███▎      | 64348/196351 [9:09:28<18:47:26,  1.95it/s]global step 260700, epoch: 2, batch: 64349, loss: 1.63535, acc: 0.92164, time: Sat Jan 22 23:35:25 2022
epoch 2 training:  33%|███▎      | 64448/196351 [9:10:19<18:41:41,  1.96it/s]global step 260800, epoch: 2, batch: 64449, loss: 2.08430, acc: 0.92163, time: Sat Jan 22 23:36:16 2022
epoch 2 training:  33%|███▎      | 64548/196351 [9:11:11<18:55:30,  1.93it/s]global step 260900, epoch: 2, batch: 64549, loss: 3.68496, acc: 0.92166, time: Sat Jan 22 23:37:08 2022
epoch 2 training:  33%|███▎      | 64648/196351 [9:12:02<18:50:09,  1.94it/s]global step 261000, epoch: 2, batch: 64649, loss: 1.39937, acc: 0.92162, time: Sat Jan 22 23:38:00 2022
epoch 2 training:  33%|███▎      | 64748/196351 [9:12:54<18:37:21,  1.96it/s]global step 261100, epoch: 2, batch: 64749, loss: 1.15400, acc: 0.92164, time: Sat Jan 22 23:38:51 2022
epoch 2 training:  33%|███▎      | 64848/196351 [9:13:45<18:47:44,  1.94it/s]global step 261200, epoch: 2, batch: 64849, loss: 0.77586, acc: 0.92166, time: Sat Jan 22 23:39:43 2022
epoch 2 training:  33%|███▎      | 64948/196351 [9:14:37<19:02:14,  1.92it/s]global step 261300, epoch: 2, batch: 64949, loss: 1.35566, acc: 0.92168, time: Sat Jan 22 23:40:35 2022
epoch 2 training:  33%|███▎      | 65048/196351 [9:15:29<18:45:22,  1.94it/s]global step 261400, epoch: 2, batch: 65049, loss: 1.80902, acc: 0.92166, time: Sat Jan 22 23:41:26 2022
epoch 2 training:  33%|███▎      | 65148/196351 [9:16:21<18:30:48,  1.97it/s]global step 261500, epoch: 2, batch: 65149, loss: 1.94397, acc: 0.92169, time: Sat Jan 22 23:42:18 2022
epoch 2 training:  33%|███▎      | 65248/196351 [9:17:12<18:25:12,  1.98it/s]global step 261600, epoch: 2, batch: 65249, loss: 1.75988, acc: 0.92173, time: Sat Jan 22 23:43:10 2022
epoch 2 training:  33%|███▎      | 65348/196351 [9:18:03<18:32:25,  1.96it/s]global step 261700, epoch: 2, batch: 65349, loss: 3.23183, acc: 0.92177, time: Sat Jan 22 23:44:01 2022
epoch 2 training:  33%|███▎      | 65448/196351 [9:18:55<18:37:39,  1.95it/s]global step 261800, epoch: 2, batch: 65449, loss: 0.72172, acc: 0.92182, time: Sat Jan 22 23:44:52 2022
epoch 2 training:  33%|███▎      | 65548/196351 [9:19:46<18:37:01,  1.95it/s]global step 261900, epoch: 2, batch: 65549, loss: 1.66723, acc: 0.92181, time: Sat Jan 22 23:45:44 2022
epoch 2 training:  33%|███▎      | 65648/196351 [9:20:38<18:31:54,  1.96it/s]global step 262000, epoch: 2, batch: 65649, loss: 0.76057, acc: 0.92183, time: Sat Jan 22 23:46:35 2022
epoch 2 training:  33%|███▎      | 65748/196351 [9:21:29<18:26:26,  1.97it/s]global step 262100, epoch: 2, batch: 65749, loss: 0.54358, acc: 0.92182, time: Sat Jan 22 23:47:26 2022
epoch 2 training:  34%|███▎      | 65848/196351 [9:22:20<18:24:26,  1.97it/s]global step 262200, epoch: 2, batch: 65849, loss: 1.32778, acc: 0.92177, time: Sat Jan 22 23:48:17 2022
epoch 2 training:  34%|███▎      | 65948/196351 [9:23:11<18:30:48,  1.96it/s]global step 262300, epoch: 2, batch: 65949, loss: 1.06185, acc: 0.92177, time: Sat Jan 22 23:49:09 2022
epoch 2 training:  34%|███▎      | 66048/196351 [9:24:02<18:16:59,  1.98it/s]global step 262400, epoch: 2, batch: 66049, loss: 0.82012, acc: 0.92181, time: Sat Jan 22 23:50:00 2022
epoch 2 training:  34%|███▎      | 66148/196351 [9:24:53<18:07:47,  1.99it/s]global step 262500, epoch: 2, batch: 66149, loss: 1.08357, acc: 0.92184, time: Sat Jan 22 23:50:51 2022
epoch 2 training:  34%|███▎      | 66248/196351 [9:25:44<18:12:36,  1.98it/s]global step 262600, epoch: 2, batch: 66249, loss: 1.18680, acc: 0.92185, time: Sat Jan 22 23:51:42 2022
epoch 2 training:  34%|███▍      | 66348/196351 [9:26:35<18:14:45,  1.98it/s]global step 262700, epoch: 2, batch: 66349, loss: 1.01760, acc: 0.92185, time: Sat Jan 22 23:52:33 2022
epoch 2 training:  34%|███▍      | 66448/196351 [9:27:26<18:14:33,  1.98it/s]global step 262800, epoch: 2, batch: 66449, loss: 1.97526, acc: 0.92186, time: Sat Jan 22 23:53:24 2022
epoch 2 training:  34%|███▍      | 66548/196351 [9:28:18<18:17:52,  1.97it/s]global step 262900, epoch: 2, batch: 66549, loss: 2.99696, acc: 0.92188, time: Sat Jan 22 23:54:15 2022
epoch 2 training:  34%|███▍      | 66648/196351 [9:29:09<18:33:34,  1.94it/s]global step 263000, epoch: 2, batch: 66649, loss: 0.90913, acc: 0.92189, time: Sat Jan 22 23:55:06 2022
epoch 2 training:  34%|███▍      | 66748/196351 [9:30:00<18:15:46,  1.97it/s]global step 263100, epoch: 2, batch: 66749, loss: 1.41198, acc: 0.92188, time: Sat Jan 22 23:55:57 2022
epoch 2 training:  34%|███▍      | 66848/196351 [9:30:51<18:33:17,  1.94it/s]global step 263200, epoch: 2, batch: 66849, loss: 1.24109, acc: 0.92181, time: Sat Jan 22 23:56:48 2022
epoch 2 training:  34%|███▍      | 66948/196351 [9:31:42<18:41:07,  1.92it/s]global step 263300, epoch: 2, batch: 66949, loss: 0.90420, acc: 0.92180, time: Sat Jan 22 23:57:39 2022
epoch 2 training:  34%|███▍      | 67048/196351 [9:32:33<18:26:03,  1.95it/s]global step 263400, epoch: 2, batch: 67049, loss: 1.87486, acc: 0.92182, time: Sat Jan 22 23:58:30 2022
epoch 2 training:  34%|███▍      | 67148/196351 [9:33:24<18:18:18,  1.96it/s]global step 263500, epoch: 2, batch: 67149, loss: 2.57532, acc: 0.92186, time: Sat Jan 22 23:59:21 2022
epoch 2 training:  34%|███▍      | 67248/196351 [9:34:15<18:41:02,  1.92it/s]global step 263600, epoch: 2, batch: 67249, loss: 0.48787, acc: 0.92189, time: Sun Jan 23 00:00:13 2022
epoch 2 training:  34%|███▍      | 67348/196351 [9:35:08<18:31:54,  1.93it/s]global step 263700, epoch: 2, batch: 67349, loss: 2.06963, acc: 0.92185, time: Sun Jan 23 00:01:05 2022
epoch 2 training:  34%|███▍      | 67448/196351 [9:35:59<18:22:02,  1.95it/s]global step 263800, epoch: 2, batch: 67449, loss: 1.68478, acc: 0.92182, time: Sun Jan 23 00:01:56 2022
epoch 2 training:  34%|███▍      | 67548/196351 [9:36:50<18:40:47,  1.92it/s]global step 263900, epoch: 2, batch: 67549, loss: 2.01331, acc: 0.92180, time: Sun Jan 23 00:02:48 2022
epoch 2 training:  34%|███▍      | 67648/196351 [9:37:42<18:54:49,  1.89it/s]global step 264000, epoch: 2, batch: 67649, loss: 4.68254, acc: 0.92182, time: Sun Jan 23 00:03:39 2022
epoch 2 training:  35%|███▍      | 67748/196351 [9:38:33<18:06:47,  1.97it/s]global step 264100, epoch: 2, batch: 67749, loss: 0.38688, acc: 0.92181, time: Sun Jan 23 00:04:31 2022
epoch 2 training:  35%|███▍      | 67848/196351 [9:39:25<18:07:29,  1.97it/s]global step 264200, epoch: 2, batch: 67849, loss: 2.25561, acc: 0.92178, time: Sun Jan 23 00:05:22 2022
epoch 2 training:  35%|███▍      | 67948/196351 [9:40:16<18:18:47,  1.95it/s]global step 264300, epoch: 2, batch: 67949, loss: 0.43433, acc: 0.92182, time: Sun Jan 23 00:06:13 2022
epoch 2 training:  35%|███▍      | 68048/196351 [9:41:07<18:20:33,  1.94it/s]global step 264400, epoch: 2, batch: 68049, loss: 0.72047, acc: 0.92178, time: Sun Jan 23 00:07:04 2022
epoch 2 training:  35%|███▍      | 68148/196351 [9:41:58<17:59:38,  1.98it/s]global step 264500, epoch: 2, batch: 68149, loss: 0.79346, acc: 0.92177, time: Sun Jan 23 00:07:55 2022
epoch 2 training:  35%|███▍      | 68248/196351 [9:42:49<18:06:39,  1.96it/s]global step 264600, epoch: 2, batch: 68249, loss: 1.37811, acc: 0.92176, time: Sun Jan 23 00:08:47 2022
epoch 2 training:  35%|███▍      | 68348/196351 [9:43:41<18:06:27,  1.96it/s]global step 264700, epoch: 2, batch: 68349, loss: 2.48326, acc: 0.92174, time: Sun Jan 23 00:09:38 2022
epoch 2 training:  35%|███▍      | 68448/196351 [9:44:32<18:30:52,  1.92it/s]global step 264800, epoch: 2, batch: 68449, loss: 1.90268, acc: 0.92179, time: Sun Jan 23 00:10:30 2022
epoch 2 training:  35%|███▍      | 68548/196351 [9:45:23<17:52:59,  1.99it/s]global step 264900, epoch: 2, batch: 68549, loss: 0.61830, acc: 0.92178, time: Sun Jan 23 00:11:21 2022
epoch 2 training:  35%|███▍      | 68648/196351 [9:46:14<17:59:47,  1.97it/s]global step 265000, epoch: 2, batch: 68649, loss: 0.47612, acc: 0.92182, time: Sun Jan 23 00:12:12 2022
epoch 2 training:  35%|███▌      | 68748/196351 [9:47:06<18:07:41,  1.96it/s]global step 265100, epoch: 2, batch: 68749, loss: 1.15549, acc: 0.92184, time: Sun Jan 23 00:13:03 2022
epoch 2 training:  35%|███▌      | 68848/196351 [9:47:57<17:51:28,  1.98it/s]global step 265200, epoch: 2, batch: 68849, loss: 1.39427, acc: 0.92187, time: Sun Jan 23 00:13:55 2022
epoch 2 training:  35%|███▌      | 68948/196351 [9:48:49<18:21:36,  1.93it/s]global step 265300, epoch: 2, batch: 68949, loss: 1.88104, acc: 0.92188, time: Sun Jan 23 00:14:46 2022
epoch 2 training:  35%|███▌      | 69048/196351 [9:49:40<18:06:24,  1.95it/s]global step 265400, epoch: 2, batch: 69049, loss: 4.97002, acc: 0.92190, time: Sun Jan 23 00:15:38 2022
epoch 2 training:  35%|███▌      | 69148/196351 [9:50:31<18:17:37,  1.93it/s]global step 265500, epoch: 2, batch: 69149, loss: 0.46571, acc: 0.92193, time: Sun Jan 23 00:16:29 2022
epoch 2 training:  35%|███▌      | 69248/196351 [9:51:24<18:45:07,  1.88it/s]global step 265600, epoch: 2, batch: 69249, loss: 0.60613, acc: 0.92194, time: Sun Jan 23 00:17:21 2022
epoch 2 training:  35%|███▌      | 69348/196351 [9:52:15<18:04:51,  1.95it/s]global step 265700, epoch: 2, batch: 69349, loss: 0.73839, acc: 0.92199, time: Sun Jan 23 00:18:13 2022
epoch 2 training:  35%|███▌      | 69448/196351 [9:53:06<17:53:15,  1.97it/s]global step 265800, epoch: 2, batch: 69449, loss: 0.74199, acc: 0.92200, time: Sun Jan 23 00:19:04 2022
epoch 2 training:  35%|███▌      | 69548/196351 [9:53:57<17:57:19,  1.96it/s]global step 265900, epoch: 2, batch: 69549, loss: 2.09038, acc: 0.92200, time: Sun Jan 23 00:19:55 2022
epoch 2 training:  35%|███▌      | 69648/196351 [9:54:49<18:00:37,  1.95it/s]global step 266000, epoch: 2, batch: 69649, loss: 0.09271, acc: 0.92199, time: Sun Jan 23 00:20:46 2022
epoch 2 training:  36%|███▌      | 69748/196351 [9:55:40<18:18:46,  1.92it/s]global step 266100, epoch: 2, batch: 69749, loss: 0.43086, acc: 0.92201, time: Sun Jan 23 00:21:38 2022
epoch 2 training:  36%|███▌      | 69848/196351 [9:56:32<18:04:20,  1.94it/s]global step 266200, epoch: 2, batch: 69849, loss: 2.38745, acc: 0.92202, time: Sun Jan 23 00:22:29 2022
epoch 2 training:  36%|███▌      | 69948/196351 [9:57:23<18:05:39,  1.94it/s]global step 266300, epoch: 2, batch: 69949, loss: 1.44990, acc: 0.92205, time: Sun Jan 23 00:23:21 2022
epoch 2 training:  36%|███▌      | 70048/196351 [9:58:15<17:59:29,  1.95it/s]global step 266400, epoch: 2, batch: 70049, loss: 3.20903, acc: 0.92209, time: Sun Jan 23 00:24:12 2022
epoch 2 training:  36%|███▌      | 70148/196351 [9:59:07<18:05:00,  1.94it/s]global step 266500, epoch: 2, batch: 70149, loss: 1.46650, acc: 0.92213, time: Sun Jan 23 00:25:04 2022
epoch 2 training:  36%|███▌      | 70248/196351 [9:59:59<18:09:48,  1.93it/s]global step 266600, epoch: 2, batch: 70249, loss: 1.82417, acc: 0.92214, time: Sun Jan 23 00:25:56 2022
epoch 2 training:  36%|███▌      | 70348/196351 [10:00:50<17:57:48,  1.95it/s]global step 266700, epoch: 2, batch: 70349, loss: 1.77682, acc: 0.92212, time: Sun Jan 23 00:26:47 2022
epoch 2 training:  36%|███▌      | 70448/196351 [10:01:41<18:02:58,  1.94it/s]global step 266800, epoch: 2, batch: 70449, loss: 0.79134, acc: 0.92213, time: Sun Jan 23 00:27:39 2022
epoch 2 training:  36%|███▌      | 70548/196351 [10:02:32<17:51:41,  1.96it/s]global step 266900, epoch: 2, batch: 70549, loss: 1.25950, acc: 0.92217, time: Sun Jan 23 00:28:30 2022
epoch 2 training:  36%|███▌      | 70648/196351 [10:03:23<17:50:58,  1.96it/s]global step 267000, epoch: 2, batch: 70649, loss: 0.82417, acc: 0.92218, time: Sun Jan 23 00:29:21 2022
epoch 2 training:  36%|███▌      | 70748/196351 [10:04:15<18:03:28,  1.93it/s]global step 267100, epoch: 2, batch: 70749, loss: 3.05501, acc: 0.92215, time: Sun Jan 23 00:30:12 2022
epoch 2 training:  36%|███▌      | 70848/196351 [10:05:06<17:55:30,  1.94it/s]global step 267200, epoch: 2, batch: 70849, loss: 2.87132, acc: 0.92215, time: Sun Jan 23 00:31:03 2022
epoch 2 training:  36%|███▌      | 70948/196351 [10:05:57<18:01:43,  1.93it/s]global step 267300, epoch: 2, batch: 70949, loss: 1.40759, acc: 0.92218, time: Sun Jan 23 00:31:54 2022
epoch 2 training:  36%|███▌      | 71048/196351 [10:06:48<17:53:29,  1.95it/s]global step 267400, epoch: 2, batch: 71049, loss: 1.05221, acc: 0.92217, time: Sun Jan 23 00:32:45 2022
epoch 2 training:  36%|███▌      | 71148/196351 [10:07:39<17:57:33,  1.94it/s]global step 267500, epoch: 2, batch: 71149, loss: 4.06139, acc: 0.92213, time: Sun Jan 23 00:33:37 2022
epoch 2 training:  36%|███▋      | 71248/196351 [10:08:32<17:47:30,  1.95it/s]global step 267600, epoch: 2, batch: 71249, loss: 2.82819, acc: 0.92210, time: Sun Jan 23 00:34:29 2022
epoch 2 training:  36%|███▋      | 71348/196351 [10:09:23<17:48:48,  1.95it/s]global step 267700, epoch: 2, batch: 71349, loss: 3.91808, acc: 0.92213, time: Sun Jan 23 00:35:21 2022
epoch 2 training:  36%|███▋      | 71448/196351 [10:10:14<17:45:01,  1.95it/s]global step 267800, epoch: 2, batch: 71449, loss: 1.97241, acc: 0.92218, time: Sun Jan 23 00:36:12 2022
epoch 2 training:  36%|███▋      | 71548/196351 [10:11:06<17:55:04,  1.93it/s]global step 267900, epoch: 2, batch: 71549, loss: 0.58479, acc: 0.92218, time: Sun Jan 23 00:37:04 2022
epoch 2 training:  36%|███▋      | 71648/196351 [10:11:58<18:02:19,  1.92it/s]global step 268000, epoch: 2, batch: 71649, loss: 0.42339, acc: 0.92220, time: Sun Jan 23 00:37:55 2022
epoch 2 training:  37%|███▋      | 71748/196351 [10:12:49<17:51:34,  1.94it/s]global step 268100, epoch: 2, batch: 71749, loss: 1.07304, acc: 0.92219, time: Sun Jan 23 00:38:47 2022
epoch 2 training:  37%|███▋      | 71848/196351 [10:13:41<17:40:26,  1.96it/s]global step 268200, epoch: 2, batch: 71849, loss: 3.06526, acc: 0.92223, time: Sun Jan 23 00:39:38 2022
epoch 2 training:  37%|███▋      | 71948/196351 [10:14:32<17:48:00,  1.94it/s]global step 268300, epoch: 2, batch: 71949, loss: 2.09658, acc: 0.92226, time: Sun Jan 23 00:40:29 2022
epoch 2 training:  37%|███▋      | 72048/196351 [10:15:23<17:48:58,  1.94it/s]global step 268400, epoch: 2, batch: 72049, loss: 0.79098, acc: 0.92225, time: Sun Jan 23 00:41:21 2022
epoch 2 training:  37%|███▋      | 72148/196351 [10:16:15<17:36:38,  1.96it/s]global step 268500, epoch: 2, batch: 72149, loss: 0.28378, acc: 0.92226, time: Sun Jan 23 00:42:12 2022
epoch 2 training:  37%|███▋      | 72248/196351 [10:17:06<17:40:20,  1.95it/s]global step 268600, epoch: 2, batch: 72249, loss: 1.24033, acc: 0.92224, time: Sun Jan 23 00:43:04 2022
epoch 2 training:  37%|███▋      | 72348/196351 [10:17:57<17:38:30,  1.95it/s]global step 268700, epoch: 2, batch: 72349, loss: 2.93939, acc: 0.92226, time: Sun Jan 23 00:43:55 2022
epoch 2 training:  37%|███▋      | 72448/196351 [10:18:48<17:36:49,  1.95it/s]global step 268800, epoch: 2, batch: 72449, loss: 1.20956, acc: 0.92225, time: Sun Jan 23 00:44:46 2022
epoch 2 training:  37%|███▋      | 72548/196351 [10:19:40<17:36:28,  1.95it/s]global step 268900, epoch: 2, batch: 72549, loss: 1.16344, acc: 0.92228, time: Sun Jan 23 00:45:37 2022
epoch 2 training:  37%|███▋      | 72648/196351 [10:20:31<17:25:24,  1.97it/s]global step 269000, epoch: 2, batch: 72649, loss: 0.82915, acc: 0.92229, time: Sun Jan 23 00:46:28 2022
epoch 2 training:  37%|███▋      | 72748/196351 [10:21:22<17:33:24,  1.96it/s]global step 269100, epoch: 2, batch: 72749, loss: 0.20903, acc: 0.92230, time: Sun Jan 23 00:47:19 2022
epoch 2 training:  37%|███▋      | 72848/196351 [10:22:13<17:37:52,  1.95it/s]global step 269200, epoch: 2, batch: 72849, loss: 1.61516, acc: 0.92229, time: Sun Jan 23 00:48:10 2022
epoch 2 training:  37%|███▋      | 72948/196351 [10:23:04<17:20:14,  1.98it/s]global step 269300, epoch: 2, batch: 72949, loss: 1.99197, acc: 0.92230, time: Sun Jan 23 00:49:01 2022
epoch 2 training:  37%|███▋      | 73048/196351 [10:23:55<17:26:46,  1.96it/s]global step 269400, epoch: 2, batch: 73049, loss: 0.99728, acc: 0.92229, time: Sun Jan 23 00:49:52 2022
epoch 2 training:  37%|███▋      | 73148/196351 [10:24:46<17:38:13,  1.94it/s]global step 269500, epoch: 2, batch: 73149, loss: 0.35361, acc: 0.92226, time: Sun Jan 23 00:50:43 2022
epoch 2 training:  37%|███▋      | 73248/196351 [10:25:37<17:37:17,  1.94it/s]global step 269600, epoch: 2, batch: 73249, loss: 0.65091, acc: 0.92228, time: Sun Jan 23 00:51:35 2022
epoch 2 training:  37%|███▋      | 73348/196351 [10:26:29<17:33:21,  1.95it/s]global step 269700, epoch: 2, batch: 73349, loss: 1.99810, acc: 0.92228, time: Sun Jan 23 00:52:26 2022
epoch 2 training:  37%|███▋      | 73448/196351 [10:27:20<17:10:02,  1.99it/s]global step 269800, epoch: 2, batch: 73449, loss: 1.30729, acc: 0.92232, time: Sun Jan 23 00:53:17 2022
epoch 2 training:  37%|███▋      | 73548/196351 [10:28:11<17:35:52,  1.94it/s]global step 269900, epoch: 2, batch: 73549, loss: 0.80322, acc: 0.92233, time: Sun Jan 23 00:54:09 2022
epoch 2 training:  38%|███▊      | 73648/196351 [10:29:03<17:30:36,  1.95it/s]global step 270000, epoch: 2, batch: 73649, loss: 0.83317, acc: 0.92234, time: Sun Jan 23 00:55:00 2022
epoch 2 training:  38%|███▊      | 73748/196351 [10:29:54<17:41:05,  1.93it/s]global step 270100, epoch: 2, batch: 73749, loss: 3.79196, acc: 0.92237, time: Sun Jan 23 00:55:52 2022
epoch 2 training:  38%|███▊      | 73848/196351 [10:30:46<17:24:31,  1.95it/s]global step 270200, epoch: 2, batch: 73849, loss: 2.07307, acc: 0.92239, time: Sun Jan 23 00:56:43 2022
epoch 2 training:  38%|███▊      | 73948/196351 [10:31:37<17:55:05,  1.90it/s]global step 270300, epoch: 2, batch: 73949, loss: 3.25446, acc: 0.92241, time: Sun Jan 23 00:57:35 2022
epoch 2 training:  38%|███▊      | 74048/196351 [10:32:29<17:39:39,  1.92it/s]global step 270400, epoch: 2, batch: 74049, loss: 0.45419, acc: 0.92240, time: Sun Jan 23 00:58:27 2022
epoch 2 training:  38%|███▊      | 74148/196351 [10:33:21<17:41:34,  1.92it/s]global step 270500, epoch: 2, batch: 74149, loss: 1.17623, acc: 0.92242, time: Sun Jan 23 00:59:18 2022
epoch 2 training:  38%|███▊      | 74248/196351 [10:34:12<17:30:14,  1.94it/s]global step 270600, epoch: 2, batch: 74249, loss: 1.50858, acc: 0.92242, time: Sun Jan 23 01:00:10 2022
epoch 2 training:  38%|███▊      | 74348/196351 [10:35:04<17:54:34,  1.89it/s]global step 270700, epoch: 2, batch: 74349, loss: 1.09943, acc: 0.92247, time: Sun Jan 23 01:01:01 2022
epoch 2 training:  38%|███▊      | 74448/196351 [10:35:55<17:27:37,  1.94it/s]global step 270800, epoch: 2, batch: 74449, loss: 2.96869, acc: 0.92248, time: Sun Jan 23 01:01:53 2022
epoch 2 training:  38%|███▊      | 74548/196351 [10:36:47<17:16:25,  1.96it/s]global step 270900, epoch: 2, batch: 74549, loss: 1.21184, acc: 0.92251, time: Sun Jan 23 01:02:44 2022
epoch 2 training:  38%|███▊      | 74648/196351 [10:37:38<17:39:43,  1.91it/s]global step 271000, epoch: 2, batch: 74649, loss: 0.14452, acc: 0.92254, time: Sun Jan 23 01:03:35 2022
epoch 2 training:  38%|███▊      | 74748/196351 [10:38:30<17:10:15,  1.97it/s]global step 271100, epoch: 2, batch: 74749, loss: 1.30259, acc: 0.92255, time: Sun Jan 23 01:04:27 2022
epoch 2 training:  38%|███▊      | 74848/196351 [10:39:21<17:08:52,  1.97it/s]global step 271200, epoch: 2, batch: 74849, loss: 3.66710, acc: 0.92261, time: Sun Jan 23 01:05:18 2022
epoch 2 training:  38%|███▊      | 74948/196351 [10:40:12<17:11:52,  1.96it/s]global step 271300, epoch: 2, batch: 74949, loss: 0.24860, acc: 0.92262, time: Sun Jan 23 01:06:09 2022
epoch 2 training:  38%|███▊      | 75048/196351 [10:41:04<17:51:37,  1.89it/s]global step 271400, epoch: 2, batch: 75049, loss: 0.73914, acc: 0.92266, time: Sun Jan 23 01:07:01 2022
epoch 2 training:  38%|███▊      | 75148/196351 [10:41:55<17:19:59,  1.94it/s]global step 271500, epoch: 2, batch: 75149, loss: 0.70977, acc: 0.92270, time: Sun Jan 23 01:07:53 2022
epoch 2 training:  38%|███▊      | 75248/196351 [10:42:47<17:36:14,  1.91it/s]global step 271600, epoch: 2, batch: 75249, loss: 0.52309, acc: 0.92272, time: Sun Jan 23 01:08:44 2022
epoch 2 training:  38%|███▊      | 75348/196351 [10:43:38<17:17:42,  1.94it/s]global step 271700, epoch: 2, batch: 75349, loss: 2.44130, acc: 0.92275, time: Sun Jan 23 01:09:35 2022
epoch 2 training:  38%|███▊      | 75448/196351 [10:44:29<16:57:52,  1.98it/s]global step 271800, epoch: 2, batch: 75449, loss: 0.86853, acc: 0.92273, time: Sun Jan 23 01:10:27 2022
epoch 2 training:  38%|███▊      | 75548/196351 [10:45:21<19:49:05,  1.69it/s]global step 271900, epoch: 2, batch: 75549, loss: 0.50012, acc: 0.92275, time: Sun Jan 23 01:11:18 2022
epoch 2 training:  39%|███▊      | 75648/196351 [10:46:13<17:09:08,  1.95it/s]global step 272000, epoch: 2, batch: 75649, loss: 0.68160, acc: 0.92277, time: Sun Jan 23 01:12:10 2022
epoch 2 training:  39%|███▊      | 75748/196351 [10:47:04<17:24:09,  1.93it/s]global step 272100, epoch: 2, batch: 75749, loss: 1.76950, acc: 0.92281, time: Sun Jan 23 01:13:01 2022
epoch 2 training:  39%|███▊      | 75848/196351 [10:47:55<17:09:55,  1.95it/s]global step 272200, epoch: 2, batch: 75849, loss: 1.79722, acc: 0.92285, time: Sun Jan 23 01:13:52 2022
epoch 2 training:  39%|███▊      | 75948/196351 [10:48:46<17:30:53,  1.91it/s]global step 272300, epoch: 2, batch: 75949, loss: 1.00354, acc: 0.92289, time: Sun Jan 23 01:14:44 2022
epoch 2 training:  39%|███▊      | 76048/196351 [10:49:37<17:17:24,  1.93it/s]global step 272400, epoch: 2, batch: 76049, loss: 0.36857, acc: 0.92290, time: Sun Jan 23 01:15:35 2022
epoch 2 training:  39%|███▉      | 76148/196351 [10:50:29<17:07:12,  1.95it/s]global step 272500, epoch: 2, batch: 76149, loss: 1.19476, acc: 0.92293, time: Sun Jan 23 01:16:26 2022
epoch 2 training:  39%|███▉      | 76248/196351 [10:51:20<17:31:40,  1.90it/s]global step 272600, epoch: 2, batch: 76249, loss: 2.65527, acc: 0.92293, time: Sun Jan 23 01:17:18 2022
epoch 2 training:  39%|███▉      | 76348/196351 [10:52:12<17:13:11,  1.94it/s]global step 272700, epoch: 2, batch: 76349, loss: 0.37458, acc: 0.92297, time: Sun Jan 23 01:18:09 2022
epoch 2 training:  39%|███▉      | 76448/196351 [10:53:03<16:54:20,  1.97it/s]global step 272800, epoch: 2, batch: 76449, loss: 0.80506, acc: 0.92297, time: Sun Jan 23 01:19:00 2022
epoch 2 training:  39%|███▉      | 76548/196351 [10:53:53<16:41:26,  1.99it/s]global step 272900, epoch: 2, batch: 76549, loss: 3.06218, acc: 0.92293, time: Sun Jan 23 01:19:51 2022
epoch 2 training:  39%|███▉      | 76648/196351 [10:54:44<17:09:28,  1.94it/s]global step 273000, epoch: 2, batch: 76649, loss: 0.29483, acc: 0.92296, time: Sun Jan 23 01:20:42 2022
epoch 2 training:  39%|███▉      | 76748/196351 [10:55:35<16:58:16,  1.96it/s]global step 273100, epoch: 2, batch: 76749, loss: 2.42105, acc: 0.92297, time: Sun Jan 23 01:21:32 2022
epoch 2 training:  39%|███▉      | 76848/196351 [10:56:26<17:00:24,  1.95it/s]global step 273200, epoch: 2, batch: 76849, loss: 1.88816, acc: 0.92300, time: Sun Jan 23 01:22:24 2022
epoch 2 training:  39%|███▉      | 76948/196351 [10:57:17<16:51:30,  1.97it/s]global step 273300, epoch: 2, batch: 76949, loss: 1.18269, acc: 0.92301, time: Sun Jan 23 01:23:15 2022
epoch 2 training:  39%|███▉      | 77048/196351 [10:58:09<17:08:43,  1.93it/s]global step 273400, epoch: 2, batch: 77049, loss: 0.56775, acc: 0.92300, time: Sun Jan 23 01:24:06 2022
epoch 2 training:  39%|███▉      | 77148/196351 [10:59:00<17:02:54,  1.94it/s]global step 273500, epoch: 2, batch: 77149, loss: 1.36010, acc: 0.92301, time: Sun Jan 23 01:24:57 2022
epoch 2 training:  39%|███▉      | 77248/196351 [10:59:51<17:04:08,  1.94it/s]global step 273600, epoch: 2, batch: 77249, loss: 1.26464, acc: 0.92303, time: Sun Jan 23 01:25:49 2022
epoch 2 training:  39%|███▉      | 77348/196351 [11:00:43<16:46:22,  1.97it/s]global step 273700, epoch: 2, batch: 77349, loss: 2.28394, acc: 0.92306, time: Sun Jan 23 01:26:40 2022
epoch 2 training:  39%|███▉      | 77448/196351 [11:01:34<16:55:33,  1.95it/s]global step 273800, epoch: 2, batch: 77449, loss: 0.54888, acc: 0.92308, time: Sun Jan 23 01:27:32 2022
epoch 2 training:  39%|███▉      | 77548/196351 [11:02:26<17:04:51,  1.93it/s]global step 273900, epoch: 2, batch: 77549, loss: 1.28908, acc: 0.92312, time: Sun Jan 23 01:28:23 2022
epoch 2 training:  40%|███▉      | 77648/196351 [11:03:17<16:48:09,  1.96it/s]global step 274000, epoch: 2, batch: 77649, loss: 1.26797, acc: 0.92315, time: Sun Jan 23 01:29:14 2022
epoch 2 training:  40%|███▉      | 77748/196351 [11:04:08<17:11:16,  1.92it/s]global step 274100, epoch: 2, batch: 77749, loss: 3.03964, acc: 0.92316, time: Sun Jan 23 01:30:06 2022
epoch 2 training:  40%|███▉      | 77848/196351 [11:05:00<16:49:46,  1.96it/s]global step 274200, epoch: 2, batch: 77849, loss: 4.43395, acc: 0.92318, time: Sun Jan 23 01:30:58 2022
epoch 2 training:  40%|███▉      | 77948/196351 [11:05:52<16:39:16,  1.97it/s]global step 274300, epoch: 2, batch: 77949, loss: 1.08508, acc: 0.92323, time: Sun Jan 23 01:31:49 2022
epoch 2 training:  40%|███▉      | 78048/196351 [11:06:43<16:39:25,  1.97it/s]global step 274400, epoch: 2, batch: 78049, loss: 1.20077, acc: 0.92320, time: Sun Jan 23 01:32:40 2022
epoch 2 training:  40%|███▉      | 78148/196351 [11:07:34<16:37:43,  1.97it/s]global step 274500, epoch: 2, batch: 78149, loss: 0.91304, acc: 0.92324, time: Sun Jan 23 01:33:32 2022
epoch 2 training:  40%|███▉      | 78248/196351 [11:08:26<17:14:11,  1.90it/s]global step 274600, epoch: 2, batch: 78249, loss: 1.07594, acc: 0.92326, time: Sun Jan 23 01:34:23 2022
epoch 2 training:  40%|███▉      | 78348/196351 [11:09:17<17:06:47,  1.92it/s]global step 274700, epoch: 2, batch: 78349, loss: 1.91748, acc: 0.92327, time: Sun Jan 23 01:35:15 2022
epoch 2 training:  40%|███▉      | 78448/196351 [11:10:08<17:14:46,  1.90it/s]global step 274800, epoch: 2, batch: 78449, loss: 2.80444, acc: 0.92327, time: Sun Jan 23 01:36:06 2022
epoch 2 training:  40%|████      | 78548/196351 [11:11:00<16:44:52,  1.95it/s]global step 274900, epoch: 2, batch: 78549, loss: 0.45419, acc: 0.92332, time: Sun Jan 23 01:36:57 2022
epoch 2 training:  40%|████      | 78648/196351 [11:11:51<16:54:00,  1.93it/s]global step 275000, epoch: 2, batch: 78649, loss: 3.81356, acc: 0.92334, time: Sun Jan 23 01:37:49 2022
epoch 2 training:  40%|████      | 78748/196351 [11:12:43<16:55:41,  1.93it/s]global step 275100, epoch: 2, batch: 78749, loss: 1.64060, acc: 0.92337, time: Sun Jan 23 01:38:40 2022
epoch 2 training:  40%|████      | 78848/196351 [11:13:35<16:48:19,  1.94it/s]global step 275200, epoch: 2, batch: 78849, loss: 2.66158, acc: 0.92334, time: Sun Jan 23 01:39:32 2022
epoch 2 training:  40%|████      | 78948/196351 [11:14:27<16:42:14,  1.95it/s]global step 275300, epoch: 2, batch: 78949, loss: 1.95328, acc: 0.92332, time: Sun Jan 23 01:40:24 2022
epoch 2 training:  40%|████      | 79048/196351 [11:15:18<16:48:07,  1.94it/s]global step 275400, epoch: 2, batch: 79049, loss: 0.43514, acc: 0.92331, time: Sun Jan 23 01:41:15 2022
epoch 2 training:  40%|████      | 79148/196351 [11:16:09<16:39:03,  1.96it/s]global step 275500, epoch: 2, batch: 79149, loss: 0.61013, acc: 0.92335, time: Sun Jan 23 01:42:07 2022
epoch 2 training:  40%|████      | 79248/196351 [11:17:00<16:42:00,  1.95it/s]global step 275600, epoch: 2, batch: 79249, loss: 1.53699, acc: 0.92336, time: Sun Jan 23 01:42:58 2022
epoch 2 training:  40%|████      | 79348/196351 [11:17:52<17:02:31,  1.91it/s]global step 275700, epoch: 2, batch: 79349, loss: 1.47440, acc: 0.92331, time: Sun Jan 23 01:43:49 2022
epoch 2 training:  40%|████      | 79448/196351 [11:18:43<16:35:10,  1.96it/s]global step 275800, epoch: 2, batch: 79449, loss: 1.12774, acc: 0.92333, time: Sun Jan 23 01:44:41 2022
epoch 2 training:  41%|████      | 79548/196351 [11:19:35<16:50:40,  1.93it/s]global step 275900, epoch: 2, batch: 79549, loss: 1.74763, acc: 0.92337, time: Sun Jan 23 01:45:32 2022
epoch 2 training:  41%|████      | 79648/196351 [11:20:26<16:33:03,  1.96it/s]global step 276000, epoch: 2, batch: 79649, loss: 0.81480, acc: 0.92339, time: Sun Jan 23 01:46:24 2022
epoch 2 training:  41%|████      | 79748/196351 [11:21:18<16:39:29,  1.94it/s]global step 276100, epoch: 2, batch: 79749, loss: 1.21730, acc: 0.92342, time: Sun Jan 23 01:47:15 2022
epoch 2 training:  41%|████      | 79848/196351 [11:22:09<16:46:02,  1.93it/s]global step 276200, epoch: 2, batch: 79849, loss: 0.34016, acc: 0.92344, time: Sun Jan 23 01:48:06 2022
epoch 2 training:  41%|████      | 79948/196351 [11:23:01<16:31:25,  1.96it/s]global step 276300, epoch: 2, batch: 79949, loss: 0.45292, acc: 0.92344, time: Sun Jan 23 01:48:58 2022
epoch 2 training:  41%|████      | 80048/196351 [11:23:52<16:11:16,  2.00it/s]global step 276400, epoch: 2, batch: 80049, loss: 2.82457, acc: 0.92344, time: Sun Jan 23 01:49:50 2022
epoch 2 training:  41%|████      | 80148/196351 [11:24:43<16:26:31,  1.96it/s]global step 276500, epoch: 2, batch: 80149, loss: 0.66040, acc: 0.92346, time: Sun Jan 23 01:50:41 2022
epoch 2 training:  41%|████      | 80248/196351 [11:25:35<16:40:10,  1.93it/s]global step 276600, epoch: 2, batch: 80249, loss: 2.75747, acc: 0.92346, time: Sun Jan 23 01:51:32 2022
epoch 2 training:  41%|████      | 80348/196351 [11:26:27<16:55:41,  1.90it/s]global step 276700, epoch: 2, batch: 80349, loss: 1.90490, acc: 0.92345, time: Sun Jan 23 01:52:24 2022
epoch 2 training:  41%|████      | 80448/196351 [11:27:19<16:48:35,  1.92it/s]global step 276800, epoch: 2, batch: 80449, loss: 1.68469, acc: 0.92347, time: Sun Jan 23 01:53:16 2022
epoch 2 training:  41%|████      | 80548/196351 [11:28:10<16:12:20,  1.98it/s]global step 276900, epoch: 2, batch: 80549, loss: 1.28039, acc: 0.92349, time: Sun Jan 23 01:54:08 2022
epoch 2 training:  41%|████      | 80648/196351 [11:29:02<16:33:20,  1.94it/s]global step 277000, epoch: 2, batch: 80649, loss: 2.58002, acc: 0.92347, time: Sun Jan 23 01:54:59 2022
epoch 2 training:  41%|████      | 80748/196351 [11:29:53<16:19:55,  1.97it/s]global step 277100, epoch: 2, batch: 80749, loss: 2.51182, acc: 0.92350, time: Sun Jan 23 01:55:50 2022
epoch 2 training:  41%|████      | 80848/196351 [11:30:44<16:19:33,  1.97it/s]global step 277200, epoch: 2, batch: 80849, loss: 1.59833, acc: 0.92350, time: Sun Jan 23 01:56:42 2022
epoch 2 training:  41%|████      | 80948/196351 [11:31:36<16:28:04,  1.95it/s]global step 277300, epoch: 2, batch: 80949, loss: 2.71397, acc: 0.92350, time: Sun Jan 23 01:57:33 2022
epoch 2 training:  41%|████▏     | 81048/196351 [11:32:27<16:41:31,  1.92it/s]global step 277400, epoch: 2, batch: 81049, loss: 2.77787, acc: 0.92347, time: Sun Jan 23 01:58:24 2022
epoch 2 training:  41%|████▏     | 81148/196351 [11:33:18<16:42:39,  1.91it/s]global step 277500, epoch: 2, batch: 81149, loss: 2.22080, acc: 0.92346, time: Sun Jan 23 01:59:16 2022
epoch 2 training:  41%|████▏     | 81248/196351 [11:34:10<16:26:29,  1.94it/s]global step 277600, epoch: 2, batch: 81249, loss: 1.51808, acc: 0.92348, time: Sun Jan 23 02:00:08 2022
epoch 2 training:  41%|████▏     | 81348/196351 [11:35:02<16:21:32,  1.95it/s]global step 277700, epoch: 2, batch: 81349, loss: 2.38213, acc: 0.92343, time: Sun Jan 23 02:00:59 2022
epoch 2 training:  41%|████▏     | 81448/196351 [11:35:53<16:11:14,  1.97it/s]global step 277800, epoch: 2, batch: 81449, loss: 2.42049, acc: 0.92343, time: Sun Jan 23 02:01:51 2022
epoch 2 training:  42%|████▏     | 81548/196351 [11:36:45<16:21:05,  1.95it/s]global step 277900, epoch: 2, batch: 81549, loss: 1.96972, acc: 0.92346, time: Sun Jan 23 02:02:42 2022
epoch 2 training:  42%|████▏     | 81648/196351 [11:37:36<16:19:59,  1.95it/s]global step 278000, epoch: 2, batch: 81649, loss: 1.15166, acc: 0.92345, time: Sun Jan 23 02:03:33 2022
epoch 2 training:  42%|████▏     | 81748/196351 [11:38:28<16:27:29,  1.93it/s]global step 278100, epoch: 2, batch: 81749, loss: 0.18681, acc: 0.92349, time: Sun Jan 23 02:04:26 2022
epoch 2 training:  42%|████▏     | 81848/196351 [11:39:20<16:17:23,  1.95it/s]global step 278200, epoch: 2, batch: 81849, loss: 1.17092, acc: 0.92345, time: Sun Jan 23 02:05:17 2022
epoch 2 training:  42%|████▏     | 81948/196351 [11:40:11<16:38:42,  1.91it/s]global step 278300, epoch: 2, batch: 81949, loss: 3.57917, acc: 0.92344, time: Sun Jan 23 02:06:09 2022
epoch 2 training:  42%|████▏     | 82048/196351 [11:41:03<16:21:39,  1.94it/s]global step 278400, epoch: 2, batch: 82049, loss: 1.73483, acc: 0.92343, time: Sun Jan 23 02:07:00 2022
epoch 2 training:  42%|████▏     | 82148/196351 [11:41:55<16:22:41,  1.94it/s]global step 278500, epoch: 2, batch: 82149, loss: 0.35486, acc: 0.92343, time: Sun Jan 23 02:07:52 2022
epoch 2 training:  42%|████▏     | 82248/196351 [11:42:46<16:18:56,  1.94it/s]global step 278600, epoch: 2, batch: 82249, loss: 3.87529, acc: 0.92346, time: Sun Jan 23 02:08:43 2022
epoch 2 training:  42%|████▏     | 82348/196351 [11:43:37<16:06:29,  1.97it/s]global step 278700, epoch: 2, batch: 82349, loss: 1.65463, acc: 0.92348, time: Sun Jan 23 02:09:35 2022
epoch 2 training:  42%|████▏     | 82448/196351 [11:44:29<16:01:49,  1.97it/s]global step 278800, epoch: 2, batch: 82449, loss: 0.99182, acc: 0.92349, time: Sun Jan 23 02:10:26 2022
epoch 2 training:  42%|████▏     | 82548/196351 [11:45:20<16:08:45,  1.96it/s]global step 278900, epoch: 2, batch: 82549, loss: 0.68599, acc: 0.92348, time: Sun Jan 23 02:11:18 2022
epoch 2 training:  42%|████▏     | 82648/196351 [11:46:12<16:16:44,  1.94it/s]global step 279000, epoch: 2, batch: 82649, loss: 2.38271, acc: 0.92345, time: Sun Jan 23 02:12:09 2022
epoch 2 training:  42%|████▏     | 82748/196351 [11:47:03<16:33:34,  1.91it/s]global step 279100, epoch: 2, batch: 82749, loss: 1.12265, acc: 0.92350, time: Sun Jan 23 02:13:01 2022
epoch 2 training:  42%|████▏     | 82848/196351 [11:47:56<16:22:23,  1.93it/s]global step 279200, epoch: 2, batch: 82849, loss: 0.53743, acc: 0.92353, time: Sun Jan 23 02:13:53 2022
epoch 2 training:  42%|████▏     | 82948/196351 [11:48:47<16:21:11,  1.93it/s]global step 279300, epoch: 2, batch: 82949, loss: 2.03000, acc: 0.92354, time: Sun Jan 23 02:14:45 2022
epoch 2 training:  42%|████▏     | 83048/196351 [11:49:39<16:06:37,  1.95it/s]global step 279400, epoch: 2, batch: 83049, loss: 0.06579, acc: 0.92359, time: Sun Jan 23 02:15:36 2022
epoch 2 training:  42%|████▏     | 83148/196351 [11:50:31<16:26:59,  1.91it/s]global step 279500, epoch: 2, batch: 83149, loss: 2.12915, acc: 0.92360, time: Sun Jan 23 02:16:28 2022
epoch 2 training:  42%|████▏     | 83248/196351 [11:51:22<16:11:39,  1.94it/s]global step 279600, epoch: 2, batch: 83249, loss: 0.47254, acc: 0.92358, time: Sun Jan 23 02:17:20 2022
epoch 2 training:  42%|████▏     | 83348/196351 [11:52:14<16:30:10,  1.90it/s]global step 279700, epoch: 2, batch: 83349, loss: 1.64363, acc: 0.92360, time: Sun Jan 23 02:18:12 2022
epoch 2 training:  42%|████▏     | 83448/196351 [11:53:06<16:08:13,  1.94it/s]global step 279800, epoch: 2, batch: 83449, loss: 1.54713, acc: 0.92364, time: Sun Jan 23 02:19:03 2022
epoch 2 training:  43%|████▎     | 83548/196351 [11:53:57<15:46:53,  1.99it/s]global step 279900, epoch: 2, batch: 83549, loss: 3.54357, acc: 0.92361, time: Sun Jan 23 02:19:54 2022
epoch 2 training:  43%|████▎     | 83648/196351 [11:54:47<15:46:18,  1.98it/s]global step 280000, epoch: 2, batch: 83649, loss: 2.45656, acc: 0.92364, time: Sun Jan 23 02:20:45 2022
epoch 2 training:  43%|████▎     | 83748/196351 [11:55:38<15:57:10,  1.96it/s]global step 280100, epoch: 2, batch: 83749, loss: 0.73044, acc: 0.92364, time: Sun Jan 23 02:21:36 2022
epoch 2 training:  43%|████▎     | 83848/196351 [11:56:30<16:15:54,  1.92it/s]global step 280200, epoch: 2, batch: 83849, loss: 1.39193, acc: 0.92364, time: Sun Jan 23 02:22:28 2022
epoch 2 training:  43%|████▎     | 83948/196351 [11:57:21<16:04:13,  1.94it/s]global step 280300, epoch: 2, batch: 83949, loss: 0.88456, acc: 0.92369, time: Sun Jan 23 02:23:19 2022
epoch 2 training:  43%|████▎     | 84048/196351 [11:58:12<15:51:49,  1.97it/s]global step 280400, epoch: 2, batch: 84049, loss: 0.44315, acc: 0.92374, time: Sun Jan 23 02:24:10 2022
epoch 2 training:  43%|████▎     | 84148/196351 [11:59:04<15:55:58,  1.96it/s]global step 280500, epoch: 2, batch: 84149, loss: 1.63248, acc: 0.92375, time: Sun Jan 23 02:25:01 2022
epoch 2 training:  43%|████▎     | 84248/196351 [11:59:55<15:57:48,  1.95it/s]global step 280600, epoch: 2, batch: 84249, loss: 1.01098, acc: 0.92373, time: Sun Jan 23 02:25:53 2022
epoch 2 training:  43%|████▎     | 84348/196351 [12:00:47<15:50:59,  1.96it/s]global step 280700, epoch: 2, batch: 84349, loss: 4.71947, acc: 0.92373, time: Sun Jan 23 02:26:44 2022
epoch 2 training:  43%|████▎     | 84448/196351 [12:01:38<15:59:22,  1.94it/s]global step 280800, epoch: 2, batch: 84449, loss: 0.95420, acc: 0.92373, time: Sun Jan 23 02:27:36 2022
epoch 2 training:  43%|████▎     | 84548/196351 [12:02:30<16:00:57,  1.94it/s]global step 280900, epoch: 2, batch: 84549, loss: 1.84530, acc: 0.92374, time: Sun Jan 23 02:28:27 2022
epoch 2 training:  43%|████▎     | 84648/196351 [12:03:22<16:19:56,  1.90it/s]global step 281000, epoch: 2, batch: 84649, loss: 2.39998, acc: 0.92378, time: Sun Jan 23 02:29:19 2022
epoch 2 training:  43%|████▎     | 84748/196351 [12:04:13<16:10:46,  1.92it/s]global step 281100, epoch: 2, batch: 84749, loss: 2.63201, acc: 0.92379, time: Sun Jan 23 02:30:11 2022
epoch 2 training:  43%|████▎     | 84848/196351 [12:05:05<15:55:43,  1.94it/s]global step 281200, epoch: 2, batch: 84849, loss: 0.89958, acc: 0.92380, time: Sun Jan 23 02:31:02 2022
epoch 2 training:  43%|████▎     | 84948/196351 [12:05:56<15:53:33,  1.95it/s]global step 281300, epoch: 2, batch: 84949, loss: 1.69217, acc: 0.92381, time: Sun Jan 23 02:31:54 2022
epoch 2 training:  43%|████▎     | 85048/196351 [12:06:48<15:49:26,  1.95it/s]global step 281400, epoch: 2, batch: 85049, loss: 2.94139, acc: 0.92384, time: Sun Jan 23 02:32:45 2022
epoch 2 training:  43%|████▎     | 85148/196351 [12:07:38<15:40:07,  1.97it/s]global step 281500, epoch: 2, batch: 85149, loss: 2.62569, acc: 0.92383, time: Sun Jan 23 02:33:36 2022
epoch 2 training:  43%|████▎     | 85248/196351 [12:08:29<15:32:44,  1.99it/s]global step 281600, epoch: 2, batch: 85249, loss: 1.11440, acc: 0.92386, time: Sun Jan 23 02:34:27 2022
epoch 2 training:  43%|████▎     | 85348/196351 [12:09:20<15:48:52,  1.95it/s]global step 281700, epoch: 2, batch: 85349, loss: 3.89607, acc: 0.92388, time: Sun Jan 23 02:35:18 2022
epoch 2 training:  44%|████▎     | 85448/196351 [12:10:12<15:55:36,  1.93it/s]global step 281800, epoch: 2, batch: 85449, loss: 2.07802, acc: 0.92388, time: Sun Jan 23 02:36:09 2022
epoch 2 training:  44%|████▎     | 85548/196351 [12:11:03<15:36:34,  1.97it/s]global step 281900, epoch: 2, batch: 85549, loss: 3.02268, acc: 0.92391, time: Sun Jan 23 02:37:00 2022
epoch 2 training:  44%|████▎     | 85648/196351 [12:11:54<15:50:02,  1.94it/s]global step 282000, epoch: 2, batch: 85649, loss: 4.53510, acc: 0.92390, time: Sun Jan 23 02:37:52 2022
epoch 2 training:  44%|████▎     | 85748/196351 [12:12:45<15:45:50,  1.95it/s]global step 282100, epoch: 2, batch: 85749, loss: 3.27039, acc: 0.92393, time: Sun Jan 23 02:38:43 2022
epoch 2 training:  44%|████▎     | 85848/196351 [12:13:36<15:30:33,  1.98it/s]global step 282200, epoch: 2, batch: 85849, loss: 1.05337, acc: 0.92395, time: Sun Jan 23 02:39:34 2022
epoch 2 training:  44%|████▍     | 85948/196351 [12:14:28<15:42:42,  1.95it/s]global step 282300, epoch: 2, batch: 85949, loss: 1.18488, acc: 0.92395, time: Sun Jan 23 02:40:25 2022
epoch 2 training:  44%|████▍     | 86048/196351 [12:15:19<15:44:10,  1.95it/s]global step 282400, epoch: 2, batch: 86049, loss: 3.13761, acc: 0.92395, time: Sun Jan 23 02:41:16 2022
epoch 2 training:  44%|████▍     | 86148/196351 [12:16:10<15:43:44,  1.95it/s]global step 282500, epoch: 2, batch: 86149, loss: 1.53360, acc: 0.92396, time: Sun Jan 23 02:42:08 2022
epoch 2 training:  44%|████▍     | 86248/196351 [12:17:01<15:37:08,  1.96it/s]global step 282600, epoch: 2, batch: 86249, loss: 0.49203, acc: 0.92392, time: Sun Jan 23 02:42:59 2022
epoch 2 training:  44%|████▍     | 86348/196351 [12:17:53<15:49:24,  1.93it/s]global step 282700, epoch: 2, batch: 86349, loss: 0.71218, acc: 0.92390, time: Sun Jan 23 02:43:50 2022
epoch 2 training:  44%|████▍     | 86448/196351 [12:18:45<15:26:06,  1.98it/s]global step 282800, epoch: 2, batch: 86449, loss: 2.21131, acc: 0.92391, time: Sun Jan 23 02:44:42 2022
epoch 2 training:  44%|████▍     | 86548/196351 [12:19:35<15:33:22,  1.96it/s]global step 282900, epoch: 2, batch: 86549, loss: 1.83800, acc: 0.92393, time: Sun Jan 23 02:45:33 2022
epoch 2 training:  44%|████▍     | 86648/196351 [12:20:27<15:24:10,  1.98it/s]global step 283000, epoch: 2, batch: 86649, loss: 1.03278, acc: 0.92394, time: Sun Jan 23 02:46:24 2022
epoch 2 training:  44%|████▍     | 86748/196351 [12:21:18<15:26:20,  1.97it/s]global step 283100, epoch: 2, batch: 86749, loss: 0.70959, acc: 0.92394, time: Sun Jan 23 02:47:16 2022
epoch 2 training:  44%|████▍     | 86848/196351 [12:22:09<15:19:10,  1.99it/s]global step 283200, epoch: 2, batch: 86849, loss: 2.03956, acc: 0.92396, time: Sun Jan 23 02:48:07 2022
epoch 2 training:  44%|████▍     | 86948/196351 [12:23:01<15:35:44,  1.95it/s]global step 283300, epoch: 2, batch: 86949, loss: 1.08871, acc: 0.92396, time: Sun Jan 23 02:48:58 2022
epoch 2 training:  44%|████▍     | 87048/196351 [12:23:52<15:30:55,  1.96it/s]global step 283400, epoch: 2, batch: 87049, loss: 0.21824, acc: 0.92395, time: Sun Jan 23 02:49:49 2022
epoch 2 training:  44%|████▍     | 87148/196351 [12:24:43<15:32:19,  1.95it/s]global step 283500, epoch: 2, batch: 87149, loss: 1.45025, acc: 0.92393, time: Sun Jan 23 02:50:40 2022
epoch 2 training:  44%|████▍     | 87248/196351 [12:25:34<15:26:29,  1.96it/s]global step 283600, epoch: 2, batch: 87249, loss: 1.62357, acc: 0.92391, time: Sun Jan 23 02:51:31 2022
epoch 2 training:  44%|████▍     | 87348/196351 [12:26:25<15:38:34,  1.94it/s]global step 283700, epoch: 2, batch: 87349, loss: 0.86382, acc: 0.92392, time: Sun Jan 23 02:52:22 2022
epoch 2 training:  45%|████▍     | 87448/196351 [12:27:16<15:23:30,  1.97it/s]global step 283800, epoch: 2, batch: 87449, loss: 1.01703, acc: 0.92394, time: Sun Jan 23 02:53:14 2022
epoch 2 training:  45%|████▍     | 87548/196351 [12:28:07<15:22:28,  1.97it/s]global step 283900, epoch: 2, batch: 87549, loss: 0.20307, acc: 0.92392, time: Sun Jan 23 02:54:05 2022
epoch 2 training:  45%|████▍     | 87648/196351 [12:29:00<17:48:36,  1.70it/s]global step 284000, epoch: 2, batch: 87649, loss: 1.64187, acc: 0.92392, time: Sun Jan 23 02:54:57 2022
epoch 2 training:  45%|████▍     | 87748/196351 [12:29:52<15:32:58,  1.94it/s]global step 284100, epoch: 2, batch: 87749, loss: 2.68113, acc: 0.92390, time: Sun Jan 23 02:55:49 2022
epoch 2 training:  45%|████▍     | 87848/196351 [12:30:43<15:37:22,  1.93it/s]global step 284200, epoch: 2, batch: 87849, loss: 4.14448, acc: 0.92388, time: Sun Jan 23 02:56:41 2022
epoch 2 training:  45%|████▍     | 87948/196351 [12:31:35<15:27:28,  1.95it/s]global step 284300, epoch: 2, batch: 87949, loss: 2.23905, acc: 0.92388, time: Sun Jan 23 02:57:32 2022
epoch 2 training:  45%|████▍     | 88048/196351 [12:32:27<15:42:56,  1.91it/s]global step 284400, epoch: 2, batch: 88049, loss: 2.81133, acc: 0.92391, time: Sun Jan 23 02:58:24 2022
epoch 2 training:  45%|████▍     | 88148/196351 [12:33:18<15:37:14,  1.92it/s]global step 284500, epoch: 2, batch: 88149, loss: 0.61248, acc: 0.92393, time: Sun Jan 23 02:59:15 2022
epoch 2 training:  45%|████▍     | 88248/196351 [12:34:09<15:10:39,  1.98it/s]global step 284600, epoch: 2, batch: 88249, loss: 1.57451, acc: 0.92391, time: Sun Jan 23 03:00:07 2022
epoch 2 training:  45%|████▍     | 88348/196351 [12:35:00<15:25:53,  1.94it/s]global step 284700, epoch: 2, batch: 88349, loss: 1.56850, acc: 0.92388, time: Sun Jan 23 03:00:58 2022
epoch 2 training:  45%|████▌     | 88448/196351 [12:35:51<15:51:08,  1.89it/s]global step 284800, epoch: 2, batch: 88449, loss: 2.96824, acc: 0.92390, time: Sun Jan 23 03:01:49 2022
epoch 2 training:  45%|████▌     | 88548/196351 [12:36:43<15:39:02,  1.91it/s]global step 284900, epoch: 2, batch: 88549, loss: 0.46962, acc: 0.92389, time: Sun Jan 23 03:02:40 2022
epoch 2 training:  45%|████▌     | 88648/196351 [12:37:34<15:18:41,  1.95it/s]global step 285000, epoch: 2, batch: 88649, loss: 1.61531, acc: 0.92389, time: Sun Jan 23 03:03:31 2022
epoch 2 training:  45%|████▌     | 88748/196351 [12:38:25<15:26:58,  1.93it/s]global step 285100, epoch: 2, batch: 88749, loss: 0.51362, acc: 0.92392, time: Sun Jan 23 03:04:23 2022
epoch 2 training:  45%|████▌     | 88848/196351 [12:39:17<15:21:50,  1.94it/s]global step 285200, epoch: 2, batch: 88849, loss: 0.69289, acc: 0.92391, time: Sun Jan 23 03:05:14 2022
epoch 2 training:  45%|████▌     | 88948/196351 [12:40:08<15:15:00,  1.96it/s]global step 285300, epoch: 2, batch: 88949, loss: 0.62734, acc: 0.92393, time: Sun Jan 23 03:06:05 2022
epoch 2 training:  45%|████▌     | 89048/196351 [12:40:59<15:32:45,  1.92it/s]global step 285400, epoch: 2, batch: 89049, loss: 3.06561, acc: 0.92394, time: Sun Jan 23 03:06:56 2022
epoch 2 training:  45%|████▌     | 89148/196351 [12:41:50<15:17:04,  1.95it/s]global step 285500, epoch: 2, batch: 89149, loss: 2.18621, acc: 0.92398, time: Sun Jan 23 03:07:48 2022
epoch 2 training:  45%|████▌     | 89248/196351 [12:42:41<15:23:38,  1.93it/s]global step 285600, epoch: 2, batch: 89249, loss: 0.70215, acc: 0.92398, time: Sun Jan 23 03:08:39 2022
epoch 2 training:  46%|████▌     | 89348/196351 [12:43:33<15:25:04,  1.93it/s]global step 285700, epoch: 2, batch: 89349, loss: 1.15197, acc: 0.92396, time: Sun Jan 23 03:09:30 2022
epoch 2 training:  46%|████▌     | 89448/196351 [12:44:25<15:20:41,  1.94it/s]global step 285800, epoch: 2, batch: 89449, loss: 0.74078, acc: 0.92400, time: Sun Jan 23 03:10:22 2022
epoch 2 training:  46%|████▌     | 89548/196351 [12:45:17<15:17:40,  1.94it/s]global step 285900, epoch: 2, batch: 89549, loss: 1.83806, acc: 0.92395, time: Sun Jan 23 03:11:14 2022
epoch 2 training:  46%|████▌     | 89648/196351 [12:46:08<14:58:51,  1.98it/s]global step 286000, epoch: 2, batch: 89649, loss: 1.84215, acc: 0.92399, time: Sun Jan 23 03:12:06 2022
epoch 2 training:  46%|████▌     | 89748/196351 [12:47:00<15:09:26,  1.95it/s]global step 286100, epoch: 2, batch: 89749, loss: 0.62991, acc: 0.92396, time: Sun Jan 23 03:12:57 2022
epoch 2 training:  46%|████▌     | 89848/196351 [12:47:51<15:14:11,  1.94it/s]global step 286200, epoch: 2, batch: 89849, loss: 2.75603, acc: 0.92394, time: Sun Jan 23 03:13:48 2022
epoch 2 training:  46%|████▌     | 89948/196351 [12:48:42<15:18:51,  1.93it/s]global step 286300, epoch: 2, batch: 89949, loss: 0.54846, acc: 0.92395, time: Sun Jan 23 03:14:39 2022
epoch 2 training:  46%|████▌     | 90048/196351 [12:49:34<15:12:13,  1.94it/s]global step 286400, epoch: 2, batch: 90049, loss: 2.44141, acc: 0.92394, time: Sun Jan 23 03:15:31 2022
epoch 2 training:  46%|████▌     | 90148/196351 [12:50:25<15:07:23,  1.95it/s]global step 286500, epoch: 2, batch: 90149, loss: 0.43590, acc: 0.92392, time: Sun Jan 23 03:16:23 2022
epoch 2 training:  46%|████▌     | 90248/196351 [12:51:16<14:55:27,  1.97it/s]global step 286600, epoch: 2, batch: 90249, loss: 0.44492, acc: 0.92392, time: Sun Jan 23 03:17:14 2022
epoch 2 training:  46%|████▌     | 90348/196351 [12:52:08<14:50:10,  1.98it/s]global step 286700, epoch: 2, batch: 90349, loss: 0.52379, acc: 0.92393, time: Sun Jan 23 03:18:05 2022
epoch 2 training:  46%|████▌     | 90448/196351 [12:52:59<14:59:01,  1.96it/s]global step 286800, epoch: 2, batch: 90449, loss: 0.40766, acc: 0.92392, time: Sun Jan 23 03:18:56 2022
epoch 2 training:  46%|████▌     | 90548/196351 [12:53:50<15:06:25,  1.95it/s]global step 286900, epoch: 2, batch: 90549, loss: 1.56728, acc: 0.92399, time: Sun Jan 23 03:19:48 2022
epoch 2 training:  46%|████▌     | 90648/196351 [12:54:41<14:47:19,  1.99it/s]global step 287000, epoch: 2, batch: 90649, loss: 0.48726, acc: 0.92401, time: Sun Jan 23 03:20:39 2022
epoch 2 training:  46%|████▌     | 90748/196351 [12:55:32<15:14:42,  1.92it/s]global step 287100, epoch: 2, batch: 90749, loss: 0.59687, acc: 0.92403, time: Sun Jan 23 03:21:30 2022
epoch 2 training:  46%|████▋     | 90848/196351 [12:56:23<15:00:53,  1.95it/s]global step 287200, epoch: 2, batch: 90849, loss: 2.17157, acc: 0.92403, time: Sun Jan 23 03:22:21 2022
epoch 2 training:  46%|████▋     | 90948/196351 [12:57:14<15:20:19,  1.91it/s]global step 287300, epoch: 2, batch: 90949, loss: 5.05682, acc: 0.92405, time: Sun Jan 23 03:23:12 2022
epoch 2 training:  46%|████▋     | 91048/196351 [12:58:06<14:56:36,  1.96it/s]global step 287400, epoch: 2, batch: 91049, loss: 3.77536, acc: 0.92406, time: Sun Jan 23 03:24:03 2022
epoch 2 training:  46%|████▋     | 91148/196351 [12:58:57<14:59:05,  1.95it/s]global step 287500, epoch: 2, batch: 91149, loss: 1.03192, acc: 0.92409, time: Sun Jan 23 03:24:54 2022
epoch 2 training:  46%|████▋     | 91248/196351 [12:59:49<15:12:32,  1.92it/s]global step 287600, epoch: 2, batch: 91249, loss: 1.36978, acc: 0.92406, time: Sun Jan 23 03:25:46 2022
epoch 2 training:  47%|████▋     | 91348/196351 [13:00:40<15:06:52,  1.93it/s]global step 287700, epoch: 2, batch: 91349, loss: 1.74092, acc: 0.92405, time: Sun Jan 23 03:26:38 2022
epoch 2 training:  47%|████▋     | 91448/196351 [13:01:32<14:50:44,  1.96it/s]global step 287800, epoch: 2, batch: 91449, loss: 2.27369, acc: 0.92408, time: Sun Jan 23 03:27:29 2022
epoch 2 training:  47%|████▋     | 91548/196351 [13:02:24<14:49:56,  1.96it/s]global step 287900, epoch: 2, batch: 91549, loss: 0.29321, acc: 0.92408, time: Sun Jan 23 03:28:21 2022
epoch 2 training:  47%|████▋     | 91648/196351 [13:03:15<14:51:58,  1.96it/s]global step 288000, epoch: 2, batch: 91649, loss: 0.19550, acc: 0.92405, time: Sun Jan 23 03:29:12 2022
epoch 2 training:  47%|████▋     | 91748/196351 [13:04:07<15:03:43,  1.93it/s]global step 288100, epoch: 2, batch: 91749, loss: 3.17180, acc: 0.92407, time: Sun Jan 23 03:30:04 2022
epoch 2 training:  47%|████▋     | 91848/196351 [13:04:58<14:53:56,  1.95it/s]global step 288200, epoch: 2, batch: 91849, loss: 1.91761, acc: 0.92408, time: Sun Jan 23 03:30:55 2022
epoch 2 training:  47%|████▋     | 91948/196351 [13:05:49<14:52:29,  1.95it/s]global step 288300, epoch: 2, batch: 91949, loss: 1.99867, acc: 0.92408, time: Sun Jan 23 03:31:46 2022
epoch 2 training:  47%|████▋     | 92048/196351 [13:06:40<14:53:21,  1.95it/s]global step 288400, epoch: 2, batch: 92049, loss: 1.90356, acc: 0.92407, time: Sun Jan 23 03:32:38 2022
epoch 2 training:  47%|████▋     | 92148/196351 [13:07:32<14:55:04,  1.94it/s]global step 288500, epoch: 2, batch: 92149, loss: 1.45321, acc: 0.92405, time: Sun Jan 23 03:33:29 2022
epoch 2 training:  47%|████▋     | 92248/196351 [13:08:24<14:49:34,  1.95it/s]global step 288600, epoch: 2, batch: 92249, loss: 2.06105, acc: 0.92403, time: Sun Jan 23 03:34:21 2022
epoch 2 training:  47%|████▋     | 92348/196351 [13:09:15<14:39:47,  1.97it/s]global step 288700, epoch: 2, batch: 92349, loss: 0.49848, acc: 0.92402, time: Sun Jan 23 03:35:13 2022
epoch 2 training:  47%|████▋     | 92448/196351 [13:10:06<14:50:45,  1.94it/s]global step 288800, epoch: 2, batch: 92449, loss: 2.38628, acc: 0.92403, time: Sun Jan 23 03:36:04 2022
epoch 2 training:  47%|████▋     | 92548/196351 [13:10:58<14:51:12,  1.94it/s]global step 288900, epoch: 2, batch: 92549, loss: 1.57630, acc: 0.92404, time: Sun Jan 23 03:36:55 2022
epoch 2 training:  47%|████▋     | 92648/196351 [13:11:49<14:48:27,  1.95it/s]global step 289000, epoch: 2, batch: 92649, loss: 2.06492, acc: 0.92406, time: Sun Jan 23 03:37:46 2022
epoch 2 training:  47%|████▋     | 92748/196351 [13:12:40<14:43:19,  1.95it/s]global step 289100, epoch: 2, batch: 92749, loss: 1.79576, acc: 0.92407, time: Sun Jan 23 03:38:37 2022
epoch 2 training:  47%|████▋     | 92848/196351 [13:13:31<14:38:33,  1.96it/s]global step 289200, epoch: 2, batch: 92849, loss: 1.42282, acc: 0.92409, time: Sun Jan 23 03:39:28 2022
epoch 2 training:  47%|████▋     | 92948/196351 [13:14:22<14:48:38,  1.94it/s]global step 289300, epoch: 2, batch: 92949, loss: 1.51526, acc: 0.92414, time: Sun Jan 23 03:40:19 2022
epoch 2 training:  47%|████▋     | 93048/196351 [13:15:13<14:41:17,  1.95it/s]global step 289400, epoch: 2, batch: 93049, loss: 0.65271, acc: 0.92413, time: Sun Jan 23 03:41:11 2022
epoch 2 training:  47%|████▋     | 93148/196351 [13:16:05<14:54:59,  1.92it/s]global step 289500, epoch: 2, batch: 93149, loss: 1.90828, acc: 0.92413, time: Sun Jan 23 03:42:02 2022
epoch 2 training:  47%|████▋     | 93248/196351 [13:16:57<14:44:40,  1.94it/s]global step 289600, epoch: 2, batch: 93249, loss: 2.26587, acc: 0.92412, time: Sun Jan 23 03:42:54 2022
epoch 2 training:  48%|████▊     | 93348/196351 [13:17:48<14:24:23,  1.99it/s]global step 289700, epoch: 2, batch: 93349, loss: 1.07878, acc: 0.92414, time: Sun Jan 23 03:43:46 2022
epoch 2 training:  48%|████▊     | 93448/196351 [13:18:40<14:45:33,  1.94it/s]global step 289800, epoch: 2, batch: 93449, loss: 0.35126, acc: 0.92414, time: Sun Jan 23 03:44:37 2022
epoch 2 training:  48%|████▊     | 93548/196351 [13:19:31<14:35:30,  1.96it/s]global step 289900, epoch: 2, batch: 93549, loss: 0.98109, acc: 0.92417, time: Sun Jan 23 03:45:28 2022
epoch 2 training:  48%|████▊     | 93648/196351 [13:20:22<14:31:44,  1.96it/s]global step 290000, epoch: 2, batch: 93649, loss: 0.18535, acc: 0.92417, time: Sun Jan 23 03:46:20 2022
epoch 2 training:  48%|████▊     | 93748/196351 [13:21:14<14:32:57,  1.96it/s]global step 290100, epoch: 2, batch: 93749, loss: 0.64774, acc: 0.92420, time: Sun Jan 23 03:47:11 2022
epoch 2 training:  48%|████▊     | 93848/196351 [13:22:05<14:46:08,  1.93it/s]global step 290200, epoch: 2, batch: 93849, loss: 2.64429, acc: 0.92422, time: Sun Jan 23 03:48:02 2022
epoch 2 training:  48%|████▊     | 93948/196351 [13:22:56<14:31:18,  1.96it/s]global step 290300, epoch: 2, batch: 93949, loss: 1.65669, acc: 0.92424, time: Sun Jan 23 03:48:54 2022
epoch 2 training:  48%|████▊     | 94048/196351 [13:23:48<14:16:07,  1.99it/s]global step 290400, epoch: 2, batch: 94049, loss: 0.89875, acc: 0.92420, time: Sun Jan 23 03:49:45 2022
epoch 2 training:  48%|████▊     | 94148/196351 [13:24:39<14:41:52,  1.93it/s]global step 290500, epoch: 2, batch: 94149, loss: 2.00219, acc: 0.92423, time: Sun Jan 23 03:50:36 2022
epoch 2 training:  48%|████▊     | 94248/196351 [13:25:30<14:34:05,  1.95it/s]global step 290600, epoch: 2, batch: 94249, loss: 1.25189, acc: 0.92424, time: Sun Jan 23 03:51:28 2022
epoch 2 training:  48%|████▊     | 94348/196351 [13:26:21<14:43:08,  1.93it/s]global step 290700, epoch: 2, batch: 94349, loss: 1.43822, acc: 0.92425, time: Sun Jan 23 03:52:18 2022
epoch 2 training:  48%|████▊     | 94448/196351 [13:27:12<14:24:18,  1.97it/s]global step 290800, epoch: 2, batch: 94449, loss: 1.32157, acc: 0.92427, time: Sun Jan 23 03:53:10 2022
epoch 2 training:  48%|████▊     | 94548/196351 [13:28:03<14:21:33,  1.97it/s]global step 290900, epoch: 2, batch: 94549, loss: 0.86891, acc: 0.92428, time: Sun Jan 23 03:54:01 2022
epoch 2 training:  48%|████▊     | 94648/196351 [13:28:54<14:36:06,  1.93it/s]global step 291000, epoch: 2, batch: 94649, loss: 0.45484, acc: 0.92429, time: Sun Jan 23 03:54:52 2022
epoch 2 training:  48%|████▊     | 94748/196351 [13:29:45<14:47:58,  1.91it/s]global step 291100, epoch: 2, batch: 94749, loss: 1.58640, acc: 0.92431, time: Sun Jan 23 03:55:42 2022
epoch 2 training:  48%|████▊     | 94848/196351 [13:30:36<14:14:03,  1.98it/s]global step 291200, epoch: 2, batch: 94849, loss: 3.03244, acc: 0.92429, time: Sun Jan 23 03:56:34 2022
epoch 2 training:  48%|████▊     | 94948/196351 [13:31:27<14:17:41,  1.97it/s]global step 291300, epoch: 2, batch: 94949, loss: 0.79620, acc: 0.92432, time: Sun Jan 23 03:57:25 2022
epoch 2 training:  48%|████▊     | 95048/196351 [13:32:18<14:13:39,  1.98it/s]global step 291400, epoch: 2, batch: 95049, loss: 3.23240, acc: 0.92431, time: Sun Jan 23 03:58:16 2022
epoch 2 training:  48%|████▊     | 95148/196351 [13:33:09<14:05:51,  1.99it/s]global step 291500, epoch: 2, batch: 95149, loss: 2.58495, acc: 0.92432, time: Sun Jan 23 03:59:06 2022
epoch 2 training:  49%|████▊     | 95248/196351 [13:34:00<14:24:43,  1.95it/s]global step 291600, epoch: 2, batch: 95249, loss: 2.70624, acc: 0.92434, time: Sun Jan 23 03:59:57 2022
epoch 2 training:  49%|████▊     | 95348/196351 [13:34:51<14:05:00,  1.99it/s]global step 291700, epoch: 2, batch: 95349, loss: 0.82300, acc: 0.92432, time: Sun Jan 23 04:00:48 2022
epoch 2 training:  49%|████▊     | 95448/196351 [13:35:42<14:00:08,  2.00it/s]global step 291800, epoch: 2, batch: 95449, loss: 3.39972, acc: 0.92433, time: Sun Jan 23 04:01:40 2022
epoch 2 training:  49%|████▊     | 95548/196351 [13:36:33<14:16:21,  1.96it/s]global step 291900, epoch: 2, batch: 95549, loss: 1.23804, acc: 0.92431, time: Sun Jan 23 04:02:31 2022
epoch 2 training:  49%|████▊     | 95648/196351 [13:37:25<14:11:00,  1.97it/s]global step 292000, epoch: 2, batch: 95649, loss: 0.71516, acc: 0.92434, time: Sun Jan 23 04:03:22 2022
epoch 2 training:  49%|████▉     | 95748/196351 [13:38:16<14:06:41,  1.98it/s]global step 292100, epoch: 2, batch: 95749, loss: 0.21314, acc: 0.92439, time: Sun Jan 23 04:04:13 2022
epoch 2 training:  49%|████▉     | 95848/196351 [13:39:07<14:08:50,  1.97it/s]global step 292200, epoch: 2, batch: 95849, loss: 1.87753, acc: 0.92440, time: Sun Jan 23 04:05:04 2022
epoch 2 training:  49%|████▉     | 95948/196351 [13:39:57<14:13:12,  1.96it/s]global step 292300, epoch: 2, batch: 95949, loss: 1.25641, acc: 0.92441, time: Sun Jan 23 04:05:55 2022
epoch 2 training:  49%|████▉     | 96048/196351 [13:40:49<14:29:37,  1.92it/s]global step 292400, epoch: 2, batch: 96049, loss: 0.93422, acc: 0.92446, time: Sun Jan 23 04:06:47 2022
epoch 2 training:  49%|████▉     | 96148/196351 [13:41:41<14:26:17,  1.93it/s]global step 292500, epoch: 2, batch: 96149, loss: 2.97842, acc: 0.92444, time: Sun Jan 23 04:07:38 2022
epoch 2 training:  49%|████▉     | 96248/196351 [13:42:33<14:15:14,  1.95it/s]global step 292600, epoch: 2, batch: 96249, loss: 4.53658, acc: 0.92443, time: Sun Jan 23 04:08:30 2022
epoch 2 training:  49%|████▉     | 96348/196351 [13:43:24<14:08:23,  1.96it/s]global step 292700, epoch: 2, batch: 96349, loss: 3.00257, acc: 0.92444, time: Sun Jan 23 04:09:22 2022
epoch 2 training:  49%|████▉     | 96448/196351 [13:44:16<14:19:03,  1.94it/s]global step 292800, epoch: 2, batch: 96449, loss: 1.78522, acc: 0.92446, time: Sun Jan 23 04:10:13 2022
epoch 2 training:  49%|████▉     | 96548/196351 [13:45:07<14:06:57,  1.96it/s]global step 292900, epoch: 2, batch: 96549, loss: 0.79993, acc: 0.92448, time: Sun Jan 23 04:11:05 2022
epoch 2 training:  49%|████▉     | 96648/196351 [13:45:59<14:15:23,  1.94it/s]global step 293000, epoch: 2, batch: 96649, loss: 0.57170, acc: 0.92448, time: Sun Jan 23 04:11:56 2022
epoch 2 training:  49%|████▉     | 96748/196351 [13:46:51<14:33:22,  1.90it/s]global step 293100, epoch: 2, batch: 96749, loss: 0.78474, acc: 0.92447, time: Sun Jan 23 04:12:48 2022
epoch 2 training:  49%|████▉     | 96848/196351 [13:47:42<14:12:48,  1.94it/s]global step 293200, epoch: 2, batch: 96849, loss: 2.81871, acc: 0.92449, time: Sun Jan 23 04:13:40 2022
epoch 2 training:  49%|████▉     | 96948/196351 [13:48:34<14:00:32,  1.97it/s]global step 293300, epoch: 2, batch: 96949, loss: 1.59843, acc: 0.92448, time: Sun Jan 23 04:14:31 2022
epoch 2 training:  49%|████▉     | 97048/196351 [13:49:24<14:19:53,  1.92it/s]global step 293400, epoch: 2, batch: 97049, loss: 1.35680, acc: 0.92451, time: Sun Jan 23 04:15:22 2022
epoch 2 training:  49%|████▉     | 97148/196351 [13:50:15<13:54:57,  1.98it/s]global step 293500, epoch: 2, batch: 97149, loss: 3.58881, acc: 0.92454, time: Sun Jan 23 04:16:13 2022
epoch 2 training:  50%|████▉     | 97248/196351 [13:51:07<14:01:33,  1.96it/s]global step 293600, epoch: 2, batch: 97249, loss: 1.92746, acc: 0.92458, time: Sun Jan 23 04:17:05 2022
epoch 2 training:  50%|████▉     | 97348/196351 [13:51:59<14:11:55,  1.94it/s]global step 293700, epoch: 2, batch: 97349, loss: 0.98801, acc: 0.92457, time: Sun Jan 23 04:17:56 2022
epoch 2 training:  50%|████▉     | 97448/196351 [13:52:50<13:57:50,  1.97it/s]global step 293800, epoch: 2, batch: 97449, loss: 0.94024, acc: 0.92459, time: Sun Jan 23 04:18:47 2022
epoch 2 training:  50%|████▉     | 97548/196351 [13:53:41<14:09:25,  1.94it/s]global step 293900, epoch: 2, batch: 97549, loss: 1.01629, acc: 0.92458, time: Sun Jan 23 04:19:39 2022
epoch 2 training:  50%|████▉     | 97648/196351 [13:54:33<14:08:48,  1.94it/s]global step 294000, epoch: 2, batch: 97649, loss: 1.46942, acc: 0.92455, time: Sun Jan 23 04:20:30 2022
epoch 2 training:  50%|████▉     | 97748/196351 [13:55:24<14:16:56,  1.92it/s]global step 294100, epoch: 2, batch: 97749, loss: 0.61416, acc: 0.92456, time: Sun Jan 23 04:21:22 2022
epoch 2 training:  50%|████▉     | 97848/196351 [13:56:15<13:56:58,  1.96it/s]global step 294200, epoch: 2, batch: 97849, loss: 3.07855, acc: 0.92460, time: Sun Jan 23 04:22:13 2022
epoch 2 training:  50%|████▉     | 97948/196351 [13:57:07<13:57:28,  1.96it/s]global step 294300, epoch: 2, batch: 97949, loss: 1.92866, acc: 0.92456, time: Sun Jan 23 04:23:04 2022
epoch 2 training:  50%|████▉     | 98048/196351 [13:57:58<13:54:54,  1.96it/s]global step 294400, epoch: 2, batch: 98049, loss: 0.92953, acc: 0.92454, time: Sun Jan 23 04:23:55 2022
epoch 2 training:  50%|████▉     | 98148/196351 [13:58:48<13:52:45,  1.97it/s]global step 294500, epoch: 2, batch: 98149, loss: 2.72598, acc: 0.92454, time: Sun Jan 23 04:24:46 2022
epoch 2 training:  50%|█████     | 98248/196351 [13:59:40<14:02:19,  1.94it/s]global step 294600, epoch: 2, batch: 98249, loss: 1.69264, acc: 0.92457, time: Sun Jan 23 04:25:37 2022
epoch 2 training:  50%|█████     | 98348/196351 [14:00:31<13:49:13,  1.97it/s]global step 294700, epoch: 2, batch: 98349, loss: 0.58460, acc: 0.92461, time: Sun Jan 23 04:26:28 2022
epoch 2 training:  50%|█████     | 98448/196351 [14:01:22<13:49:54,  1.97it/s]global step 294800, epoch: 2, batch: 98449, loss: 3.82339, acc: 0.92459, time: Sun Jan 23 04:27:20 2022
epoch 2 training:  50%|█████     | 98548/196351 [14:02:14<13:53:11,  1.96it/s]global step 294900, epoch: 2, batch: 98549, loss: 0.48761, acc: 0.92460, time: Sun Jan 23 04:28:11 2022
epoch 2 training:  50%|█████     | 98648/196351 [14:03:05<13:54:40,  1.95it/s]global step 295000, epoch: 2, batch: 98649, loss: 1.08828, acc: 0.92459, time: Sun Jan 23 04:29:02 2022
epoch 2 training:  50%|█████     | 98748/196351 [14:03:56<13:40:26,  1.98it/s]global step 295100, epoch: 2, batch: 98749, loss: 2.16839, acc: 0.92459, time: Sun Jan 23 04:29:54 2022
epoch 2 training:  50%|█████     | 98848/196351 [14:04:47<13:52:01,  1.95it/s]global step 295200, epoch: 2, batch: 98849, loss: 1.91637, acc: 0.92459, time: Sun Jan 23 04:30:45 2022
epoch 2 training:  50%|█████     | 98948/196351 [14:05:38<13:41:56,  1.98it/s]global step 295300, epoch: 2, batch: 98949, loss: 0.90766, acc: 0.92461, time: Sun Jan 23 04:31:35 2022
epoch 2 training:  50%|█████     | 99048/196351 [14:06:29<13:45:53,  1.96it/s]global step 295400, epoch: 2, batch: 99049, loss: 0.44087, acc: 0.92459, time: Sun Jan 23 04:32:26 2022
epoch 2 training:  50%|█████     | 99148/196351 [14:07:20<14:04:07,  1.92it/s]global step 295500, epoch: 2, batch: 99149, loss: 1.35384, acc: 0.92459, time: Sun Jan 23 04:33:18 2022
epoch 2 training:  51%|█████     | 99248/196351 [14:08:12<14:06:55,  1.91it/s]global step 295600, epoch: 2, batch: 99249, loss: 2.33079, acc: 0.92455, time: Sun Jan 23 04:34:09 2022
epoch 2 training:  51%|█████     | 99348/196351 [14:09:02<13:22:20,  2.02it/s]global step 295700, epoch: 2, batch: 99349, loss: 1.43402, acc: 0.92456, time: Sun Jan 23 04:35:00 2022
epoch 2 training:  51%|█████     | 99448/196351 [14:09:53<13:52:20,  1.94it/s]global step 295800, epoch: 2, batch: 99449, loss: 1.75133, acc: 0.92459, time: Sun Jan 23 04:35:51 2022
epoch 2 training:  51%|█████     | 99548/196351 [14:10:45<13:57:46,  1.93it/s]global step 295900, epoch: 2, batch: 99549, loss: 1.32281, acc: 0.92459, time: Sun Jan 23 04:36:42 2022
epoch 2 training:  51%|█████     | 99648/196351 [14:11:37<13:55:32,  1.93it/s]global step 296000, epoch: 2, batch: 99649, loss: 1.22990, acc: 0.92461, time: Sun Jan 23 04:37:34 2022
epoch 2 training:  51%|█████     | 99748/196351 [14:12:28<13:30:53,  1.99it/s]global step 296100, epoch: 2, batch: 99749, loss: 0.55498, acc: 0.92461, time: Sun Jan 23 04:38:25 2022
epoch 2 training:  51%|█████     | 99848/196351 [14:13:19<13:35:29,  1.97it/s]global step 296200, epoch: 2, batch: 99849, loss: 2.23885, acc: 0.92461, time: Sun Jan 23 04:39:17 2022
epoch 2 training:  51%|█████     | 99948/196351 [14:14:11<13:47:15,  1.94it/s]global step 296300, epoch: 2, batch: 99949, loss: 0.46196, acc: 0.92461, time: Sun Jan 23 04:40:08 2022
epoch 2 training:  51%|█████     | 100048/196351 [14:15:02<13:51:01,  1.93it/s]global step 296400, epoch: 2, batch: 100049, loss: 4.00980, acc: 0.92461, time: Sun Jan 23 04:41:00 2022
epoch 2 training:  51%|█████     | 100148/196351 [14:15:54<13:26:02,  1.99it/s]global step 296500, epoch: 2, batch: 100149, loss: 3.86153, acc: 0.92464, time: Sun Jan 23 04:41:51 2022
epoch 2 training:  51%|█████     | 100248/196351 [14:16:45<13:34:51,  1.97it/s]global step 296600, epoch: 2, batch: 100249, loss: 1.50684, acc: 0.92464, time: Sun Jan 23 04:42:42 2022
epoch 2 training:  51%|█████     | 100348/196351 [14:17:36<13:24:41,  1.99it/s]global step 296700, epoch: 2, batch: 100349, loss: 0.74582, acc: 0.92463, time: Sun Jan 23 04:43:33 2022
epoch 2 training:  51%|█████     | 100448/196351 [14:18:27<13:34:28,  1.96it/s]global step 296800, epoch: 2, batch: 100449, loss: 3.43518, acc: 0.92463, time: Sun Jan 23 04:44:24 2022
epoch 2 training:  51%|█████     | 100548/196351 [14:19:18<13:44:59,  1.94it/s]global step 296900, epoch: 2, batch: 100549, loss: 1.72842, acc: 0.92464, time: Sun Jan 23 04:45:15 2022
epoch 2 training:  51%|█████▏    | 100648/196351 [14:20:09<13:23:42,  1.98it/s]global step 297000, epoch: 2, batch: 100649, loss: 2.09101, acc: 0.92464, time: Sun Jan 23 04:46:07 2022
epoch 2 training:  51%|█████▏    | 100748/196351 [14:21:00<13:33:57,  1.96it/s]global step 297100, epoch: 2, batch: 100749, loss: 1.41368, acc: 0.92465, time: Sun Jan 23 04:46:58 2022
epoch 2 training:  51%|█████▏    | 100848/196351 [14:21:52<13:42:52,  1.93it/s]global step 297200, epoch: 2, batch: 100849, loss: 0.23230, acc: 0.92463, time: Sun Jan 23 04:47:49 2022
epoch 2 training:  51%|█████▏    | 100948/196351 [14:22:43<13:35:55,  1.95it/s]global step 297300, epoch: 2, batch: 100949, loss: 2.39617, acc: 0.92465, time: Sun Jan 23 04:48:41 2022
epoch 2 training:  51%|█████▏    | 101048/196351 [14:23:35<14:10:37,  1.87it/s]global step 297400, epoch: 2, batch: 101049, loss: 2.81841, acc: 0.92463, time: Sun Jan 23 04:49:32 2022
epoch 2 training:  52%|█████▏    | 101148/196351 [14:24:27<13:47:22,  1.92it/s]global step 297500, epoch: 2, batch: 101149, loss: 1.90472, acc: 0.92462, time: Sun Jan 23 04:50:24 2022
epoch 2 training:  52%|█████▏    | 101248/196351 [14:25:19<13:28:37,  1.96it/s]global step 297600, epoch: 2, batch: 101249, loss: 2.69269, acc: 0.92463, time: Sun Jan 23 04:51:16 2022
epoch 2 training:  52%|█████▏    | 101348/196351 [14:26:10<13:44:54,  1.92it/s]global step 297700, epoch: 2, batch: 101349, loss: 0.39332, acc: 0.92466, time: Sun Jan 23 04:52:08 2022
epoch 2 training:  52%|█████▏    | 101448/196351 [14:27:02<13:37:32,  1.93it/s]global step 297800, epoch: 2, batch: 101449, loss: 1.42650, acc: 0.92465, time: Sun Jan 23 04:53:00 2022
epoch 2 training:  52%|█████▏    | 101548/196351 [14:27:54<13:21:41,  1.97it/s]global step 297900, epoch: 2, batch: 101549, loss: 0.69309, acc: 0.92467, time: Sun Jan 23 04:53:51 2022
epoch 2 training:  52%|█████▏    | 101648/196351 [14:28:45<13:22:45,  1.97it/s]global step 298000, epoch: 2, batch: 101649, loss: 0.82824, acc: 0.92468, time: Sun Jan 23 04:54:43 2022
epoch 2 training:  52%|█████▏    | 101748/196351 [14:29:37<13:22:13,  1.97it/s]global step 298100, epoch: 2, batch: 101749, loss: 0.25803, acc: 0.92468, time: Sun Jan 23 04:55:34 2022
epoch 2 training:  52%|█████▏    | 101848/196351 [14:30:28<13:17:09,  1.98it/s]global step 298200, epoch: 2, batch: 101849, loss: 1.76730, acc: 0.92468, time: Sun Jan 23 04:56:26 2022
epoch 2 training:  52%|█████▏    | 101948/196351 [14:31:20<13:40:37,  1.92it/s]global step 298300, epoch: 2, batch: 101949, loss: 2.29576, acc: 0.92465, time: Sun Jan 23 04:57:18 2022
epoch 2 training:  52%|█████▏    | 102048/196351 [14:32:12<13:33:06,  1.93it/s]global step 298400, epoch: 2, batch: 102049, loss: 0.60370, acc: 0.92466, time: Sun Jan 23 04:58:09 2022
epoch 2 training:  52%|█████▏    | 102148/196351 [14:33:04<13:14:12,  1.98it/s]global step 298500, epoch: 2, batch: 102149, loss: 3.41520, acc: 0.92465, time: Sun Jan 23 04:59:01 2022
epoch 2 training:  52%|█████▏    | 102248/196351 [14:33:55<13:20:46,  1.96it/s]global step 298600, epoch: 2, batch: 102249, loss: 2.19763, acc: 0.92468, time: Sun Jan 23 04:59:52 2022
epoch 2 training:  52%|█████▏    | 102348/196351 [14:34:46<13:24:31,  1.95it/s]global step 298700, epoch: 2, batch: 102349, loss: 1.18312, acc: 0.92468, time: Sun Jan 23 05:00:44 2022
epoch 2 training:  52%|█████▏    | 102448/196351 [14:35:38<13:18:16,  1.96it/s]global step 298800, epoch: 2, batch: 102449, loss: 4.67340, acc: 0.92470, time: Sun Jan 23 05:01:35 2022
epoch 2 training:  52%|█████▏    | 102548/196351 [14:36:29<13:23:06,  1.95it/s]global step 298900, epoch: 2, batch: 102549, loss: 1.16212, acc: 0.92469, time: Sun Jan 23 05:02:26 2022
epoch 2 training:  52%|█████▏    | 102648/196351 [14:37:21<13:45:09,  1.89it/s]global step 299000, epoch: 2, batch: 102649, loss: 1.25886, acc: 0.92470, time: Sun Jan 23 05:03:18 2022
epoch 2 training:  52%|█████▏    | 102748/196351 [14:38:12<13:22:59,  1.94it/s]global step 299100, epoch: 2, batch: 102749, loss: 1.58147, acc: 0.92473, time: Sun Jan 23 05:04:10 2022
epoch 2 training:  52%|█████▏    | 102848/196351 [14:39:04<13:20:00,  1.95it/s]global step 299200, epoch: 2, batch: 102849, loss: 3.23748, acc: 0.92473, time: Sun Jan 23 05:05:01 2022
epoch 2 training:  52%|█████▏    | 102948/196351 [14:39:56<13:21:39,  1.94it/s]global step 299300, epoch: 2, batch: 102949, loss: 2.24783, acc: 0.92474, time: Sun Jan 23 05:05:53 2022
epoch 2 training:  52%|█████▏    | 103048/196351 [14:40:47<13:22:15,  1.94it/s]global step 299400, epoch: 2, batch: 103049, loss: 1.05717, acc: 0.92476, time: Sun Jan 23 05:06:45 2022
epoch 2 training:  53%|█████▎    | 103148/196351 [14:41:39<13:20:25,  1.94it/s]global step 299500, epoch: 2, batch: 103149, loss: 0.14303, acc: 0.92477, time: Sun Jan 23 05:07:37 2022
epoch 2 training:  53%|█████▎    | 103248/196351 [14:42:31<13:12:18,  1.96it/s]global step 299600, epoch: 2, batch: 103249, loss: 0.47430, acc: 0.92479, time: Sun Jan 23 05:08:28 2022
epoch 2 training:  53%|█████▎    | 103348/196351 [14:43:22<13:09:34,  1.96it/s]global step 299700, epoch: 2, batch: 103349, loss: 1.06074, acc: 0.92476, time: Sun Jan 23 05:09:19 2022
epoch 2 training:  53%|█████▎    | 103448/196351 [14:44:13<13:18:22,  1.94it/s]global step 299800, epoch: 2, batch: 103449, loss: 1.61513, acc: 0.92475, time: Sun Jan 23 05:10:11 2022
epoch 2 training:  53%|█████▎    | 103548/196351 [14:45:05<13:26:53,  1.92it/s]global step 299900, epoch: 2, batch: 103549, loss: 2.29461, acc: 0.92474, time: Sun Jan 23 05:11:02 2022
epoch 2 training:  53%|█████▎    | 103648/196351 [14:45:56<13:07:47,  1.96it/s]global step 300000, epoch: 2, batch: 103649, loss: 1.56105, acc: 0.92475, time: Sun Jan 23 05:11:54 2022
epoch 2 training:  53%|█████▎    | 103748/196351 [14:46:48<13:05:21,  1.97it/s]global step 300100, epoch: 2, batch: 103749, loss: 1.10119, acc: 0.92475, time: Sun Jan 23 05:12:45 2022
epoch 2 training:  53%|█████▎    | 103848/196351 [14:47:39<13:07:03,  1.96it/s]global step 300200, epoch: 2, batch: 103849, loss: 0.62398, acc: 0.92476, time: Sun Jan 23 05:13:36 2022
epoch 2 training:  53%|█████▎    | 103948/196351 [14:48:30<12:55:27,  1.99it/s]global step 300300, epoch: 2, batch: 103949, loss: 1.09325, acc: 0.92474, time: Sun Jan 23 05:14:27 2022
epoch 2 training:  53%|█████▎    | 104048/196351 [14:49:22<13:18:34,  1.93it/s]global step 300400, epoch: 2, batch: 104049, loss: 0.74837, acc: 0.92472, time: Sun Jan 23 05:15:19 2022
epoch 2 training:  53%|█████▎    | 104148/196351 [14:50:13<13:30:11,  1.90it/s]global step 300500, epoch: 2, batch: 104149, loss: 3.38482, acc: 0.92471, time: Sun Jan 23 05:16:11 2022
epoch 2 training:  53%|█████▎    | 104248/196351 [14:51:05<13:13:49,  1.93it/s]global step 300600, epoch: 2, batch: 104249, loss: 0.51303, acc: 0.92472, time: Sun Jan 23 05:17:02 2022
epoch 2 training:  53%|█████▎    | 104348/196351 [14:51:56<13:01:54,  1.96it/s]global step 300700, epoch: 2, batch: 104349, loss: 1.43088, acc: 0.92473, time: Sun Jan 23 05:17:54 2022
epoch 2 training:  53%|█████▎    | 104448/196351 [14:52:48<13:00:31,  1.96it/s]global step 300800, epoch: 2, batch: 104449, loss: 0.89709, acc: 0.92474, time: Sun Jan 23 05:18:45 2022
epoch 2 training:  53%|█████▎    | 104548/196351 [14:53:39<13:01:15,  1.96it/s]global step 300900, epoch: 2, batch: 104549, loss: 0.52629, acc: 0.92473, time: Sun Jan 23 05:19:37 2022
epoch 2 training:  53%|█████▎    | 104648/196351 [14:54:31<13:03:13,  1.95it/s]global step 301000, epoch: 2, batch: 104649, loss: 1.84390, acc: 0.92471, time: Sun Jan 23 05:20:28 2022
epoch 2 training:  53%|█████▎    | 104748/196351 [14:55:22<13:08:28,  1.94it/s]global step 301100, epoch: 2, batch: 104749, loss: 2.19335, acc: 0.92472, time: Sun Jan 23 05:21:19 2022
epoch 2 training:  53%|█████▎    | 104848/196351 [14:56:13<13:05:42,  1.94it/s]global step 301200, epoch: 2, batch: 104849, loss: 1.97250, acc: 0.92468, time: Sun Jan 23 05:22:11 2022
epoch 2 training:  53%|█████▎    | 104948/196351 [14:57:05<12:52:48,  1.97it/s]global step 301300, epoch: 2, batch: 104949, loss: 1.88555, acc: 0.92469, time: Sun Jan 23 05:23:02 2022
epoch 2 training:  54%|█████▎    | 105048/196351 [14:57:56<12:55:16,  1.96it/s]global step 301400, epoch: 2, batch: 105049, loss: 1.22579, acc: 0.92466, time: Sun Jan 23 05:23:54 2022
epoch 2 training:  54%|█████▎    | 105148/196351 [14:58:48<12:52:38,  1.97it/s]global step 301500, epoch: 2, batch: 105149, loss: 0.97128, acc: 0.92464, time: Sun Jan 23 05:24:45 2022
epoch 2 training:  54%|█████▎    | 105248/196351 [14:59:39<12:57:05,  1.95it/s]global step 301600, epoch: 2, batch: 105249, loss: 0.37823, acc: 0.92464, time: Sun Jan 23 05:25:36 2022
epoch 2 training:  54%|█████▎    | 105348/196351 [15:00:30<13:10:34,  1.92it/s]global step 301700, epoch: 2, batch: 105349, loss: 2.61180, acc: 0.92466, time: Sun Jan 23 05:26:28 2022
epoch 2 training:  54%|█████▎    | 105448/196351 [15:01:22<12:56:49,  1.95it/s]global step 301800, epoch: 2, batch: 105449, loss: 1.04177, acc: 0.92466, time: Sun Jan 23 05:27:19 2022
epoch 2 training:  54%|█████▍    | 105548/196351 [15:02:13<12:52:27,  1.96it/s]global step 301900, epoch: 2, batch: 105549, loss: 0.68947, acc: 0.92470, time: Sun Jan 23 05:28:11 2022
epoch 2 training:  54%|█████▍    | 105648/196351 [15:03:04<12:52:40,  1.96it/s]global step 302000, epoch: 2, batch: 105649, loss: 0.64014, acc: 0.92467, time: Sun Jan 23 05:29:02 2022
epoch 2 training:  54%|█████▍    | 105748/196351 [15:03:56<12:52:39,  1.95it/s]global step 302100, epoch: 2, batch: 105749, loss: 0.86150, acc: 0.92467, time: Sun Jan 23 05:29:53 2022
epoch 2 training:  54%|█████▍    | 105848/196351 [15:04:47<12:56:11,  1.94it/s]global step 302200, epoch: 2, batch: 105849, loss: 2.68255, acc: 0.92468, time: Sun Jan 23 05:30:45 2022
epoch 2 training:  54%|█████▍    | 105948/196351 [15:05:39<12:59:03,  1.93it/s]global step 302300, epoch: 2, batch: 105949, loss: 1.91692, acc: 0.92468, time: Sun Jan 23 05:31:36 2022
epoch 2 training:  54%|█████▍    | 106048/196351 [15:06:30<12:50:45,  1.95it/s]global step 302400, epoch: 2, batch: 106049, loss: 2.16849, acc: 0.92470, time: Sun Jan 23 05:32:28 2022
epoch 2 training:  54%|█████▍    | 106148/196351 [15:07:22<12:50:47,  1.95it/s]global step 302500, epoch: 2, batch: 106149, loss: 2.25341, acc: 0.92470, time: Sun Jan 23 05:33:19 2022
epoch 2 training:  54%|█████▍    | 106248/196351 [15:08:13<12:51:43,  1.95it/s]global step 302600, epoch: 2, batch: 106249, loss: 2.53141, acc: 0.92468, time: Sun Jan 23 05:34:10 2022
epoch 2 training:  54%|█████▍    | 106348/196351 [15:09:04<12:44:42,  1.96it/s]global step 302700, epoch: 2, batch: 106349, loss: 2.90025, acc: 0.92468, time: Sun Jan 23 05:35:02 2022
epoch 2 training:  54%|█████▍    | 106448/196351 [15:09:55<12:44:01,  1.96it/s]global step 302800, epoch: 2, batch: 106449, loss: 1.79915, acc: 0.92467, time: Sun Jan 23 05:35:53 2022
epoch 2 training:  54%|█████▍    | 106548/196351 [15:10:47<13:00:03,  1.92it/s]global step 302900, epoch: 2, batch: 106549, loss: 1.70409, acc: 0.92467, time: Sun Jan 23 05:36:44 2022
epoch 2 training:  54%|█████▍    | 106648/196351 [15:11:39<13:05:36,  1.90it/s]global step 303000, epoch: 2, batch: 106649, loss: 1.89319, acc: 0.92465, time: Sun Jan 23 05:37:36 2022
epoch 2 training:  54%|█████▍    | 106748/196351 [15:12:30<12:51:05,  1.94it/s]global step 303100, epoch: 2, batch: 106749, loss: 1.63471, acc: 0.92466, time: Sun Jan 23 05:38:27 2022
epoch 2 training:  54%|█████▍    | 106848/196351 [15:13:21<12:46:49,  1.95it/s]global step 303200, epoch: 2, batch: 106849, loss: 1.58280, acc: 0.92466, time: Sun Jan 23 05:39:19 2022
epoch 2 training:  54%|█████▍    | 106948/196351 [15:14:13<12:39:45,  1.96it/s]global step 303300, epoch: 2, batch: 106949, loss: 0.87450, acc: 0.92468, time: Sun Jan 23 05:40:10 2022
epoch 2 training:  55%|█████▍    | 107048/196351 [15:15:05<12:45:59,  1.94it/s]global step 303400, epoch: 2, batch: 107049, loss: 1.65802, acc: 0.92469, time: Sun Jan 23 05:41:02 2022
epoch 2 training:  55%|█████▍    | 107148/196351 [15:15:56<12:43:27,  1.95it/s]global step 303500, epoch: 2, batch: 107149, loss: 0.14224, acc: 0.92469, time: Sun Jan 23 05:41:54 2022
epoch 2 training:  55%|█████▍    | 107248/196351 [15:16:48<12:43:18,  1.95it/s]global step 303600, epoch: 2, batch: 107249, loss: 1.79926, acc: 0.92470, time: Sun Jan 23 05:42:45 2022
epoch 2 training:  55%|█████▍    | 107348/196351 [15:17:39<12:28:57,  1.98it/s]global step 303700, epoch: 2, batch: 107349, loss: 1.77275, acc: 0.92471, time: Sun Jan 23 05:43:37 2022
epoch 2 training:  55%|█████▍    | 107448/196351 [15:18:31<12:55:05,  1.91it/s]global step 303800, epoch: 2, batch: 107449, loss: 0.59242, acc: 0.92472, time: Sun Jan 23 05:44:28 2022
epoch 2 training:  55%|█████▍    | 107548/196351 [15:19:22<12:41:59,  1.94it/s]global step 303900, epoch: 2, batch: 107549, loss: 2.37263, acc: 0.92473, time: Sun Jan 23 05:45:19 2022
epoch 2 training:  55%|█████▍    | 107648/196351 [15:20:13<12:30:35,  1.97it/s]global step 304000, epoch: 2, batch: 107649, loss: 0.34533, acc: 0.92476, time: Sun Jan 23 05:46:11 2022
epoch 2 training:  55%|█████▍    | 107748/196351 [15:21:05<12:46:05,  1.93it/s]global step 304100, epoch: 2, batch: 107749, loss: 1.06368, acc: 0.92477, time: Sun Jan 23 05:47:02 2022
epoch 2 training:  55%|█████▍    | 107848/196351 [15:21:56<12:41:10,  1.94it/s]global step 304200, epoch: 2, batch: 107849, loss: 3.23897, acc: 0.92476, time: Sun Jan 23 05:47:54 2022
epoch 2 training:  55%|█████▍    | 107948/196351 [15:22:47<12:32:30,  1.96it/s]global step 304300, epoch: 2, batch: 107949, loss: 2.07629, acc: 0.92473, time: Sun Jan 23 05:48:45 2022
epoch 2 training:  55%|█████▌    | 108048/196351 [15:23:39<12:40:45,  1.93it/s]global step 304400, epoch: 2, batch: 108049, loss: 4.00458, acc: 0.92472, time: Sun Jan 23 05:49:36 2022
epoch 2 training:  55%|█████▌    | 108148/196351 [15:24:30<12:28:49,  1.96it/s]global step 304500, epoch: 2, batch: 108149, loss: 0.30560, acc: 0.92475, time: Sun Jan 23 05:50:27 2022
epoch 2 training:  55%|█████▌    | 108248/196351 [15:25:21<12:20:35,  1.98it/s]global step 304600, epoch: 2, batch: 108249, loss: 2.74478, acc: 0.92475, time: Sun Jan 23 05:51:18 2022
epoch 2 training:  55%|█████▌    | 108348/196351 [15:26:12<12:40:09,  1.93it/s]global step 304700, epoch: 2, batch: 108349, loss: 0.54760, acc: 0.92475, time: Sun Jan 23 05:52:09 2022
epoch 2 training:  55%|█████▌    | 108448/196351 [15:27:03<12:47:30,  1.91it/s]global step 304800, epoch: 2, batch: 108449, loss: 1.41543, acc: 0.92477, time: Sun Jan 23 05:53:01 2022
epoch 2 training:  55%|█████▌    | 108548/196351 [15:27:54<12:19:09,  1.98it/s]global step 304900, epoch: 2, batch: 108549, loss: 2.92390, acc: 0.92479, time: Sun Jan 23 05:53:52 2022
epoch 2 training:  55%|█████▌    | 108648/196351 [15:28:45<12:12:00,  2.00it/s]global step 305000, epoch: 2, batch: 108649, loss: 2.22682, acc: 0.92480, time: Sun Jan 23 05:54:43 2022
epoch 2 training:  55%|█████▌    | 108748/196351 [15:29:36<12:32:20,  1.94it/s]global step 305100, epoch: 2, batch: 108749, loss: 2.52063, acc: 0.92481, time: Sun Jan 23 05:55:34 2022
epoch 2 training:  55%|█████▌    | 108848/196351 [15:30:28<12:30:20,  1.94it/s]global step 305200, epoch: 2, batch: 108849, loss: 1.40303, acc: 0.92482, time: Sun Jan 23 05:56:25 2022
epoch 2 training:  55%|█████▌    | 108948/196351 [15:31:19<12:15:01,  1.98it/s]global step 305300, epoch: 2, batch: 108949, loss: 1.44281, acc: 0.92482, time: Sun Jan 23 05:57:17 2022
epoch 2 training:  56%|█████▌    | 109048/196351 [15:32:10<12:20:14,  1.97it/s]global step 305400, epoch: 2, batch: 109049, loss: 1.04545, acc: 0.92482, time: Sun Jan 23 05:58:08 2022
epoch 2 training:  56%|█████▌    | 109148/196351 [15:33:01<12:14:27,  1.98it/s]global step 305500, epoch: 2, batch: 109149, loss: 1.26609, acc: 0.92482, time: Sun Jan 23 05:58:59 2022
epoch 2 training:  56%|█████▌    | 109248/196351 [15:33:52<12:08:56,  1.99it/s]global step 305600, epoch: 2, batch: 109249, loss: 0.73649, acc: 0.92482, time: Sun Jan 23 05:59:50 2022
epoch 2 training:  56%|█████▌    | 109348/196351 [15:34:43<12:16:04,  1.97it/s]global step 305700, epoch: 2, batch: 109349, loss: 2.19557, acc: 0.92484, time: Sun Jan 23 06:00:40 2022
epoch 2 training:  56%|█████▌    | 109448/196351 [15:35:34<12:05:28,  2.00it/s]global step 305800, epoch: 2, batch: 109449, loss: 0.72643, acc: 0.92487, time: Sun Jan 23 06:01:31 2022
epoch 2 training:  56%|█████▌    | 109548/196351 [15:36:25<12:16:14,  1.97it/s]global step 305900, epoch: 2, batch: 109549, loss: 0.11817, acc: 0.92489, time: Sun Jan 23 06:02:22 2022
epoch 2 training:  56%|█████▌    | 109648/196351 [15:37:17<12:22:51,  1.95it/s]global step 306000, epoch: 2, batch: 109649, loss: 0.55249, acc: 0.92489, time: Sun Jan 23 06:03:14 2022
epoch 2 training:  56%|█████▌    | 109748/196351 [15:38:08<12:09:32,  1.98it/s]global step 306100, epoch: 2, batch: 109749, loss: 1.49334, acc: 0.92488, time: Sun Jan 23 06:04:05 2022
epoch 2 training:  56%|█████▌    | 109848/196351 [15:38:59<14:36:29,  1.64it/s]global step 306200, epoch: 2, batch: 109849, loss: 1.41120, acc: 0.92487, time: Sun Jan 23 06:04:57 2022
epoch 2 training:  56%|█████▌    | 109948/196351 [15:39:51<12:09:21,  1.97it/s]global step 306300, epoch: 2, batch: 109949, loss: 0.28550, acc: 0.92487, time: Sun Jan 23 06:05:48 2022
epoch 2 training:  56%|█████▌    | 110048/196351 [15:40:42<12:25:20,  1.93it/s]global step 306400, epoch: 2, batch: 110049, loss: 2.65657, acc: 0.92490, time: Sun Jan 23 06:06:40 2022
epoch 2 training:  56%|█████▌    | 110148/196351 [15:41:33<12:22:33,  1.93it/s]global step 306500, epoch: 2, batch: 110149, loss: 0.76626, acc: 0.92491, time: Sun Jan 23 06:07:31 2022
epoch 2 training:  56%|█████▌    | 110248/196351 [15:42:25<12:33:19,  1.90it/s]global step 306600, epoch: 2, batch: 110249, loss: 1.46223, acc: 0.92492, time: Sun Jan 23 06:08:22 2022
epoch 2 training:  56%|█████▌    | 110348/196351 [15:43:16<12:21:17,  1.93it/s]global step 306700, epoch: 2, batch: 110349, loss: 1.39291, acc: 0.92492, time: Sun Jan 23 06:09:14 2022
epoch 2 training:  56%|█████▋    | 110448/196351 [15:44:08<12:17:49,  1.94it/s]global step 306800, epoch: 2, batch: 110449, loss: 1.07268, acc: 0.92492, time: Sun Jan 23 06:10:05 2022
epoch 2 training:  56%|█████▋    | 110548/196351 [15:44:58<12:13:14,  1.95it/s]global step 306900, epoch: 2, batch: 110549, loss: 2.15273, acc: 0.92492, time: Sun Jan 23 06:10:56 2022
epoch 2 training:  56%|█████▋    | 110648/196351 [15:45:50<12:14:48,  1.94it/s]global step 307000, epoch: 2, batch: 110649, loss: 2.42229, acc: 0.92493, time: Sun Jan 23 06:11:47 2022
epoch 2 training:  56%|█████▋    | 110748/196351 [15:46:41<12:12:35,  1.95it/s]global step 307100, epoch: 2, batch: 110749, loss: 3.86476, acc: 0.92492, time: Sun Jan 23 06:12:38 2022
epoch 2 training:  56%|█████▋    | 110848/196351 [15:47:33<12:19:30,  1.93it/s]global step 307200, epoch: 2, batch: 110849, loss: 2.23353, acc: 0.92490, time: Sun Jan 23 06:13:30 2022
epoch 2 training:  57%|█████▋    | 110948/196351 [15:48:24<12:13:24,  1.94it/s]global step 307300, epoch: 2, batch: 110949, loss: 0.49098, acc: 0.92492, time: Sun Jan 23 06:14:21 2022
epoch 2 training:  57%|█████▋    | 111048/196351 [15:49:15<12:02:23,  1.97it/s]global step 307400, epoch: 2, batch: 111049, loss: 2.06586, acc: 0.92492, time: Sun Jan 23 06:15:13 2022
epoch 2 training:  57%|█████▋    | 111148/196351 [15:50:07<12:26:40,  1.90it/s]global step 307500, epoch: 2, batch: 111149, loss: 2.06625, acc: 0.92491, time: Sun Jan 23 06:16:05 2022
epoch 2 training:  57%|█████▋    | 111248/196351 [15:50:59<12:16:38,  1.93it/s]global step 307600, epoch: 2, batch: 111249, loss: 0.55904, acc: 0.92490, time: Sun Jan 23 06:16:57 2022
epoch 2 training:  57%|█████▋    | 111348/196351 [15:51:51<12:03:03,  1.96it/s]global step 307700, epoch: 2, batch: 111349, loss: 2.08004, acc: 0.92489, time: Sun Jan 23 06:17:48 2022
epoch 2 training:  57%|█████▋    | 111448/196351 [15:52:42<12:02:51,  1.96it/s]global step 307800, epoch: 2, batch: 111449, loss: 0.52192, acc: 0.92490, time: Sun Jan 23 06:18:40 2022
epoch 2 training:  57%|█████▋    | 111548/196351 [15:53:33<11:59:08,  1.97it/s]global step 307900, epoch: 2, batch: 111549, loss: 2.32098, acc: 0.92490, time: Sun Jan 23 06:19:31 2022
epoch 2 training:  57%|█████▋    | 111648/196351 [15:54:25<12:09:13,  1.94it/s]global step 308000, epoch: 2, batch: 111649, loss: 4.62906, acc: 0.92489, time: Sun Jan 23 06:20:22 2022
epoch 2 training:  57%|█████▋    | 111748/196351 [15:55:16<11:57:51,  1.96it/s]global step 308100, epoch: 2, batch: 111749, loss: 0.45966, acc: 0.92488, time: Sun Jan 23 06:21:13 2022
epoch 2 training:  57%|█████▋    | 111848/196351 [15:56:07<12:01:08,  1.95it/s]global step 308200, epoch: 2, batch: 111849, loss: 2.03939, acc: 0.92488, time: Sun Jan 23 06:22:04 2022
epoch 2 training:  57%|█████▋    | 111948/196351 [15:56:58<11:55:19,  1.97it/s]global step 308300, epoch: 2, batch: 111949, loss: 4.20767, acc: 0.92486, time: Sun Jan 23 06:22:55 2022
epoch 2 training:  57%|█████▋    | 112048/196351 [15:57:49<11:54:57,  1.97it/s]global step 308400, epoch: 2, batch: 112049, loss: 0.11482, acc: 0.92485, time: Sun Jan 23 06:23:46 2022
epoch 2 training:  57%|█████▋    | 112148/196351 [15:58:39<11:56:56,  1.96it/s]global step 308500, epoch: 2, batch: 112149, loss: 0.66368, acc: 0.92486, time: Sun Jan 23 06:24:37 2022
epoch 2 training:  57%|█████▋    | 112248/196351 [15:59:31<11:53:42,  1.96it/s]global step 308600, epoch: 2, batch: 112249, loss: 1.63639, acc: 0.92487, time: Sun Jan 23 06:25:28 2022
epoch 2 training:  57%|█████▋    | 112348/196351 [16:00:22<11:55:42,  1.96it/s]global step 308700, epoch: 2, batch: 112349, loss: 1.66256, acc: 0.92489, time: Sun Jan 23 06:26:20 2022
epoch 2 training:  57%|█████▋    | 112448/196351 [16:01:13<11:57:09,  1.95it/s]global step 308800, epoch: 2, batch: 112449, loss: 0.85207, acc: 0.92486, time: Sun Jan 23 06:27:11 2022
epoch 2 training:  57%|█████▋    | 112548/196351 [16:02:05<11:45:36,  1.98it/s]global step 308900, epoch: 2, batch: 112549, loss: 2.39337, acc: 0.92487, time: Sun Jan 23 06:28:02 2022
epoch 2 training:  57%|█████▋    | 112648/196351 [16:02:56<12:01:50,  1.93it/s]global step 309000, epoch: 2, batch: 112649, loss: 1.31928, acc: 0.92488, time: Sun Jan 23 06:28:54 2022
epoch 2 training:  57%|█████▋    | 112748/196351 [16:03:48<12:04:44,  1.92it/s]global step 309100, epoch: 2, batch: 112749, loss: 1.07713, acc: 0.92488, time: Sun Jan 23 06:29:45 2022
epoch 2 training:  57%|█████▋    | 112848/196351 [16:04:39<11:56:01,  1.94it/s]global step 309200, epoch: 2, batch: 112849, loss: 1.75701, acc: 0.92491, time: Sun Jan 23 06:30:37 2022
epoch 2 training:  58%|█████▊    | 112948/196351 [16:05:31<11:46:45,  1.97it/s]global step 309300, epoch: 2, batch: 112949, loss: 0.54563, acc: 0.92494, time: Sun Jan 23 06:31:28 2022
epoch 2 training:  58%|█████▊    | 113048/196351 [16:06:22<12:06:47,  1.91it/s]global step 309400, epoch: 2, batch: 113049, loss: 2.60556, acc: 0.92491, time: Sun Jan 23 06:32:20 2022
epoch 2 training:  58%|█████▊    | 113148/196351 [16:07:14<11:50:32,  1.95it/s]global step 309500, epoch: 2, batch: 113149, loss: 2.64838, acc: 0.92490, time: Sun Jan 23 06:33:11 2022
epoch 2 training:  58%|█████▊    | 113248/196351 [16:08:05<11:49:09,  1.95it/s]global step 309600, epoch: 2, batch: 113249, loss: 2.26142, acc: 0.92490, time: Sun Jan 23 06:34:02 2022
epoch 2 training:  58%|█████▊    | 113348/196351 [16:08:56<11:51:47,  1.94it/s]global step 309700, epoch: 2, batch: 113349, loss: 0.83108, acc: 0.92492, time: Sun Jan 23 06:34:54 2022
epoch 2 training:  58%|█████▊    | 113448/196351 [16:09:47<11:37:02,  1.98it/s]global step 309800, epoch: 2, batch: 113449, loss: 3.81840, acc: 0.92494, time: Sun Jan 23 06:35:45 2022
epoch 2 training:  58%|█████▊    | 113548/196351 [16:10:38<11:37:53,  1.98it/s]global step 309900, epoch: 2, batch: 113549, loss: 1.88711, acc: 0.92496, time: Sun Jan 23 06:36:36 2022
epoch 2 training:  58%|█████▊    | 113648/196351 [16:11:29<11:41:20,  1.97it/s]global step 310000, epoch: 2, batch: 113649, loss: 0.54473, acc: 0.92497, time: Sun Jan 23 06:37:27 2022
epoch 2 training:  58%|█████▊    | 113748/196351 [16:12:21<11:52:26,  1.93it/s]global step 310100, epoch: 2, batch: 113749, loss: 1.30065, acc: 0.92496, time: Sun Jan 23 06:38:19 2022
epoch 2 training:  58%|█████▊    | 113848/196351 [16:13:12<11:29:26,  1.99it/s]global step 310200, epoch: 2, batch: 113849, loss: 3.26229, acc: 0.92494, time: Sun Jan 23 06:39:10 2022
epoch 2 training:  58%|█████▊    | 113948/196351 [16:14:03<11:39:59,  1.96it/s]global step 310300, epoch: 2, batch: 113949, loss: 3.28587, acc: 0.92493, time: Sun Jan 23 06:40:01 2022
epoch 2 training:  58%|█████▊    | 114048/196351 [16:14:54<11:39:53,  1.96it/s]global step 310400, epoch: 2, batch: 114049, loss: 1.74963, acc: 0.92492, time: Sun Jan 23 06:40:52 2022
epoch 2 training:  58%|█████▊    | 114148/196351 [16:15:46<11:51:43,  1.92it/s]global step 310500, epoch: 2, batch: 114149, loss: 0.29967, acc: 0.92494, time: Sun Jan 23 06:41:43 2022
epoch 2 training:  58%|█████▊    | 114248/196351 [16:16:38<11:55:42,  1.91it/s]global step 310600, epoch: 2, batch: 114249, loss: 2.01578, acc: 0.92497, time: Sun Jan 23 06:42:35 2022
epoch 2 training:  58%|█████▊    | 114348/196351 [16:17:29<11:57:23,  1.91it/s]global step 310700, epoch: 2, batch: 114349, loss: 1.10246, acc: 0.92501, time: Sun Jan 23 06:43:27 2022
epoch 2 training:  58%|█████▊    | 114448/196351 [16:18:21<11:35:10,  1.96it/s]global step 310800, epoch: 2, batch: 114449, loss: 0.34812, acc: 0.92501, time: Sun Jan 23 06:44:18 2022
epoch 2 training:  58%|█████▊    | 114548/196351 [16:19:12<11:38:17,  1.95it/s]global step 310900, epoch: 2, batch: 114549, loss: 1.18051, acc: 0.92501, time: Sun Jan 23 06:45:10 2022
epoch 2 training:  58%|█████▊    | 114648/196351 [16:20:04<11:46:11,  1.93it/s]global step 311000, epoch: 2, batch: 114649, loss: 2.71269, acc: 0.92501, time: Sun Jan 23 06:46:01 2022
epoch 2 training:  58%|█████▊    | 114748/196351 [16:20:56<11:36:38,  1.95it/s]global step 311100, epoch: 2, batch: 114749, loss: 0.50108, acc: 0.92503, time: Sun Jan 23 06:46:53 2022
epoch 2 training:  58%|█████▊    | 114848/196351 [16:21:47<11:35:04,  1.95it/s]global step 311200, epoch: 2, batch: 114849, loss: 1.95832, acc: 0.92504, time: Sun Jan 23 06:47:44 2022
epoch 2 training:  59%|█████▊    | 114948/196351 [16:22:38<11:28:04,  1.97it/s]global step 311300, epoch: 2, batch: 114949, loss: 0.40414, acc: 0.92508, time: Sun Jan 23 06:48:35 2022
epoch 2 training:  59%|█████▊    | 115048/196351 [16:23:29<11:20:55,  1.99it/s]global step 311400, epoch: 2, batch: 115049, loss: 0.18554, acc: 0.92507, time: Sun Jan 23 06:49:26 2022
epoch 2 training:  59%|█████▊    | 115148/196351 [16:24:20<11:34:08,  1.95it/s]global step 311500, epoch: 2, batch: 115149, loss: 0.44375, acc: 0.92510, time: Sun Jan 23 06:50:17 2022
epoch 2 training:  59%|█████▊    | 115248/196351 [16:25:11<11:45:03,  1.92it/s]global step 311600, epoch: 2, batch: 115249, loss: 1.81998, acc: 0.92510, time: Sun Jan 23 06:51:09 2022
epoch 2 training:  59%|█████▊    | 115348/196351 [16:26:02<11:26:27,  1.97it/s]global step 311700, epoch: 2, batch: 115349, loss: 2.40122, acc: 0.92510, time: Sun Jan 23 06:52:00 2022
epoch 2 training:  59%|█████▉    | 115448/196351 [16:26:53<11:24:46,  1.97it/s]global step 311800, epoch: 2, batch: 115449, loss: 1.12099, acc: 0.92510, time: Sun Jan 23 06:52:51 2022
epoch 2 training:  59%|█████▉    | 115548/196351 [16:27:44<11:31:26,  1.95it/s]global step 311900, epoch: 2, batch: 115549, loss: 0.74610, acc: 0.92511, time: Sun Jan 23 06:53:41 2022
epoch 2 training:  59%|█████▉    | 115648/196351 [16:28:35<11:35:20,  1.93it/s]global step 312000, epoch: 2, batch: 115649, loss: 0.28387, acc: 0.92514, time: Sun Jan 23 06:54:32 2022
epoch 2 training:  59%|█████▉    | 115748/196351 [16:29:26<11:27:05,  1.96it/s]global step 312100, epoch: 2, batch: 115749, loss: 0.21332, acc: 0.92516, time: Sun Jan 23 06:55:23 2022
epoch 2 training:  59%|█████▉    | 115848/196351 [16:30:17<11:29:19,  1.95it/s]global step 312200, epoch: 2, batch: 115849, loss: 2.34478, acc: 0.92517, time: Sun Jan 23 06:56:14 2022
epoch 2 training:  59%|█████▉    | 115948/196351 [16:31:08<11:14:52,  1.99it/s]global step 312300, epoch: 2, batch: 115949, loss: 0.71301, acc: 0.92521, time: Sun Jan 23 06:57:06 2022
epoch 2 training:  59%|█████▉    | 116048/196351 [16:32:00<11:29:38,  1.94it/s]global step 312400, epoch: 2, batch: 116049, loss: 2.50123, acc: 0.92521, time: Sun Jan 23 06:57:57 2022
epoch 2 training:  59%|█████▉    | 116148/196351 [16:32:51<11:36:24,  1.92it/s]global step 312500, epoch: 2, batch: 116149, loss: 3.38960, acc: 0.92520, time: Sun Jan 23 06:58:48 2022
epoch 2 training:  59%|█████▉    | 116248/196351 [16:33:42<11:25:57,  1.95it/s]global step 312600, epoch: 2, batch: 116249, loss: 1.46979, acc: 0.92519, time: Sun Jan 23 06:59:40 2022
epoch 2 training:  59%|█████▉    | 116348/196351 [16:34:33<11:24:30,  1.95it/s]global step 312700, epoch: 2, batch: 116349, loss: 0.41692, acc: 0.92522, time: Sun Jan 23 07:00:31 2022
epoch 2 training:  59%|█████▉    | 116448/196351 [16:35:24<11:22:11,  1.95it/s]global step 312800, epoch: 2, batch: 116449, loss: 1.05428, acc: 0.92520, time: Sun Jan 23 07:01:22 2022
epoch 2 training:  59%|█████▉    | 116548/196351 [16:36:16<11:17:51,  1.96it/s]global step 312900, epoch: 2, batch: 116549, loss: 1.99873, acc: 0.92521, time: Sun Jan 23 07:02:13 2022
epoch 2 training:  59%|█████▉    | 116648/196351 [16:37:07<11:16:14,  1.96it/s]global step 313000, epoch: 2, batch: 116649, loss: 2.23186, acc: 0.92519, time: Sun Jan 23 07:03:04 2022
epoch 2 training:  59%|█████▉    | 116748/196351 [16:37:57<11:09:49,  1.98it/s]global step 313100, epoch: 2, batch: 116749, loss: 1.98641, acc: 0.92520, time: Sun Jan 23 07:03:55 2022
epoch 2 training:  60%|█████▉    | 116848/196351 [16:38:49<11:11:40,  1.97it/s]global step 313200, epoch: 2, batch: 116849, loss: 0.99259, acc: 0.92520, time: Sun Jan 23 07:04:46 2022
epoch 2 training:  60%|█████▉    | 116948/196351 [16:39:40<11:13:35,  1.96it/s]global step 313300, epoch: 2, batch: 116949, loss: 1.65174, acc: 0.92520, time: Sun Jan 23 07:05:38 2022
epoch 2 training:  60%|█████▉    | 117048/196351 [16:40:31<11:18:23,  1.95it/s]global step 313400, epoch: 2, batch: 117049, loss: 0.23370, acc: 0.92522, time: Sun Jan 23 07:06:29 2022
epoch 2 training:  60%|█████▉    | 117148/196351 [16:41:23<11:07:03,  1.98it/s]global step 313500, epoch: 2, batch: 117149, loss: 1.57753, acc: 0.92522, time: Sun Jan 23 07:07:20 2022
epoch 2 training:  60%|█████▉    | 117248/196351 [16:42:14<11:21:54,  1.93it/s]global step 313600, epoch: 2, batch: 117249, loss: 2.22670, acc: 0.92523, time: Sun Jan 23 07:08:11 2022
epoch 2 training:  60%|█████▉    | 117348/196351 [16:43:05<11:19:47,  1.94it/s]global step 313700, epoch: 2, batch: 117349, loss: 3.83001, acc: 0.92523, time: Sun Jan 23 07:09:03 2022
epoch 2 training:  60%|█████▉    | 117448/196351 [16:43:57<11:32:30,  1.90it/s]global step 313800, epoch: 2, batch: 117449, loss: 2.86935, acc: 0.92524, time: Sun Jan 23 07:09:54 2022
epoch 2 training:  60%|█████▉    | 117548/196351 [16:44:48<11:39:22,  1.88it/s]global step 313900, epoch: 2, batch: 117549, loss: 0.69191, acc: 0.92525, time: Sun Jan 23 07:10:46 2022
epoch 2 training:  60%|█████▉    | 117648/196351 [16:45:40<11:04:26,  1.97it/s]global step 314000, epoch: 2, batch: 117649, loss: 0.62457, acc: 0.92528, time: Sun Jan 23 07:11:38 2022
epoch 2 training:  60%|█████▉    | 117748/196351 [16:46:32<11:08:36,  1.96it/s]global step 314100, epoch: 2, batch: 117749, loss: 1.94884, acc: 0.92528, time: Sun Jan 23 07:12:29 2022
epoch 2 training:  60%|██████    | 117848/196351 [16:47:23<11:23:15,  1.91it/s]global step 314200, epoch: 2, batch: 117849, loss: 0.68249, acc: 0.92531, time: Sun Jan 23 07:13:20 2022
epoch 2 training:  60%|██████    | 117948/196351 [16:48:15<11:34:02,  1.88it/s]global step 314300, epoch: 2, batch: 117949, loss: 1.72297, acc: 0.92528, time: Sun Jan 23 07:14:12 2022
epoch 2 training:  60%|██████    | 118048/196351 [16:49:06<10:56:51,  1.99it/s]global step 314400, epoch: 2, batch: 118049, loss: 1.44610, acc: 0.92530, time: Sun Jan 23 07:15:04 2022
epoch 2 training:  60%|██████    | 118148/196351 [16:49:57<11:12:23,  1.94it/s]global step 314500, epoch: 2, batch: 118149, loss: 1.31605, acc: 0.92531, time: Sun Jan 23 07:15:54 2022
epoch 2 training:  60%|██████    | 118248/196351 [16:50:48<11:03:49,  1.96it/s]global step 314600, epoch: 2, batch: 118249, loss: 0.95701, acc: 0.92531, time: Sun Jan 23 07:16:45 2022
epoch 2 training:  60%|██████    | 118348/196351 [16:51:39<11:17:12,  1.92it/s]global step 314700, epoch: 2, batch: 118349, loss: 0.15556, acc: 0.92534, time: Sun Jan 23 07:17:36 2022
epoch 2 training:  60%|██████    | 118448/196351 [16:52:31<11:13:15,  1.93it/s]global step 314800, epoch: 2, batch: 118449, loss: 3.09218, acc: 0.92535, time: Sun Jan 23 07:18:28 2022
epoch 2 training:  60%|██████    | 118548/196351 [16:53:22<11:11:59,  1.93it/s]global step 314900, epoch: 2, batch: 118549, loss: 1.75774, acc: 0.92536, time: Sun Jan 23 07:19:19 2022
epoch 2 training:  60%|██████    | 118648/196351 [16:54:14<11:15:19,  1.92it/s]global step 315000, epoch: 2, batch: 118649, loss: 2.95658, acc: 0.92538, time: Sun Jan 23 07:20:12 2022
epoch 2 training:  60%|██████    | 118748/196351 [16:55:06<11:05:36,  1.94it/s]global step 315100, epoch: 2, batch: 118749, loss: 2.95218, acc: 0.92536, time: Sun Jan 23 07:21:03 2022
epoch 2 training:  61%|██████    | 118848/196351 [16:55:57<11:12:06,  1.92it/s]global step 315200, epoch: 2, batch: 118849, loss: 2.24877, acc: 0.92539, time: Sun Jan 23 07:21:54 2022
epoch 2 training:  61%|██████    | 118948/196351 [16:56:48<11:03:47,  1.94it/s]global step 315300, epoch: 2, batch: 118949, loss: 0.77605, acc: 0.92540, time: Sun Jan 23 07:22:46 2022
epoch 2 training:  61%|██████    | 119048/196351 [16:57:40<11:10:28,  1.92it/s]global step 315400, epoch: 2, batch: 119049, loss: 1.46636, acc: 0.92542, time: Sun Jan 23 07:23:37 2022
epoch 2 training:  61%|██████    | 119148/196351 [16:58:31<11:14:18,  1.91it/s]global step 315500, epoch: 2, batch: 119149, loss: 1.96171, acc: 0.92542, time: Sun Jan 23 07:24:29 2022
epoch 2 training:  61%|██████    | 119248/196351 [16:59:23<11:02:22,  1.94it/s]global step 315600, epoch: 2, batch: 119249, loss: 1.92698, acc: 0.92545, time: Sun Jan 23 07:25:20 2022
epoch 2 training:  61%|██████    | 119348/196351 [17:00:14<11:12:47,  1.91it/s]global step 315700, epoch: 2, batch: 119349, loss: 1.57785, acc: 0.92544, time: Sun Jan 23 07:26:11 2022
epoch 2 training:  61%|██████    | 119448/196351 [17:01:05<10:51:41,  1.97it/s]global step 315800, epoch: 2, batch: 119449, loss: 1.76576, acc: 0.92544, time: Sun Jan 23 07:27:02 2022
epoch 2 training:  61%|██████    | 119548/196351 [17:01:56<10:58:29,  1.94it/s]global step 315900, epoch: 2, batch: 119549, loss: 1.29599, acc: 0.92544, time: Sun Jan 23 07:27:53 2022
epoch 2 training:  61%|██████    | 119648/196351 [17:02:47<10:58:08,  1.94it/s]global step 316000, epoch: 2, batch: 119649, loss: 2.37883, acc: 0.92545, time: Sun Jan 23 07:28:44 2022
epoch 2 training:  61%|██████    | 119748/196351 [17:03:38<10:59:15,  1.94it/s]global step 316100, epoch: 2, batch: 119749, loss: 0.60165, acc: 0.92547, time: Sun Jan 23 07:29:36 2022
epoch 2 training:  61%|██████    | 119848/196351 [17:04:30<10:53:43,  1.95it/s]global step 316200, epoch: 2, batch: 119849, loss: 1.48230, acc: 0.92550, time: Sun Jan 23 07:30:27 2022
epoch 2 training:  61%|██████    | 119948/196351 [17:05:20<10:48:52,  1.96it/s]global step 316300, epoch: 2, batch: 119949, loss: 1.10907, acc: 0.92552, time: Sun Jan 23 07:31:18 2022
epoch 2 training:  61%|██████    | 120048/196351 [17:06:12<10:48:54,  1.96it/s]global step 316400, epoch: 2, batch: 120049, loss: 0.62668, acc: 0.92552, time: Sun Jan 23 07:32:09 2022
epoch 2 training:  61%|██████    | 120148/196351 [17:07:04<10:46:58,  1.96it/s]global step 316500, epoch: 2, batch: 120149, loss: 0.72647, acc: 0.92553, time: Sun Jan 23 07:33:01 2022
epoch 2 training:  61%|██████    | 120248/196351 [17:07:55<10:56:42,  1.93it/s]global step 316600, epoch: 2, batch: 120249, loss: 2.12105, acc: 0.92552, time: Sun Jan 23 07:33:53 2022
epoch 2 training:  61%|██████▏   | 120348/196351 [17:08:47<11:01:06,  1.92it/s]global step 316700, epoch: 2, batch: 120349, loss: 0.40473, acc: 0.92552, time: Sun Jan 23 07:34:44 2022
epoch 2 training:  61%|██████▏   | 120448/196351 [17:09:39<10:53:14,  1.94it/s]global step 316800, epoch: 2, batch: 120449, loss: 3.35606, acc: 0.92551, time: Sun Jan 23 07:35:36 2022
epoch 2 training:  61%|██████▏   | 120548/196351 [17:10:30<10:44:50,  1.96it/s]global step 316900, epoch: 2, batch: 120549, loss: 1.36524, acc: 0.92550, time: Sun Jan 23 07:36:28 2022
epoch 2 training:  61%|██████▏   | 120648/196351 [17:11:21<10:39:41,  1.97it/s]global step 317000, epoch: 2, batch: 120649, loss: 2.48003, acc: 0.92549, time: Sun Jan 23 07:37:19 2022
epoch 2 training:  61%|██████▏   | 120748/196351 [17:12:12<10:43:32,  1.96it/s]global step 317100, epoch: 2, batch: 120749, loss: 1.87638, acc: 0.92549, time: Sun Jan 23 07:38:10 2022
epoch 2 training:  62%|██████▏   | 120848/196351 [17:13:04<10:38:35,  1.97it/s]global step 317200, epoch: 2, batch: 120849, loss: 0.91516, acc: 0.92552, time: Sun Jan 23 07:39:01 2022
epoch 2 training:  62%|██████▏   | 120948/196351 [17:13:55<10:36:26,  1.97it/s]global step 317300, epoch: 2, batch: 120949, loss: 0.94111, acc: 0.92553, time: Sun Jan 23 07:39:52 2022
epoch 2 training:  62%|██████▏   | 121048/196351 [17:14:45<10:42:44,  1.95it/s]global step 317400, epoch: 2, batch: 121049, loss: 2.52763, acc: 0.92555, time: Sun Jan 23 07:40:43 2022
epoch 2 training:  62%|██████▏   | 121148/196351 [17:15:37<10:37:50,  1.97it/s]global step 317500, epoch: 2, batch: 121149, loss: 2.97376, acc: 0.92557, time: Sun Jan 23 07:41:34 2022
epoch 2 training:  62%|██████▏   | 121248/196351 [17:16:28<10:39:04,  1.96it/s]global step 317600, epoch: 2, batch: 121249, loss: 1.48273, acc: 0.92556, time: Sun Jan 23 07:42:25 2022
epoch 2 training:  62%|██████▏   | 121348/196351 [17:17:19<10:38:15,  1.96it/s]global step 317700, epoch: 2, batch: 121349, loss: 2.69857, acc: 0.92556, time: Sun Jan 23 07:43:16 2022
epoch 2 training:  62%|██████▏   | 121448/196351 [17:18:11<11:01:33,  1.89it/s]global step 317800, epoch: 2, batch: 121449, loss: 2.10873, acc: 0.92555, time: Sun Jan 23 07:44:08 2022
epoch 2 training:  62%|██████▏   | 121548/196351 [17:19:02<10:35:56,  1.96it/s]global step 317900, epoch: 2, batch: 121549, loss: 1.60880, acc: 0.92554, time: Sun Jan 23 07:45:00 2022
epoch 2 training:  62%|██████▏   | 121648/196351 [17:19:54<10:57:07,  1.89it/s]global step 318000, epoch: 2, batch: 121649, loss: 0.84247, acc: 0.92554, time: Sun Jan 23 07:45:51 2022
epoch 2 training:  62%|██████▏   | 121748/196351 [17:20:45<10:44:20,  1.93it/s]global step 318100, epoch: 2, batch: 121749, loss: 0.24240, acc: 0.92556, time: Sun Jan 23 07:46:43 2022
epoch 2 training:  62%|██████▏   | 121848/196351 [17:21:37<10:56:51,  1.89it/s]global step 318200, epoch: 2, batch: 121849, loss: 1.32652, acc: 0.92558, time: Sun Jan 23 07:47:34 2022
epoch 2 training:  62%|██████▏   | 121948/196351 [17:22:29<10:33:52,  1.96it/s]global step 318300, epoch: 2, batch: 121949, loss: 0.75215, acc: 0.92558, time: Sun Jan 23 07:48:26 2022
epoch 2 training:  62%|██████▏   | 122048/196351 [17:23:20<10:43:52,  1.92it/s]global step 318400, epoch: 2, batch: 122049, loss: 0.32178, acc: 0.92555, time: Sun Jan 23 07:49:18 2022
epoch 2 training:  62%|██████▏   | 122148/196351 [17:24:12<10:42:16,  1.93it/s]global step 318500, epoch: 2, batch: 122149, loss: 3.37813, acc: 0.92555, time: Sun Jan 23 07:50:09 2022
epoch 2 training:  62%|██████▏   | 122248/196351 [17:25:04<10:37:32,  1.94it/s]global step 318600, epoch: 2, batch: 122249, loss: 1.23640, acc: 0.92557, time: Sun Jan 23 07:51:01 2022
epoch 2 training:  62%|██████▏   | 122348/196351 [17:25:55<10:33:54,  1.95it/s]global step 318700, epoch: 2, batch: 122349, loss: 3.39984, acc: 0.92557, time: Sun Jan 23 07:51:52 2022
epoch 2 training:  62%|██████▏   | 122448/196351 [17:26:47<10:34:26,  1.94it/s]global step 318800, epoch: 2, batch: 122449, loss: 0.80775, acc: 0.92556, time: Sun Jan 23 07:52:44 2022
epoch 2 training:  62%|██████▏   | 122548/196351 [17:27:38<10:35:16,  1.94it/s]global step 318900, epoch: 2, batch: 122549, loss: 0.52776, acc: 0.92559, time: Sun Jan 23 07:53:36 2022
epoch 2 training:  62%|██████▏   | 122648/196351 [17:28:30<10:33:25,  1.94it/s]global step 319000, epoch: 2, batch: 122649, loss: 0.58676, acc: 0.92561, time: Sun Jan 23 07:54:27 2022
epoch 2 training:  63%|██████▎   | 122748/196351 [17:29:21<10:36:24,  1.93it/s]global step 319100, epoch: 2, batch: 122749, loss: 0.36267, acc: 0.92563, time: Sun Jan 23 07:55:19 2022
epoch 2 training:  63%|██████▎   | 122848/196351 [17:30:13<10:22:11,  1.97it/s]global step 319200, epoch: 2, batch: 122849, loss: 0.69070, acc: 0.92563, time: Sun Jan 23 07:56:10 2022
epoch 2 training:  63%|██████▎   | 122948/196351 [17:31:05<10:35:33,  1.92it/s]global step 319300, epoch: 2, batch: 122949, loss: 1.89936, acc: 0.92565, time: Sun Jan 23 07:57:02 2022
epoch 2 training:  63%|██████▎   | 123048/196351 [17:31:56<10:36:38,  1.92it/s]global step 319400, epoch: 2, batch: 123049, loss: 1.01765, acc: 0.92566, time: Sun Jan 23 07:57:54 2022
epoch 2 training:  63%|██████▎   | 123148/196351 [17:32:48<10:20:44,  1.97it/s]global step 319500, epoch: 2, batch: 123149, loss: 0.66626, acc: 0.92566, time: Sun Jan 23 07:58:45 2022
epoch 2 training:  63%|██████▎   | 123248/196351 [17:33:39<10:24:18,  1.95it/s]global step 319600, epoch: 2, batch: 123249, loss: 2.32347, acc: 0.92568, time: Sun Jan 23 07:59:37 2022
epoch 2 training:  63%|██████▎   | 123348/196351 [17:34:31<10:31:28,  1.93it/s]global step 319700, epoch: 2, batch: 123349, loss: 3.35970, acc: 0.92571, time: Sun Jan 23 08:00:28 2022
epoch 2 training:  63%|██████▎   | 123448/196351 [17:35:22<10:27:11,  1.94it/s]global step 319800, epoch: 2, batch: 123449, loss: 3.29955, acc: 0.92573, time: Sun Jan 23 08:01:20 2022
epoch 2 training:  63%|██████▎   | 123548/196351 [17:36:14<10:21:52,  1.95it/s]global step 319900, epoch: 2, batch: 123549, loss: 0.56945, acc: 0.92572, time: Sun Jan 23 08:02:11 2022
epoch 2 training:  63%|██████▎   | 123648/196351 [17:37:05<10:17:15,  1.96it/s]global step 320000, epoch: 2, batch: 123649, loss: 0.29440, acc: 0.92572, time: Sun Jan 23 08:03:03 2022
epoch 2 training:  63%|██████▎   | 123748/196351 [17:37:56<10:13:13,  1.97it/s]global step 320100, epoch: 2, batch: 123749, loss: 1.90281, acc: 0.92573, time: Sun Jan 23 08:03:54 2022
epoch 2 training:  63%|██████▎   | 123848/196351 [17:38:47<10:16:26,  1.96it/s]global step 320200, epoch: 2, batch: 123849, loss: 0.48286, acc: 0.92574, time: Sun Jan 23 08:04:45 2022
epoch 2 training:  63%|██████▎   | 123948/196351 [17:39:39<10:17:44,  1.95it/s]global step 320300, epoch: 2, batch: 123949, loss: 0.61178, acc: 0.92576, time: Sun Jan 23 08:05:36 2022
epoch 2 training:  63%|██████▎   | 124048/196351 [17:40:29<10:13:42,  1.96it/s]global step 320400, epoch: 2, batch: 124049, loss: 1.91797, acc: 0.92576, time: Sun Jan 23 08:06:27 2022
epoch 2 training:  63%|██████▎   | 124148/196351 [17:41:20<10:13:48,  1.96it/s]global step 320500, epoch: 2, batch: 124149, loss: 2.72087, acc: 0.92577, time: Sun Jan 23 08:07:18 2022
epoch 2 training:  63%|██████▎   | 124248/196351 [17:42:12<11:56:01,  1.68it/s]global step 320600, epoch: 2, batch: 124249, loss: 1.33254, acc: 0.92578, time: Sun Jan 23 08:08:09 2022
epoch 2 training:  63%|██████▎   | 124348/196351 [17:43:03<10:13:22,  1.96it/s]global step 320700, epoch: 2, batch: 124349, loss: 2.39480, acc: 0.92581, time: Sun Jan 23 08:09:00 2022
epoch 2 training:  63%|██████▎   | 124448/196351 [17:43:54<10:07:08,  1.97it/s]global step 320800, epoch: 2, batch: 124449, loss: 1.72063, acc: 0.92583, time: Sun Jan 23 08:09:51 2022
epoch 2 training:  63%|██████▎   | 124548/196351 [17:44:45<10:02:17,  1.99it/s]global step 320900, epoch: 2, batch: 124549, loss: 0.38217, acc: 0.92582, time: Sun Jan 23 08:10:42 2022
epoch 2 training:  63%|██████▎   | 124648/196351 [17:45:36<10:00:42,  1.99it/s]global step 321000, epoch: 2, batch: 124649, loss: 1.40969, acc: 0.92584, time: Sun Jan 23 08:11:33 2022
epoch 2 training:  64%|██████▎   | 124748/196351 [17:46:27<10:16:18,  1.94it/s]global step 321100, epoch: 2, batch: 124749, loss: 1.21826, acc: 0.92582, time: Sun Jan 23 08:12:24 2022
epoch 2 training:  64%|██████▎   | 124848/196351 [17:47:19<10:11:02,  1.95it/s]global step 321200, epoch: 2, batch: 124849, loss: 1.34443, acc: 0.92582, time: Sun Jan 23 08:13:16 2022
epoch 2 training:  64%|██████▎   | 124948/196351 [17:48:11<10:10:42,  1.95it/s]global step 321300, epoch: 2, batch: 124949, loss: 1.48795, acc: 0.92581, time: Sun Jan 23 08:14:08 2022
epoch 2 training:  64%|██████▎   | 125048/196351 [17:49:02<10:02:13,  1.97it/s]global step 321400, epoch: 2, batch: 125049, loss: 1.48151, acc: 0.92583, time: Sun Jan 23 08:15:00 2022
epoch 2 training:  64%|██████▎   | 125148/196351 [17:49:54<10:27:39,  1.89it/s]global step 321500, epoch: 2, batch: 125149, loss: 1.06049, acc: 0.92584, time: Sun Jan 23 08:15:51 2022
epoch 2 training:  64%|██████▍   | 125248/196351 [17:50:45<10:02:56,  1.97it/s]global step 321600, epoch: 2, batch: 125249, loss: 1.05074, acc: 0.92585, time: Sun Jan 23 08:16:43 2022
epoch 2 training:  64%|██████▍   | 125348/196351 [17:51:37<10:08:27,  1.94it/s]global step 321700, epoch: 2, batch: 125349, loss: 3.65795, acc: 0.92584, time: Sun Jan 23 08:17:34 2022
epoch 2 training:  64%|██████▍   | 125448/196351 [17:52:28<10:14:26,  1.92it/s]global step 321800, epoch: 2, batch: 125449, loss: 1.72110, acc: 0.92586, time: Sun Jan 23 08:18:26 2022
epoch 2 training:  64%|██████▍   | 125548/196351 [17:53:19<10:01:03,  1.96it/s]global step 321900, epoch: 2, batch: 125549, loss: 0.79514, acc: 0.92587, time: Sun Jan 23 08:19:17 2022
epoch 2 training:  64%|██████▍   | 125648/196351 [17:54:10<10:04:38,  1.95it/s]global step 322000, epoch: 2, batch: 125649, loss: 1.92226, acc: 0.92586, time: Sun Jan 23 08:20:08 2022
epoch 2 training:  64%|██████▍   | 125748/196351 [17:55:01<10:00:06,  1.96it/s]global step 322100, epoch: 2, batch: 125749, loss: 1.38218, acc: 0.92588, time: Sun Jan 23 08:20:59 2022
epoch 2 training:  64%|██████▍   | 125848/196351 [17:55:52<10:01:50,  1.95it/s]global step 322200, epoch: 2, batch: 125849, loss: 2.61746, acc: 0.92587, time: Sun Jan 23 08:21:50 2022
epoch 2 training:  64%|██████▍   | 125948/196351 [17:56:43<9:53:21,  1.98it/s]global step 322300, epoch: 2, batch: 125949, loss: 1.77963, acc: 0.92586, time: Sun Jan 23 08:22:41 2022
epoch 2 training:  64%|██████▍   | 126048/196351 [17:57:34<9:53:44,  1.97it/s]global step 322400, epoch: 2, batch: 126049, loss: 1.02210, acc: 0.92587, time: Sun Jan 23 08:23:32 2022
epoch 2 training:  64%|██████▍   | 126148/196351 [17:58:25<10:09:11,  1.92it/s]global step 322500, epoch: 2, batch: 126149, loss: 0.55667, acc: 0.92589, time: Sun Jan 23 08:24:23 2022
epoch 2 training:  64%|██████▍   | 126248/196351 [17:59:17<10:05:22,  1.93it/s]global step 322600, epoch: 2, batch: 126249, loss: 3.23926, acc: 0.92592, time: Sun Jan 23 08:25:14 2022
epoch 2 training:  64%|██████▍   | 126348/196351 [18:00:09<10:08:46,  1.92it/s]global step 322700, epoch: 2, batch: 126349, loss: 2.06874, acc: 0.92592, time: Sun Jan 23 08:26:06 2022
epoch 2 training:  64%|██████▍   | 126448/196351 [18:01:01<10:03:17,  1.93it/s]global step 322800, epoch: 2, batch: 126449, loss: 2.66602, acc: 0.92595, time: Sun Jan 23 08:26:58 2022
epoch 2 training:  64%|██████▍   | 126548/196351 [18:01:52<10:04:56,  1.92it/s]global step 322900, epoch: 2, batch: 126549, loss: 1.58491, acc: 0.92596, time: Sun Jan 23 08:27:50 2022
epoch 2 training:  65%|██████▍   | 126648/196351 [18:02:44<9:59:51,  1.94it/s] global step 323000, epoch: 2, batch: 126649, loss: 3.07938, acc: 0.92597, time: Sun Jan 23 08:28:41 2022
epoch 2 training:  65%|██████▍   | 126748/196351 [18:03:35<9:57:06,  1.94it/s]global step 323100, epoch: 2, batch: 126749, loss: 0.54904, acc: 0.92596, time: Sun Jan 23 08:29:33 2022
epoch 2 training:  65%|██████▍   | 126848/196351 [18:04:26<9:55:46,  1.94it/s]global step 323200, epoch: 2, batch: 126849, loss: 0.53950, acc: 0.92598, time: Sun Jan 23 08:30:24 2022
epoch 2 training:  65%|██████▍   | 126948/196351 [18:05:17<9:57:14,  1.94it/s]global step 323300, epoch: 2, batch: 126949, loss: 0.72561, acc: 0.92598, time: Sun Jan 23 08:31:15 2022
epoch 2 training:  65%|██████▍   | 127048/196351 [18:06:08<9:40:54,  1.99it/s]global step 323400, epoch: 2, batch: 127049, loss: 0.69520, acc: 0.92600, time: Sun Jan 23 08:32:06 2022
epoch 2 training:  65%|██████▍   | 127148/196351 [18:07:00<9:49:11,  1.96it/s]global step 323500, epoch: 2, batch: 127149, loss: 0.33516, acc: 0.92602, time: Sun Jan 23 08:32:57 2022
epoch 2 training:  65%|██████▍   | 127248/196351 [18:07:51<9:46:03,  1.97it/s]global step 323600, epoch: 2, batch: 127249, loss: 0.10062, acc: 0.92602, time: Sun Jan 23 08:33:48 2022
epoch 2 training:  65%|██████▍   | 127348/196351 [18:08:42<9:44:54,  1.97it/s]global step 323700, epoch: 2, batch: 127349, loss: 1.22544, acc: 0.92603, time: Sun Jan 23 08:34:39 2022
epoch 2 training:  65%|██████▍   | 127448/196351 [18:09:33<9:52:57,  1.94it/s]global step 323800, epoch: 2, batch: 127449, loss: 2.11194, acc: 0.92603, time: Sun Jan 23 08:35:31 2022
epoch 2 training:  65%|██████▍   | 127548/196351 [18:10:25<9:47:49,  1.95it/s]global step 323900, epoch: 2, batch: 127549, loss: 1.58267, acc: 0.92603, time: Sun Jan 23 08:36:22 2022
epoch 2 training:  65%|██████▌   | 127648/196351 [18:11:16<9:58:21,  1.91it/s]global step 324000, epoch: 2, batch: 127649, loss: 0.26223, acc: 0.92605, time: Sun Jan 23 08:37:14 2022
epoch 2 training:  65%|██████▌   | 127748/196351 [18:12:08<9:56:39,  1.92it/s]global step 324100, epoch: 2, batch: 127749, loss: 1.13625, acc: 0.92604, time: Sun Jan 23 08:38:05 2022
epoch 2 training:  65%|██████▌   | 127848/196351 [18:12:59<9:50:23,  1.93it/s]global step 324200, epoch: 2, batch: 127849, loss: 1.79727, acc: 0.92605, time: Sun Jan 23 08:38:57 2022
epoch 2 training:  65%|██████▌   | 127948/196351 [18:13:50<9:43:03,  1.96it/s]global step 324300, epoch: 2, batch: 127949, loss: 2.83570, acc: 0.92607, time: Sun Jan 23 08:39:48 2022
epoch 2 training:  65%|██████▌   | 128048/196351 [18:14:42<10:01:25,  1.89it/s]global step 324400, epoch: 2, batch: 128049, loss: 0.13657, acc: 0.92607, time: Sun Jan 23 08:40:39 2022
epoch 2 training:  65%|██████▌   | 128148/196351 [18:15:34<9:52:03,  1.92it/s]global step 324500, epoch: 2, batch: 128149, loss: 1.49413, acc: 0.92608, time: Sun Jan 23 08:41:31 2022
epoch 2 training:  65%|██████▌   | 128248/196351 [18:16:25<9:49:00,  1.93it/s]global step 324600, epoch: 2, batch: 128249, loss: 1.62322, acc: 0.92610, time: Sun Jan 23 08:42:23 2022
epoch 2 training:  65%|██████▌   | 128348/196351 [18:17:17<9:49:13,  1.92it/s]global step 324700, epoch: 2, batch: 128349, loss: 0.61971, acc: 0.92610, time: Sun Jan 23 08:43:14 2022
epoch 2 training:  65%|██████▌   | 128448/196351 [18:18:08<9:43:02,  1.94it/s]global step 324800, epoch: 2, batch: 128449, loss: 3.51134, acc: 0.92612, time: Sun Jan 23 08:44:06 2022
epoch 2 training:  65%|██████▌   | 128548/196351 [18:19:00<9:45:25,  1.93it/s]global step 324900, epoch: 2, batch: 128549, loss: 1.97955, acc: 0.92611, time: Sun Jan 23 08:44:57 2022
epoch 2 training:  66%|██████▌   | 128648/196351 [18:19:51<9:46:58,  1.92it/s]global step 325000, epoch: 2, batch: 128649, loss: 2.89438, acc: 0.92611, time: Sun Jan 23 08:45:49 2022
epoch 2 training:  66%|██████▌   | 128748/196351 [18:20:42<9:35:47,  1.96it/s]global step 325100, epoch: 2, batch: 128749, loss: 1.70786, acc: 0.92612, time: Sun Jan 23 08:46:40 2022
epoch 2 training:  66%|██████▌   | 128848/196351 [18:21:34<9:40:16,  1.94it/s]global step 325200, epoch: 2, batch: 128849, loss: 0.36714, acc: 0.92610, time: Sun Jan 23 08:47:31 2022
epoch 2 training:  66%|██████▌   | 128948/196351 [18:22:26<9:26:46,  1.98it/s]global step 325300, epoch: 2, batch: 128949, loss: 0.37353, acc: 0.92612, time: Sun Jan 23 08:48:23 2022
epoch 2 training:  66%|██████▌   | 129048/196351 [18:23:17<9:33:05,  1.96it/s]global step 325400, epoch: 2, batch: 129049, loss: 0.37856, acc: 0.92612, time: Sun Jan 23 08:49:14 2022
epoch 2 training:  66%|██████▌   | 129148/196351 [18:24:08<9:27:00,  1.98it/s]global step 325500, epoch: 2, batch: 129149, loss: 1.15783, acc: 0.92612, time: Sun Jan 23 08:50:06 2022
epoch 2 training:  66%|██████▌   | 129248/196351 [18:25:00<9:24:41,  1.98it/s]global step 325600, epoch: 2, batch: 129249, loss: 2.75727, acc: 0.92614, time: Sun Jan 23 08:50:57 2022
epoch 2 training:  66%|██████▌   | 129348/196351 [18:25:50<9:17:30,  2.00it/s]global step 325700, epoch: 2, batch: 129349, loss: 2.15917, acc: 0.92613, time: Sun Jan 23 08:51:48 2022
epoch 2 training:  66%|██████▌   | 129448/196351 [18:26:41<9:27:15,  1.97it/s]global step 325800, epoch: 2, batch: 129449, loss: 2.79274, acc: 0.92612, time: Sun Jan 23 08:52:39 2022
epoch 2 training:  66%|██████▌   | 129548/196351 [18:27:32<9:24:16,  1.97it/s]global step 325900, epoch: 2, batch: 129549, loss: 1.44000, acc: 0.92611, time: Sun Jan 23 08:53:29 2022
epoch 2 training:  66%|██████▌   | 129648/196351 [18:28:23<9:24:21,  1.97it/s]global step 326000, epoch: 2, batch: 129649, loss: 1.16376, acc: 0.92611, time: Sun Jan 23 08:54:20 2022
epoch 2 training:  66%|██████▌   | 129748/196351 [18:29:14<9:16:29,  1.99it/s]global step 326100, epoch: 2, batch: 129749, loss: 2.92832, acc: 0.92610, time: Sun Jan 23 08:55:11 2022
epoch 2 training:  66%|██████▌   | 129848/196351 [18:30:05<9:32:32,  1.94it/s]global step 326200, epoch: 2, batch: 129849, loss: 0.38110, acc: 0.92610, time: Sun Jan 23 08:56:02 2022
epoch 2 training:  66%|██████▌   | 129948/196351 [18:30:57<9:30:24,  1.94it/s]global step 326300, epoch: 2, batch: 129949, loss: 0.89172, acc: 0.92613, time: Sun Jan 23 08:56:54 2022
epoch 2 training:  66%|██████▌   | 130048/196351 [18:31:49<9:37:37,  1.91it/s]global step 326400, epoch: 2, batch: 130049, loss: 0.38482, acc: 0.92616, time: Sun Jan 23 08:57:46 2022
epoch 2 training:  66%|██████▋   | 130148/196351 [18:32:40<9:25:20,  1.95it/s]global step 326500, epoch: 2, batch: 130149, loss: 1.40198, acc: 0.92617, time: Sun Jan 23 08:58:38 2022
epoch 2 training:  66%|██████▋   | 130248/196351 [18:33:32<9:22:48,  1.96it/s]global step 326600, epoch: 2, batch: 130249, loss: 0.77330, acc: 0.92617, time: Sun Jan 23 08:59:29 2022
epoch 2 training:  66%|██████▋   | 130348/196351 [18:34:23<9:17:56,  1.97it/s]global step 326700, epoch: 2, batch: 130349, loss: 0.20356, acc: 0.92617, time: Sun Jan 23 09:00:21 2022
epoch 2 training:  66%|██████▋   | 130448/196351 [18:35:15<9:24:14,  1.95it/s]global step 326800, epoch: 2, batch: 130449, loss: 0.05138, acc: 0.92617, time: Sun Jan 23 09:01:12 2022
epoch 2 training:  66%|██████▋   | 130548/196351 [18:36:06<9:31:00,  1.92it/s]global step 326900, epoch: 2, batch: 130549, loss: 2.26858, acc: 0.92618, time: Sun Jan 23 09:02:04 2022
epoch 2 training:  67%|██████▋   | 130648/196351 [18:36:58<9:33:13,  1.91it/s]global step 327000, epoch: 2, batch: 130649, loss: 2.70087, acc: 0.92620, time: Sun Jan 23 09:02:56 2022
epoch 2 training:  67%|██████▋   | 130748/196351 [18:37:50<9:26:54,  1.93it/s]global step 327100, epoch: 2, batch: 130749, loss: 0.83549, acc: 0.92618, time: Sun Jan 23 09:03:47 2022
epoch 2 training:  67%|██████▋   | 130848/196351 [18:38:41<9:22:50,  1.94it/s]global step 327200, epoch: 2, batch: 130849, loss: 1.03119, acc: 0.92620, time: Sun Jan 23 09:04:39 2022
epoch 2 training:  67%|██████▋   | 130948/196351 [18:39:32<9:17:07,  1.96it/s]global step 327300, epoch: 2, batch: 130949, loss: 2.41914, acc: 0.92618, time: Sun Jan 23 09:05:30 2022
epoch 2 training:  67%|██████▋   | 131048/196351 [18:40:24<9:24:41,  1.93it/s]global step 327400, epoch: 2, batch: 131049, loss: 3.65859, acc: 0.92618, time: Sun Jan 23 09:06:21 2022
epoch 2 training:  67%|██████▋   | 131148/196351 [18:41:15<9:29:58,  1.91it/s]global step 327500, epoch: 2, batch: 131149, loss: 3.92248, acc: 0.92617, time: Sun Jan 23 09:07:13 2022
epoch 2 training:  67%|██████▋   | 131248/196351 [18:42:07<9:12:05,  1.97it/s]global step 327600, epoch: 2, batch: 131249, loss: 0.05332, acc: 0.92616, time: Sun Jan 23 09:08:04 2022
epoch 2 training:  67%|██████▋   | 131348/196351 [18:42:58<9:10:05,  1.97it/s]global step 327700, epoch: 2, batch: 131349, loss: 1.57327, acc: 0.92617, time: Sun Jan 23 09:08:55 2022
epoch 2 training:  67%|██████▋   | 131448/196351 [18:43:49<9:22:55,  1.92it/s]global step 327800, epoch: 2, batch: 131449, loss: 2.39730, acc: 0.92617, time: Sun Jan 23 09:09:47 2022
epoch 2 training:  67%|██████▋   | 131548/196351 [18:44:41<9:10:42,  1.96it/s]global step 327900, epoch: 2, batch: 131549, loss: 5.97045, acc: 0.92616, time: Sun Jan 23 09:10:39 2022
epoch 2 training:  67%|██████▋   | 131648/196351 [18:45:32<9:15:00,  1.94it/s]global step 328000, epoch: 2, batch: 131649, loss: 1.88517, acc: 0.92617, time: Sun Jan 23 09:11:30 2022
epoch 2 training:  67%|██████▋   | 131748/196351 [18:46:24<9:16:40,  1.93it/s]global step 328100, epoch: 2, batch: 131749, loss: 1.99860, acc: 0.92616, time: Sun Jan 23 09:12:21 2022
epoch 2 training:  67%|██████▋   | 131848/196351 [18:47:16<9:19:51,  1.92it/s]global step 328200, epoch: 2, batch: 131849, loss: 0.38288, acc: 0.92617, time: Sun Jan 23 09:13:13 2022
epoch 2 training:  67%|██████▋   | 131948/196351 [18:48:07<9:24:26,  1.90it/s]global step 328300, epoch: 2, batch: 131949, loss: 0.38412, acc: 0.92616, time: Sun Jan 23 09:14:05 2022
epoch 2 training:  67%|██████▋   | 132048/196351 [18:48:59<9:11:10,  1.94it/s]global step 328400, epoch: 2, batch: 132049, loss: 0.60262, acc: 0.92616, time: Sun Jan 23 09:14:57 2022
epoch 2 training:  67%|██████▋   | 132148/196351 [18:49:51<9:08:51,  1.95it/s]global step 328500, epoch: 2, batch: 132149, loss: 1.96159, acc: 0.92616, time: Sun Jan 23 09:15:48 2022
epoch 2 training:  67%|██████▋   | 132248/196351 [18:50:42<9:18:54,  1.91it/s]global step 328600, epoch: 2, batch: 132249, loss: 3.00303, acc: 0.92616, time: Sun Jan 23 09:16:40 2022
epoch 2 training:  67%|██████▋   | 132348/196351 [18:51:34<8:58:23,  1.98it/s]global step 328700, epoch: 2, batch: 132349, loss: 2.56879, acc: 0.92613, time: Sun Jan 23 09:17:31 2022
epoch 2 training:  67%|██████▋   | 132448/196351 [18:52:25<9:00:18,  1.97it/s]global step 328800, epoch: 2, batch: 132449, loss: 1.05937, acc: 0.92614, time: Sun Jan 23 09:18:22 2022
epoch 2 training:  68%|██████▊   | 132548/196351 [18:53:16<8:52:31,  2.00it/s]global step 328900, epoch: 2, batch: 132549, loss: 2.56553, acc: 0.92611, time: Sun Jan 23 09:19:13 2022
epoch 2 training:  68%|██████▊   | 132648/196351 [18:54:07<8:55:22,  1.98it/s]global step 329000, epoch: 2, batch: 132649, loss: 2.78953, acc: 0.92613, time: Sun Jan 23 09:20:04 2022
epoch 2 training:  68%|██████▊   | 132748/196351 [18:54:58<8:51:44,  1.99it/s]global step 329100, epoch: 2, batch: 132749, loss: 2.10010, acc: 0.92615, time: Sun Jan 23 09:20:55 2022
epoch 2 training:  68%|██████▊   | 132848/196351 [18:55:49<8:56:01,  1.97it/s]global step 329200, epoch: 2, batch: 132849, loss: 1.04747, acc: 0.92615, time: Sun Jan 23 09:21:46 2022
epoch 2 training:  68%|██████▊   | 132948/196351 [18:56:40<9:07:39,  1.93it/s]global step 329300, epoch: 2, batch: 132949, loss: 0.31085, acc: 0.92616, time: Sun Jan 23 09:22:38 2022
epoch 2 training:  68%|██████▊   | 133048/196351 [18:57:31<9:04:19,  1.94it/s]global step 329400, epoch: 2, batch: 133049, loss: 0.35135, acc: 0.92618, time: Sun Jan 23 09:23:28 2022
epoch 2 training:  68%|██████▊   | 133148/196351 [18:58:22<8:56:52,  1.96it/s]global step 329500, epoch: 2, batch: 133149, loss: 0.74254, acc: 0.92620, time: Sun Jan 23 09:24:20 2022
epoch 2 training:  68%|██████▊   | 133248/196351 [18:59:14<9:11:23,  1.91it/s]global step 329600, epoch: 2, batch: 133249, loss: 0.36969, acc: 0.92622, time: Sun Jan 23 09:25:11 2022
epoch 2 training:  68%|██████▊   | 133348/196351 [19:00:05<8:53:02,  1.97it/s]global step 329700, epoch: 2, batch: 133349, loss: 2.27235, acc: 0.92621, time: Sun Jan 23 09:26:03 2022
epoch 2 training:  68%|██████▊   | 133448/196351 [19:00:57<9:07:44,  1.91it/s]global step 329800, epoch: 2, batch: 133449, loss: 0.89172, acc: 0.92622, time: Sun Jan 23 09:26:54 2022
epoch 2 training:  68%|██████▊   | 133548/196351 [19:01:48<9:05:23,  1.92it/s]global step 329900, epoch: 2, batch: 133549, loss: 1.43660, acc: 0.92622, time: Sun Jan 23 09:27:46 2022
epoch 2 training:  68%|██████▊   | 133648/196351 [19:02:40<8:55:20,  1.95it/s]global step 330000, epoch: 2, batch: 133649, loss: 1.05848, acc: 0.92621, time: Sun Jan 23 09:28:37 2022
epoch 2 training:  68%|██████▊   | 133748/196351 [19:03:31<9:20:19,  1.86it/s]global step 330100, epoch: 2, batch: 133749, loss: 2.92549, acc: 0.92620, time: Sun Jan 23 09:29:29 2022
epoch 2 training:  68%|██████▊   | 133848/196351 [19:04:23<8:50:28,  1.96it/s]global step 330200, epoch: 2, batch: 133849, loss: 4.38491, acc: 0.92620, time: Sun Jan 23 09:30:20 2022
epoch 2 training:  68%|██████▊   | 133948/196351 [19:05:13<8:44:42,  1.98it/s]global step 330300, epoch: 2, batch: 133949, loss: 2.62939, acc: 0.92621, time: Sun Jan 23 09:31:11 2022
epoch 2 training:  68%|██████▊   | 134048/196351 [19:06:05<8:40:24,  2.00it/s]global step 330400, epoch: 2, batch: 134049, loss: 1.68831, acc: 0.92619, time: Sun Jan 23 09:32:02 2022
epoch 2 training:  68%|██████▊   | 134148/196351 [19:06:55<9:05:15,  1.90it/s]global step 330500, epoch: 2, batch: 134149, loss: 2.14859, acc: 0.92619, time: Sun Jan 23 09:32:53 2022
epoch 2 training:  68%|██████▊   | 134248/196351 [19:07:46<8:45:03,  1.97it/s]global step 330600, epoch: 2, batch: 134249, loss: 3.19898, acc: 0.92619, time: Sun Jan 23 09:33:44 2022
epoch 2 training:  68%|██████▊   | 134348/196351 [19:08:37<8:42:44,  1.98it/s]global step 330700, epoch: 2, batch: 134349, loss: 0.89142, acc: 0.92620, time: Sun Jan 23 09:34:35 2022
epoch 2 training:  68%|██████▊   | 134448/196351 [19:09:29<9:02:10,  1.90it/s]global step 330800, epoch: 2, batch: 134449, loss: 0.60098, acc: 0.92619, time: Sun Jan 23 09:35:26 2022
epoch 2 training:  69%|██████▊   | 134548/196351 [19:10:20<8:53:55,  1.93it/s]global step 330900, epoch: 2, batch: 134549, loss: 1.03188, acc: 0.92618, time: Sun Jan 23 09:36:18 2022
epoch 2 training:  69%|██████▊   | 134648/196351 [19:11:12<8:41:23,  1.97it/s]global step 331000, epoch: 2, batch: 134649, loss: 0.74686, acc: 0.92615, time: Sun Jan 23 09:37:09 2022
epoch 2 training:  69%|██████▊   | 134748/196351 [19:12:04<8:54:14,  1.92it/s]global step 331100, epoch: 2, batch: 134749, loss: 2.36217, acc: 0.92617, time: Sun Jan 23 09:38:01 2022
epoch 2 training:  69%|██████▊   | 134848/196351 [19:12:55<8:52:05,  1.93it/s]global step 331200, epoch: 2, batch: 134849, loss: 0.59087, acc: 0.92617, time: Sun Jan 23 09:38:53 2022
epoch 2 training:  69%|██████▊   | 134948/196351 [19:13:47<8:43:07,  1.96it/s]global step 331300, epoch: 2, batch: 134949, loss: 1.40877, acc: 0.92619, time: Sun Jan 23 09:39:44 2022
epoch 2 training:  69%|██████▉   | 135048/196351 [19:14:38<8:43:27,  1.95it/s]global step 331400, epoch: 2, batch: 135049, loss: 0.90502, acc: 0.92622, time: Sun Jan 23 09:40:36 2022
epoch 2 training:  69%|██████▉   | 135148/196351 [19:15:30<8:54:00,  1.91it/s]global step 331500, epoch: 2, batch: 135149, loss: 0.15502, acc: 0.92624, time: Sun Jan 23 09:41:27 2022
epoch 2 training:  69%|██████▉   | 135248/196351 [19:16:21<8:55:18,  1.90it/s]global step 331600, epoch: 2, batch: 135249, loss: 1.31412, acc: 0.92624, time: Sun Jan 23 09:42:19 2022
epoch 2 training:  69%|██████▉   | 135348/196351 [19:17:13<8:40:06,  1.95it/s]global step 331700, epoch: 2, batch: 135349, loss: 1.78429, acc: 0.92622, time: Sun Jan 23 09:43:10 2022
epoch 2 training:  69%|██████▉   | 135448/196351 [19:18:05<8:40:21,  1.95it/s]global step 331800, epoch: 2, batch: 135449, loss: 0.34097, acc: 0.92625, time: Sun Jan 23 09:44:02 2022
epoch 2 training:  69%|██████▉   | 135548/196351 [19:18:56<8:36:42,  1.96it/s]global step 331900, epoch: 2, batch: 135549, loss: 2.04954, acc: 0.92625, time: Sun Jan 23 09:44:54 2022
epoch 2 training:  69%|██████▉   | 135648/196351 [19:19:47<8:29:51,  1.98it/s]global step 332000, epoch: 2, batch: 135649, loss: 0.59706, acc: 0.92622, time: Sun Jan 23 09:45:44 2022
epoch 2 training:  69%|██████▉   | 135748/196351 [19:20:38<8:45:31,  1.92it/s]global step 332100, epoch: 2, batch: 135749, loss: 1.32707, acc: 0.92623, time: Sun Jan 23 09:46:36 2022
epoch 2 training:  69%|██████▉   | 135848/196351 [19:21:30<9:04:29,  1.85it/s]global step 332200, epoch: 2, batch: 135849, loss: 0.34765, acc: 0.92623, time: Sun Jan 23 09:47:28 2022
epoch 2 training:  69%|██████▉   | 135948/196351 [19:22:22<8:45:49,  1.91it/s]global step 332300, epoch: 2, batch: 135949, loss: 1.78982, acc: 0.92623, time: Sun Jan 23 09:48:20 2022
epoch 2 training:  69%|██████▉   | 136048/196351 [19:23:14<8:34:21,  1.95it/s]global step 332400, epoch: 2, batch: 136049, loss: 0.25880, acc: 0.92623, time: Sun Jan 23 09:49:11 2022
epoch 2 training:  69%|██████▉   | 136148/196351 [19:24:06<8:42:56,  1.92it/s]global step 332500, epoch: 2, batch: 136149, loss: 1.91078, acc: 0.92622, time: Sun Jan 23 09:50:03 2022
epoch 2 training:  69%|██████▉   | 136248/196351 [19:24:57<8:37:59,  1.93it/s]global step 332600, epoch: 2, batch: 136249, loss: 0.26729, acc: 0.92620, time: Sun Jan 23 09:50:55 2022
epoch 2 training:  69%|██████▉   | 136348/196351 [19:25:49<8:25:23,  1.98it/s]global step 332700, epoch: 2, batch: 136349, loss: 2.96867, acc: 0.92621, time: Sun Jan 23 09:51:46 2022
epoch 2 training:  69%|██████▉   | 136448/196351 [19:26:40<8:40:25,  1.92it/s]global step 332800, epoch: 2, batch: 136449, loss: 3.38851, acc: 0.92623, time: Sun Jan 23 09:52:38 2022
epoch 2 training:  70%|██████▉   | 136548/196351 [19:27:32<8:25:46,  1.97it/s]global step 332900, epoch: 2, batch: 136549, loss: 1.89614, acc: 0.92622, time: Sun Jan 23 09:53:30 2022
epoch 2 training:  70%|██████▉   | 136648/196351 [19:28:24<8:27:08,  1.96it/s]global step 333000, epoch: 2, batch: 136649, loss: 0.60562, acc: 0.92622, time: Sun Jan 23 09:54:21 2022
epoch 2 training:  70%|██████▉   | 136748/196351 [19:29:15<8:37:07,  1.92it/s]global step 333100, epoch: 2, batch: 136749, loss: 0.29688, acc: 0.92623, time: Sun Jan 23 09:55:13 2022
epoch 2 training:  70%|██████▉   | 136848/196351 [19:30:07<8:25:42,  1.96it/s]global step 333200, epoch: 2, batch: 136849, loss: 0.94481, acc: 0.92625, time: Sun Jan 23 09:56:04 2022
epoch 2 training:  70%|██████▉   | 136948/196351 [19:30:59<8:32:28,  1.93it/s]global step 333300, epoch: 2, batch: 136949, loss: 0.95990, acc: 0.92626, time: Sun Jan 23 09:56:56 2022
epoch 2 training:  70%|██████▉   | 137048/196351 [19:31:50<8:25:56,  1.95it/s]global step 333400, epoch: 2, batch: 137049, loss: 1.82944, acc: 0.92627, time: Sun Jan 23 09:57:48 2022
epoch 2 training:  70%|██████▉   | 137148/196351 [19:32:42<8:26:23,  1.95it/s]global step 333500, epoch: 2, batch: 137149, loss: 0.49252, acc: 0.92629, time: Sun Jan 23 09:58:39 2022
epoch 2 training:  70%|██████▉   | 137248/196351 [19:33:33<8:20:00,  1.97it/s]global step 333600, epoch: 2, batch: 137249, loss: 2.02499, acc: 0.92630, time: Sun Jan 23 09:59:30 2022
epoch 2 training:  70%|██████▉   | 137348/196351 [19:34:24<8:21:37,  1.96it/s]global step 333700, epoch: 2, batch: 137349, loss: 3.20153, acc: 0.92632, time: Sun Jan 23 10:00:22 2022
epoch 2 training:  70%|███████   | 137448/196351 [19:35:16<8:20:48,  1.96it/s]global step 333800, epoch: 2, batch: 137449, loss: 0.37509, acc: 0.92633, time: Sun Jan 23 10:01:13 2022
epoch 2 training:  70%|███████   | 137548/196351 [19:36:07<8:19:26,  1.96it/s]global step 333900, epoch: 2, batch: 137549, loss: 2.41289, acc: 0.92630, time: Sun Jan 23 10:02:04 2022
epoch 2 training:  70%|███████   | 137648/196351 [19:36:58<8:10:53,  1.99it/s]global step 334000, epoch: 2, batch: 137649, loss: 3.18315, acc: 0.92631, time: Sun Jan 23 10:02:55 2022
epoch 2 training:  70%|███████   | 137748/196351 [19:37:49<8:20:24,  1.95it/s]global step 334100, epoch: 2, batch: 137749, loss: 0.38793, acc: 0.92634, time: Sun Jan 23 10:03:47 2022
epoch 2 training:  70%|███████   | 137848/196351 [19:38:40<8:23:56,  1.93it/s]global step 334200, epoch: 2, batch: 137849, loss: 1.14807, acc: 0.92635, time: Sun Jan 23 10:04:38 2022
epoch 2 training:  70%|███████   | 137948/196351 [19:39:32<8:19:11,  1.95it/s]global step 334300, epoch: 2, batch: 137949, loss: 0.07497, acc: 0.92635, time: Sun Jan 23 10:05:30 2022
epoch 2 training:  70%|███████   | 138048/196351 [19:40:24<8:21:11,  1.94it/s]global step 334400, epoch: 2, batch: 138049, loss: 2.20007, acc: 0.92634, time: Sun Jan 23 10:06:21 2022
epoch 2 training:  70%|███████   | 138148/196351 [19:41:15<8:10:56,  1.98it/s]global step 334500, epoch: 2, batch: 138149, loss: 1.13540, acc: 0.92633, time: Sun Jan 23 10:07:13 2022
epoch 2 training:  70%|███████   | 138248/196351 [19:42:06<8:14:45,  1.96it/s]global step 334600, epoch: 2, batch: 138249, loss: 0.86414, acc: 0.92632, time: Sun Jan 23 10:08:04 2022
epoch 2 training:  70%|███████   | 138348/196351 [19:42:57<8:11:37,  1.97it/s]global step 334700, epoch: 2, batch: 138349, loss: 1.05669, acc: 0.92633, time: Sun Jan 23 10:08:55 2022
epoch 2 training:  71%|███████   | 138448/196351 [19:43:49<8:19:41,  1.93it/s]global step 334800, epoch: 2, batch: 138449, loss: 2.25677, acc: 0.92635, time: Sun Jan 23 10:09:46 2022
epoch 2 training:  71%|███████   | 138548/196351 [19:44:40<8:14:00,  1.95it/s]global step 334900, epoch: 2, batch: 138549, loss: 3.13823, acc: 0.92635, time: Sun Jan 23 10:10:37 2022
epoch 2 training:  71%|███████   | 138648/196351 [19:45:31<8:12:11,  1.95it/s]global step 335000, epoch: 2, batch: 138649, loss: 1.63453, acc: 0.92634, time: Sun Jan 23 10:11:29 2022
epoch 2 training:  71%|███████   | 138748/196351 [19:46:22<8:07:00,  1.97it/s]global step 335100, epoch: 2, batch: 138749, loss: 2.33308, acc: 0.92635, time: Sun Jan 23 10:12:19 2022
epoch 2 training:  71%|███████   | 138848/196351 [19:47:14<8:16:24,  1.93it/s]global step 335200, epoch: 2, batch: 138849, loss: 1.21706, acc: 0.92637, time: Sun Jan 23 10:13:11 2022
epoch 2 training:  71%|███████   | 138948/196351 [19:48:05<8:15:35,  1.93it/s]global step 335300, epoch: 2, batch: 138949, loss: 1.14643, acc: 0.92637, time: Sun Jan 23 10:14:02 2022
epoch 2 training:  71%|███████   | 139048/196351 [19:48:56<8:04:10,  1.97it/s]global step 335400, epoch: 2, batch: 139049, loss: 2.26880, acc: 0.92639, time: Sun Jan 23 10:14:54 2022
epoch 2 training:  71%|███████   | 139148/196351 [19:49:48<8:10:02,  1.95it/s]global step 335500, epoch: 2, batch: 139149, loss: 1.16917, acc: 0.92639, time: Sun Jan 23 10:15:45 2022
epoch 2 training:  71%|███████   | 139248/196351 [19:50:39<8:08:27,  1.95it/s]global step 335600, epoch: 2, batch: 139249, loss: 1.69319, acc: 0.92641, time: Sun Jan 23 10:16:37 2022
epoch 2 training:  71%|███████   | 139348/196351 [19:51:31<8:10:15,  1.94it/s]global step 335700, epoch: 2, batch: 139349, loss: 2.18295, acc: 0.92641, time: Sun Jan 23 10:17:29 2022
epoch 2 training:  71%|███████   | 139448/196351 [19:52:23<8:01:09,  1.97it/s]global step 335800, epoch: 2, batch: 139449, loss: 1.67278, acc: 0.92643, time: Sun Jan 23 10:18:20 2022
epoch 2 training:  71%|███████   | 139548/196351 [19:53:14<8:04:59,  1.95it/s]global step 335900, epoch: 2, batch: 139549, loss: 1.26612, acc: 0.92640, time: Sun Jan 23 10:19:11 2022
epoch 2 training:  71%|███████   | 139648/196351 [19:54:05<8:06:17,  1.94it/s]global step 336000, epoch: 2, batch: 139649, loss: 1.62006, acc: 0.92639, time: Sun Jan 23 10:20:02 2022
epoch 2 training:  71%|███████   | 139748/196351 [19:54:57<8:03:17,  1.95it/s]global step 336100, epoch: 2, batch: 139749, loss: 1.51027, acc: 0.92639, time: Sun Jan 23 10:20:54 2022
epoch 2 training:  71%|███████   | 139848/196351 [19:55:48<8:07:34,  1.93it/s]global step 336200, epoch: 2, batch: 139849, loss: 2.17033, acc: 0.92640, time: Sun Jan 23 10:21:46 2022
epoch 2 training:  71%|███████▏  | 139948/196351 [19:56:40<8:06:47,  1.93it/s]global step 336300, epoch: 2, batch: 139949, loss: 1.09545, acc: 0.92641, time: Sun Jan 23 10:22:37 2022
epoch 2 training:  71%|███████▏  | 140048/196351 [19:57:31<8:06:46,  1.93it/s]global step 336400, epoch: 2, batch: 140049, loss: 3.50208, acc: 0.92640, time: Sun Jan 23 10:23:29 2022
epoch 2 training:  71%|███████▏  | 140148/196351 [19:58:23<7:55:33,  1.97it/s]global step 336500, epoch: 2, batch: 140149, loss: 0.87748, acc: 0.92641, time: Sun Jan 23 10:24:21 2022
epoch 2 training:  71%|███████▏  | 140248/196351 [19:59:15<8:01:28,  1.94it/s]global step 336600, epoch: 2, batch: 140249, loss: 2.89304, acc: 0.92641, time: Sun Jan 23 10:25:12 2022
epoch 2 training:  71%|███████▏  | 140348/196351 [20:00:07<8:07:15,  1.92it/s]global step 336700, epoch: 2, batch: 140349, loss: 2.14312, acc: 0.92641, time: Sun Jan 23 10:26:04 2022
epoch 2 training:  72%|███████▏  | 140448/196351 [20:00:58<8:06:33,  1.91it/s]global step 336800, epoch: 2, batch: 140449, loss: 1.53511, acc: 0.92642, time: Sun Jan 23 10:26:56 2022
epoch 2 training:  72%|███████▏  | 140548/196351 [20:01:51<7:52:43,  1.97it/s]global step 336900, epoch: 2, batch: 140549, loss: 3.92664, acc: 0.92643, time: Sun Jan 23 10:27:48 2022
epoch 2 training:  72%|███████▏  | 140648/196351 [20:02:43<8:11:48,  1.89it/s]global step 337000, epoch: 2, batch: 140649, loss: 0.93601, acc: 0.92644, time: Sun Jan 23 10:28:40 2022
epoch 2 training:  72%|███████▏  | 140748/196351 [20:03:34<7:58:17,  1.94it/s]global step 337100, epoch: 2, batch: 140749, loss: 2.01541, acc: 0.92644, time: Sun Jan 23 10:29:31 2022
epoch 2 training:  72%|███████▏  | 140848/196351 [20:04:26<8:05:42,  1.90it/s]global step 337200, epoch: 2, batch: 140849, loss: 0.66964, acc: 0.92645, time: Sun Jan 23 10:30:23 2022
epoch 2 training:  72%|███████▏  | 140948/196351 [20:05:17<7:56:59,  1.94it/s]global step 337300, epoch: 2, batch: 140949, loss: 3.10380, acc: 0.92645, time: Sun Jan 23 10:31:15 2022
epoch 2 training:  72%|███████▏  | 141048/196351 [20:06:09<7:57:49,  1.93it/s]global step 337400, epoch: 2, batch: 141049, loss: 2.16863, acc: 0.92645, time: Sun Jan 23 10:32:06 2022
epoch 2 training:  72%|███████▏  | 141148/196351 [20:07:00<7:48:13,  1.96it/s]global step 337500, epoch: 2, batch: 141149, loss: 2.51233, acc: 0.92645, time: Sun Jan 23 10:32:57 2022
epoch 2 training:  72%|███████▏  | 141248/196351 [20:07:51<7:43:59,  1.98it/s]global step 337600, epoch: 2, batch: 141249, loss: 4.36802, acc: 0.92646, time: Sun Jan 23 10:33:48 2022
epoch 2 training:  72%|███████▏  | 141348/196351 [20:08:42<7:56:12,  1.93it/s]global step 337700, epoch: 2, batch: 141349, loss: 1.31848, acc: 0.92647, time: Sun Jan 23 10:34:40 2022
epoch 2 training:  72%|███████▏  | 141448/196351 [20:09:34<7:56:04,  1.92it/s]global step 337800, epoch: 2, batch: 141449, loss: 0.72318, acc: 0.92650, time: Sun Jan 23 10:35:32 2022
epoch 2 training:  72%|███████▏  | 141548/196351 [20:10:26<7:52:17,  1.93it/s]global step 337900, epoch: 2, batch: 141549, loss: 0.20285, acc: 0.92652, time: Sun Jan 23 10:36:23 2022
epoch 2 training:  72%|███████▏  | 141648/196351 [20:11:18<7:58:07,  1.91it/s]global step 338000, epoch: 2, batch: 141649, loss: 1.99441, acc: 0.92652, time: Sun Jan 23 10:37:15 2022
epoch 2 training:  72%|███████▏  | 141748/196351 [20:12:10<7:52:49,  1.92it/s]global step 338100, epoch: 2, batch: 141749, loss: 2.22961, acc: 0.92653, time: Sun Jan 23 10:38:07 2022
epoch 2 training:  72%|███████▏  | 141848/196351 [20:13:02<7:38:34,  1.98it/s]global step 338200, epoch: 2, batch: 141849, loss: 1.67152, acc: 0.92651, time: Sun Jan 23 10:38:59 2022
epoch 2 training:  72%|███████▏  | 141948/196351 [20:13:53<7:48:59,  1.93it/s]global step 338300, epoch: 2, batch: 141949, loss: 0.76137, acc: 0.92651, time: Sun Jan 23 10:39:51 2022
epoch 2 training:  72%|███████▏  | 142048/196351 [20:14:45<7:38:13,  1.98it/s]global step 338400, epoch: 2, batch: 142049, loss: 0.88189, acc: 0.92653, time: Sun Jan 23 10:40:43 2022
epoch 2 training:  72%|███████▏  | 142148/196351 [20:15:37<7:50:13,  1.92it/s]global step 338500, epoch: 2, batch: 142149, loss: 1.03544, acc: 0.92652, time: Sun Jan 23 10:41:34 2022
epoch 2 training:  72%|███████▏  | 142248/196351 [20:16:28<7:34:48,  1.98it/s]global step 338600, epoch: 2, batch: 142249, loss: 0.37447, acc: 0.92652, time: Sun Jan 23 10:42:26 2022
epoch 2 training:  72%|███████▏  | 142348/196351 [20:17:20<7:34:39,  1.98it/s]global step 338700, epoch: 2, batch: 142349, loss: 2.15323, acc: 0.92654, time: Sun Jan 23 10:43:17 2022
epoch 2 training:  73%|███████▎  | 142448/196351 [20:18:11<7:34:21,  1.98it/s]global step 338800, epoch: 2, batch: 142449, loss: 2.22447, acc: 0.92655, time: Sun Jan 23 10:44:08 2022
epoch 2 training:  73%|███████▎  | 142548/196351 [20:19:02<7:46:25,  1.92it/s]global step 338900, epoch: 2, batch: 142549, loss: 3.75083, acc: 0.92655, time: Sun Jan 23 10:45:00 2022
epoch 2 training:  73%|███████▎  | 142648/196351 [20:19:54<7:34:12,  1.97it/s]global step 339000, epoch: 2, batch: 142649, loss: 1.59557, acc: 0.92656, time: Sun Jan 23 10:45:51 2022
epoch 2 training:  73%|███████▎  | 142748/196351 [20:20:45<7:47:42,  1.91it/s]global step 339100, epoch: 2, batch: 142749, loss: 2.27276, acc: 0.92656, time: Sun Jan 23 10:46:42 2022
epoch 2 training:  73%|███████▎  | 142848/196351 [20:21:36<7:50:03,  1.90it/s]global step 339200, epoch: 2, batch: 142849, loss: 2.30619, acc: 0.92656, time: Sun Jan 23 10:47:34 2022
epoch 2 training:  73%|███████▎  | 142948/196351 [20:22:28<7:42:48,  1.92it/s]global step 339300, epoch: 2, batch: 142949, loss: 1.06504, acc: 0.92656, time: Sun Jan 23 10:48:26 2022
epoch 2 training:  73%|███████▎  | 143048/196351 [20:23:20<7:36:27,  1.95it/s]global step 339400, epoch: 2, batch: 143049, loss: 0.24095, acc: 0.92657, time: Sun Jan 23 10:49:17 2022
epoch 2 training:  73%|███████▎  | 143148/196351 [20:24:11<7:32:39,  1.96it/s]global step 339500, epoch: 2, batch: 143149, loss: 1.07958, acc: 0.92657, time: Sun Jan 23 10:50:09 2022
epoch 2 training:  73%|███████▎  | 143248/196351 [20:25:02<7:30:34,  1.96it/s]global step 339600, epoch: 2, batch: 143249, loss: 2.96619, acc: 0.92658, time: Sun Jan 23 10:51:00 2022
epoch 2 training:  73%|███████▎  | 143348/196351 [20:25:54<7:28:58,  1.97it/s]global step 339700, epoch: 2, batch: 143349, loss: 1.19307, acc: 0.92656, time: Sun Jan 23 10:51:51 2022
epoch 2 training:  73%|███████▎  | 143448/196351 [20:26:45<7:44:50,  1.90it/s]global step 339800, epoch: 2, batch: 143449, loss: 0.62710, acc: 0.92657, time: Sun Jan 23 10:52:42 2022
epoch 2 training:  73%|███████▎  | 143548/196351 [20:27:36<7:31:16,  1.95it/s]global step 339900, epoch: 2, batch: 143549, loss: 2.58149, acc: 0.92658, time: Sun Jan 23 10:53:33 2022
epoch 2 training:  73%|███████▎  | 143648/196351 [20:28:27<7:34:35,  1.93it/s]global step 340000, epoch: 2, batch: 143649, loss: 3.74467, acc: 0.92659, time: Sun Jan 23 10:54:25 2022
epoch 2 training:  73%|███████▎  | 143748/196351 [20:29:18<7:26:34,  1.96it/s]global step 340100, epoch: 2, batch: 143749, loss: 1.16614, acc: 0.92659, time: Sun Jan 23 10:55:15 2022
epoch 2 training:  73%|███████▎  | 143848/196351 [20:30:09<7:31:23,  1.94it/s]global step 340200, epoch: 2, batch: 143849, loss: 0.45100, acc: 0.92661, time: Sun Jan 23 10:56:07 2022
epoch 2 training:  73%|███████▎  | 143948/196351 [20:31:00<7:28:14,  1.95it/s]global step 340300, epoch: 2, batch: 143949, loss: 4.47134, acc: 0.92660, time: Sun Jan 23 10:56:58 2022
epoch 2 training:  73%|███████▎  | 144048/196351 [20:31:51<7:35:20,  1.91it/s]global step 340400, epoch: 2, batch: 144049, loss: 1.58511, acc: 0.92661, time: Sun Jan 23 10:57:49 2022
epoch 2 training:  73%|███████▎  | 144148/196351 [20:32:43<7:29:15,  1.94it/s]global step 340500, epoch: 2, batch: 144149, loss: 1.32861, acc: 0.92662, time: Sun Jan 23 10:58:40 2022
epoch 2 training:  73%|███████▎  | 144248/196351 [20:33:35<7:32:06,  1.92it/s]global step 340600, epoch: 2, batch: 144249, loss: 0.30577, acc: 0.92663, time: Sun Jan 23 10:59:32 2022
epoch 2 training:  74%|███████▎  | 144348/196351 [20:34:26<7:24:01,  1.95it/s]global step 340700, epoch: 2, batch: 144349, loss: 1.48585, acc: 0.92664, time: Sun Jan 23 11:00:24 2022
epoch 2 training:  74%|███████▎  | 144448/196351 [20:35:17<7:23:22,  1.95it/s]global step 340800, epoch: 2, batch: 144449, loss: 1.35139, acc: 0.92667, time: Sun Jan 23 11:01:15 2022
epoch 2 training:  74%|███████▎  | 144548/196351 [20:36:08<7:20:29,  1.96it/s]global step 340900, epoch: 2, batch: 144549, loss: 1.64279, acc: 0.92669, time: Sun Jan 23 11:02:06 2022
epoch 2 training:  74%|███████▎  | 144648/196351 [20:37:00<7:16:02,  1.98it/s]global step 341000, epoch: 2, batch: 144649, loss: 0.51701, acc: 0.92669, time: Sun Jan 23 11:02:57 2022
epoch 2 training:  74%|███████▎  | 144748/196351 [20:37:51<7:25:32,  1.93it/s]global step 341100, epoch: 2, batch: 144749, loss: 0.72564, acc: 0.92667, time: Sun Jan 23 11:03:48 2022
epoch 2 training:  74%|███████▍  | 144848/196351 [20:38:42<7:14:15,  1.98it/s]global step 341200, epoch: 2, batch: 144849, loss: 0.55994, acc: 0.92667, time: Sun Jan 23 11:04:40 2022
epoch 2 training:  74%|███████▍  | 144948/196351 [20:39:33<7:15:32,  1.97it/s]global step 341300, epoch: 2, batch: 144949, loss: 1.97900, acc: 0.92668, time: Sun Jan 23 11:05:31 2022
epoch 2 training:  74%|███████▍  | 145048/196351 [20:40:25<7:22:46,  1.93it/s]global step 341400, epoch: 2, batch: 145049, loss: 1.60503, acc: 0.92669, time: Sun Jan 23 11:06:22 2022
epoch 2 training:  74%|███████▍  | 145148/196351 [20:41:16<7:15:51,  1.96it/s]global step 341500, epoch: 2, batch: 145149, loss: 0.47007, acc: 0.92669, time: Sun Jan 23 11:07:14 2022
epoch 2 training:  74%|███████▍  | 145248/196351 [20:42:07<7:26:35,  1.91it/s]global step 341600, epoch: 2, batch: 145249, loss: 0.67539, acc: 0.92671, time: Sun Jan 23 11:08:05 2022
epoch 2 training:  74%|███████▍  | 145348/196351 [20:42:59<7:12:27,  1.97it/s]global step 341700, epoch: 2, batch: 145349, loss: 0.46569, acc: 0.92671, time: Sun Jan 23 11:08:56 2022
epoch 2 training:  74%|███████▍  | 145448/196351 [20:43:50<7:19:40,  1.93it/s]global step 341800, epoch: 2, batch: 145449, loss: 0.38669, acc: 0.92673, time: Sun Jan 23 11:09:47 2022
epoch 2 training:  74%|███████▍  | 145548/196351 [20:44:42<7:10:14,  1.97it/s]global step 341900, epoch: 2, batch: 145549, loss: 1.57126, acc: 0.92672, time: Sun Jan 23 11:10:39 2022
epoch 2 training:  74%|███████▍  | 145648/196351 [20:45:34<7:17:35,  1.93it/s]global step 342000, epoch: 2, batch: 145649, loss: 0.61838, acc: 0.92670, time: Sun Jan 23 11:11:31 2022
epoch 2 training:  74%|███████▍  | 145748/196351 [20:46:25<7:16:55,  1.93it/s]global step 342100, epoch: 2, batch: 145749, loss: 4.18285, acc: 0.92670, time: Sun Jan 23 11:12:23 2022
epoch 2 training:  74%|███████▍  | 145848/196351 [20:47:17<7:08:51,  1.96it/s]global step 342200, epoch: 2, batch: 145849, loss: 1.48448, acc: 0.92670, time: Sun Jan 23 11:13:15 2022
epoch 2 training:  74%|███████▍  | 145948/196351 [20:48:09<7:09:06,  1.96it/s]global step 342300, epoch: 2, batch: 145949, loss: 1.07220, acc: 0.92671, time: Sun Jan 23 11:14:06 2022
epoch 2 training:  74%|███████▍  | 146048/196351 [20:49:00<7:14:07,  1.93it/s]global step 342400, epoch: 2, batch: 146049, loss: 2.00774, acc: 0.92672, time: Sun Jan 23 11:14:58 2022
epoch 2 training:  74%|███████▍  | 146148/196351 [20:49:52<7:05:16,  1.97it/s]global step 342500, epoch: 2, batch: 146149, loss: 1.51887, acc: 0.92671, time: Sun Jan 23 11:15:49 2022
epoch 2 training:  74%|███████▍  | 146248/196351 [20:50:43<7:06:27,  1.96it/s]global step 342600, epoch: 2, batch: 146249, loss: 1.07594, acc: 0.92670, time: Sun Jan 23 11:16:41 2022
epoch 2 training:  75%|███████▍  | 146348/196351 [20:51:35<7:10:54,  1.93it/s]global step 342700, epoch: 2, batch: 146349, loss: 2.19042, acc: 0.92668, time: Sun Jan 23 11:17:32 2022
epoch 2 training:  75%|███████▍  | 146448/196351 [20:52:26<7:05:03,  1.96it/s]global step 342800, epoch: 2, batch: 146449, loss: 1.54109, acc: 0.92668, time: Sun Jan 23 11:18:24 2022
epoch 2 training:  75%|███████▍  | 146548/196351 [20:53:18<7:13:13,  1.92it/s]global step 342900, epoch: 2, batch: 146549, loss: 2.60557, acc: 0.92670, time: Sun Jan 23 11:19:15 2022
epoch 2 training:  75%|███████▍  | 146648/196351 [20:54:09<6:57:35,  1.98it/s]global step 343000, epoch: 2, batch: 146649, loss: 0.44331, acc: 0.92672, time: Sun Jan 23 11:20:06 2022
epoch 2 training:  75%|███████▍  | 146748/196351 [20:55:00<7:04:31,  1.95it/s]global step 343100, epoch: 2, batch: 146749, loss: 0.58537, acc: 0.92673, time: Sun Jan 23 11:20:58 2022
epoch 2 training:  75%|███████▍  | 146848/196351 [20:55:51<6:56:32,  1.98it/s]global step 343200, epoch: 2, batch: 146849, loss: 0.86314, acc: 0.92674, time: Sun Jan 23 11:21:49 2022
epoch 2 training:  75%|███████▍  | 146948/196351 [20:56:43<7:03:07,  1.95it/s]global step 343300, epoch: 2, batch: 146949, loss: 2.25100, acc: 0.92674, time: Sun Jan 23 11:22:40 2022
epoch 2 training:  75%|███████▍  | 147048/196351 [20:57:35<7:01:49,  1.95it/s]global step 343400, epoch: 2, batch: 147049, loss: 2.57379, acc: 0.92675, time: Sun Jan 23 11:23:32 2022
epoch 2 training:  75%|███████▍  | 147148/196351 [20:58:26<6:48:49,  2.01it/s]global step 343500, epoch: 2, batch: 147149, loss: 2.90003, acc: 0.92674, time: Sun Jan 23 11:24:24 2022
epoch 2 training:  75%|███████▍  | 147248/196351 [20:59:18<7:00:31,  1.95it/s]global step 343600, epoch: 2, batch: 147249, loss: 2.41831, acc: 0.92672, time: Sun Jan 23 11:25:15 2022
epoch 2 training:  75%|███████▌  | 147348/196351 [21:00:09<6:54:44,  1.97it/s]global step 343700, epoch: 2, batch: 147349, loss: 1.84151, acc: 0.92671, time: Sun Jan 23 11:26:06 2022
epoch 2 training:  75%|███████▌  | 147448/196351 [21:01:01<7:02:42,  1.93it/s]global step 343800, epoch: 2, batch: 147449, loss: 0.24495, acc: 0.92673, time: Sun Jan 23 11:26:58 2022
epoch 2 training:  75%|███████▌  | 147548/196351 [21:01:52<6:50:45,  1.98it/s]global step 343900, epoch: 2, batch: 147549, loss: 0.93448, acc: 0.92674, time: Sun Jan 23 11:27:50 2022
epoch 2 training:  75%|███████▌  | 147648/196351 [21:02:44<7:03:55,  1.91it/s]global step 344000, epoch: 2, batch: 147649, loss: 1.90825, acc: 0.92674, time: Sun Jan 23 11:28:42 2022
epoch 2 training:  75%|███████▌  | 147748/196351 [21:03:36<6:54:38,  1.95it/s]global step 344100, epoch: 2, batch: 147749, loss: 0.10716, acc: 0.92671, time: Sun Jan 23 11:29:34 2022
epoch 2 training:  75%|███████▌  | 147848/196351 [21:04:27<6:57:15,  1.94it/s]global step 344200, epoch: 2, batch: 147849, loss: 0.44745, acc: 0.92673, time: Sun Jan 23 11:30:25 2022
epoch 2 training:  75%|███████▌  | 147948/196351 [21:05:19<6:46:16,  1.99it/s]global step 344300, epoch: 2, batch: 147949, loss: 2.48299, acc: 0.92673, time: Sun Jan 23 11:31:16 2022
epoch 2 training:  75%|███████▌  | 148048/196351 [21:06:10<6:50:05,  1.96it/s]global step 344400, epoch: 2, batch: 148049, loss: 3.47790, acc: 0.92675, time: Sun Jan 23 11:32:07 2022
epoch 2 training:  75%|███████▌  | 148148/196351 [21:07:01<6:49:48,  1.96it/s]global step 344500, epoch: 2, batch: 148149, loss: 2.95698, acc: 0.92672, time: Sun Jan 23 11:32:58 2022
epoch 2 training:  76%|███████▌  | 148248/196351 [21:07:52<6:52:14,  1.94it/s]global step 344600, epoch: 2, batch: 148249, loss: 0.47848, acc: 0.92673, time: Sun Jan 23 11:33:49 2022
epoch 2 training:  76%|███████▌  | 148348/196351 [21:08:43<6:47:03,  1.97it/s]global step 344700, epoch: 2, batch: 148349, loss: 0.78900, acc: 0.92675, time: Sun Jan 23 11:34:41 2022
epoch 2 training:  76%|███████▌  | 148448/196351 [21:09:34<6:48:58,  1.95it/s]global step 344800, epoch: 2, batch: 148449, loss: 2.61565, acc: 0.92676, time: Sun Jan 23 11:35:32 2022
epoch 2 training:  76%|███████▌  | 148548/196351 [21:10:26<6:46:20,  1.96it/s]global step 344900, epoch: 2, batch: 148549, loss: 1.01610, acc: 0.92679, time: Sun Jan 23 11:36:24 2022
epoch 2 training:  76%|███████▌  | 148648/196351 [21:11:17<6:55:19,  1.91it/s]global step 345000, epoch: 2, batch: 148649, loss: 1.60063, acc: 0.92680, time: Sun Jan 23 11:37:15 2022
epoch 2 training:  76%|███████▌  | 148748/196351 [21:12:08<6:50:07,  1.93it/s]global step 345100, epoch: 2, batch: 148749, loss: 1.20377, acc: 0.92681, time: Sun Jan 23 11:38:06 2022
epoch 2 training:  76%|███████▌  | 148848/196351 [21:13:00<6:43:15,  1.96it/s]global step 345200, epoch: 2, batch: 148849, loss: 1.92129, acc: 0.92681, time: Sun Jan 23 11:38:57 2022
epoch 2 training:  76%|███████▌  | 148948/196351 [21:13:51<6:44:53,  1.95it/s]global step 345300, epoch: 2, batch: 148949, loss: 0.46595, acc: 0.92682, time: Sun Jan 23 11:39:48 2022
epoch 2 training:  76%|███████▌  | 149048/196351 [21:14:42<6:52:08,  1.91it/s]global step 345400, epoch: 2, batch: 149049, loss: 1.50255, acc: 0.92682, time: Sun Jan 23 11:40:40 2022
epoch 2 training:  76%|███████▌  | 149148/196351 [21:15:33<6:40:42,  1.96it/s]global step 345500, epoch: 2, batch: 149149, loss: 0.19937, acc: 0.92682, time: Sun Jan 23 11:41:31 2022
epoch 2 training:  76%|███████▌  | 149248/196351 [21:16:24<6:44:02,  1.94it/s]global step 345600, epoch: 2, batch: 149249, loss: 0.78299, acc: 0.92680, time: Sun Jan 23 11:42:22 2022
epoch 2 training:  76%|███████▌  | 149348/196351 [21:17:16<6:36:38,  1.98it/s]global step 345700, epoch: 2, batch: 149349, loss: 1.79425, acc: 0.92681, time: Sun Jan 23 11:43:13 2022
epoch 2 training:  76%|███████▌  | 149448/196351 [21:18:07<6:36:23,  1.97it/s]global step 345800, epoch: 2, batch: 149449, loss: 0.99307, acc: 0.92680, time: Sun Jan 23 11:44:04 2022
epoch 2 training:  76%|███████▌  | 149548/196351 [21:18:59<6:41:16,  1.94it/s]global step 345900, epoch: 2, batch: 149549, loss: 0.87132, acc: 0.92680, time: Sun Jan 23 11:44:56 2022
epoch 2 training:  76%|███████▌  | 149648/196351 [21:19:50<6:44:23,  1.92it/s]global step 346000, epoch: 2, batch: 149649, loss: 0.92807, acc: 0.92681, time: Sun Jan 23 11:45:48 2022
epoch 2 training:  76%|███████▋  | 149748/196351 [21:20:42<6:38:21,  1.95it/s]global step 346100, epoch: 2, batch: 149749, loss: 1.14379, acc: 0.92681, time: Sun Jan 23 11:46:39 2022
epoch 2 training:  76%|███████▋  | 149848/196351 [21:21:33<6:34:31,  1.96it/s]global step 346200, epoch: 2, batch: 149849, loss: 1.76866, acc: 0.92682, time: Sun Jan 23 11:47:31 2022
epoch 2 training:  76%|███████▋  | 149948/196351 [21:22:25<6:36:15,  1.95it/s]global step 346300, epoch: 2, batch: 149949, loss: 0.31448, acc: 0.92681, time: Sun Jan 23 11:48:22 2022
epoch 2 training:  76%|███████▋  | 150048/196351 [21:23:16<6:31:18,  1.97it/s]global step 346400, epoch: 2, batch: 150049, loss: 1.54592, acc: 0.92679, time: Sun Jan 23 11:49:13 2022
epoch 2 training:  76%|███████▋  | 150148/196351 [21:24:07<6:26:51,  1.99it/s]global step 346500, epoch: 2, batch: 150149, loss: 2.82649, acc: 0.92678, time: Sun Jan 23 11:50:04 2022
epoch 2 training:  77%|███████▋  | 150248/196351 [21:24:58<6:36:45,  1.94it/s]global step 346600, epoch: 2, batch: 150249, loss: 2.14877, acc: 0.92680, time: Sun Jan 23 11:50:55 2022
epoch 2 training:  77%|███████▋  | 150348/196351 [21:25:49<6:33:44,  1.95it/s]global step 346700, epoch: 2, batch: 150349, loss: 3.22825, acc: 0.92677, time: Sun Jan 23 11:51:46 2022
epoch 2 training:  77%|███████▋  | 150448/196351 [21:26:40<6:30:33,  1.96it/s]global step 346800, epoch: 2, batch: 150449, loss: 1.22987, acc: 0.92677, time: Sun Jan 23 11:52:37 2022
epoch 2 training:  77%|███████▋  | 150548/196351 [21:27:31<6:23:32,  1.99it/s]global step 346900, epoch: 2, batch: 150549, loss: 2.31099, acc: 0.92677, time: Sun Jan 23 11:53:28 2022
epoch 2 training:  77%|███████▋  | 150648/196351 [21:28:22<6:25:20,  1.98it/s]global step 347000, epoch: 2, batch: 150649, loss: 0.16570, acc: 0.92678, time: Sun Jan 23 11:54:19 2022
epoch 2 training:  77%|███████▋  | 150748/196351 [21:29:12<6:29:56,  1.95it/s]global step 347100, epoch: 2, batch: 150749, loss: 2.15434, acc: 0.92678, time: Sun Jan 23 11:55:10 2022
epoch 2 training:  77%|███████▋  | 150848/196351 [21:30:03<6:21:48,  1.99it/s]global step 347200, epoch: 2, batch: 150849, loss: 0.25440, acc: 0.92678, time: Sun Jan 23 11:56:01 2022
epoch 2 training:  77%|███████▋  | 150948/196351 [21:30:54<6:21:16,  1.98it/s]global step 347300, epoch: 2, batch: 150949, loss: 3.20380, acc: 0.92678, time: Sun Jan 23 11:56:51 2022
epoch 2 training:  77%|███████▋  | 151048/196351 [21:31:45<6:22:45,  1.97it/s]global step 347400, epoch: 2, batch: 151049, loss: 2.09770, acc: 0.92678, time: Sun Jan 23 11:57:43 2022
epoch 2 training:  77%|███████▋  | 151148/196351 [21:32:36<6:24:08,  1.96it/s]global step 347500, epoch: 2, batch: 151149, loss: 2.82341, acc: 0.92678, time: Sun Jan 23 11:58:34 2022
epoch 2 training:  77%|███████▋  | 151248/196351 [21:33:28<6:29:13,  1.93it/s]global step 347600, epoch: 2, batch: 151249, loss: 2.19528, acc: 0.92680, time: Sun Jan 23 11:59:25 2022
epoch 2 training:  77%|███████▋  | 151348/196351 [21:34:19<6:22:32,  1.96it/s]global step 347700, epoch: 2, batch: 151349, loss: 4.00185, acc: 0.92680, time: Sun Jan 23 12:00:17 2022
epoch 2 training:  77%|███████▋  | 151448/196351 [21:35:10<6:23:22,  1.95it/s]global step 347800, epoch: 2, batch: 151449, loss: 0.73143, acc: 0.92680, time: Sun Jan 23 12:01:07 2022
epoch 2 training:  77%|███████▋  | 151548/196351 [21:36:02<6:23:07,  1.95it/s]global step 347900, epoch: 2, batch: 151549, loss: 0.29190, acc: 0.92681, time: Sun Jan 23 12:01:59 2022
epoch 2 training:  77%|███████▋  | 151648/196351 [21:36:53<6:15:55,  1.98it/s]global step 348000, epoch: 2, batch: 151649, loss: 0.43290, acc: 0.92682, time: Sun Jan 23 12:02:50 2022
epoch 2 training:  77%|███████▋  | 151748/196351 [21:37:43<6:19:53,  1.96it/s]global step 348100, epoch: 2, batch: 151749, loss: 2.08552, acc: 0.92682, time: Sun Jan 23 12:03:41 2022
epoch 2 training:  77%|███████▋  | 151848/196351 [21:38:34<6:13:32,  1.99it/s]global step 348200, epoch: 2, batch: 151849, loss: 0.94616, acc: 0.92684, time: Sun Jan 23 12:04:31 2022
epoch 2 training:  77%|███████▋  | 151948/196351 [21:39:24<6:17:53,  1.96it/s]global step 348300, epoch: 2, batch: 151949, loss: 0.53020, acc: 0.92685, time: Sun Jan 23 12:05:22 2022
epoch 2 training:  77%|███████▋  | 152048/196351 [21:40:15<6:08:31,  2.00it/s]global step 348400, epoch: 2, batch: 152049, loss: 3.48712, acc: 0.92686, time: Sun Jan 23 12:06:12 2022
epoch 2 training:  77%|███████▋  | 152148/196351 [21:41:05<6:11:31,  1.98it/s]global step 348500, epoch: 2, batch: 152149, loss: 1.35601, acc: 0.92686, time: Sun Jan 23 12:07:03 2022
epoch 2 training:  78%|███████▊  | 152248/196351 [21:41:57<6:16:21,  1.95it/s]global step 348600, epoch: 2, batch: 152249, loss: 2.79236, acc: 0.92687, time: Sun Jan 23 12:07:54 2022
epoch 2 training:  78%|███████▊  | 152348/196351 [21:42:47<6:07:04,  2.00it/s]global step 348700, epoch: 2, batch: 152349, loss: 0.69654, acc: 0.92687, time: Sun Jan 23 12:08:45 2022
epoch 2 training:  78%|███████▊  | 152448/196351 [21:43:38<6:06:49,  1.99it/s]global step 348800, epoch: 2, batch: 152449, loss: 0.76785, acc: 0.92687, time: Sun Jan 23 12:09:36 2022
epoch 2 training:  78%|███████▊  | 152548/196351 [21:44:29<6:09:22,  1.98it/s]global step 348900, epoch: 2, batch: 152549, loss: 1.93737, acc: 0.92689, time: Sun Jan 23 12:10:26 2022
epoch 2 training:  78%|███████▊  | 152648/196351 [21:45:20<6:07:59,  1.98it/s]global step 349000, epoch: 2, batch: 152649, loss: 2.51180, acc: 0.92690, time: Sun Jan 23 12:11:17 2022
epoch 2 training:  78%|███████▊  | 152748/196351 [21:46:10<6:02:13,  2.01it/s]global step 349100, epoch: 2, batch: 152749, loss: 2.55060, acc: 0.92690, time: Sun Jan 23 12:12:08 2022
epoch 2 training:  78%|███████▊  | 152848/196351 [21:47:01<6:02:03,  2.00it/s]global step 349200, epoch: 2, batch: 152849, loss: 2.11576, acc: 0.92691, time: Sun Jan 23 12:12:58 2022
epoch 2 training:  78%|███████▊  | 152948/196351 [21:47:51<6:09:47,  1.96it/s]global step 349300, epoch: 2, batch: 152949, loss: 0.88885, acc: 0.92688, time: Sun Jan 23 12:13:49 2022
epoch 2 training:  78%|███████▊  | 153048/196351 [21:48:42<6:03:07,  1.99it/s]global step 349400, epoch: 2, batch: 153049, loss: 0.12996, acc: 0.92689, time: Sun Jan 23 12:14:39 2022
epoch 2 training:  78%|███████▊  | 153148/196351 [21:49:33<6:05:00,  1.97it/s]global step 349500, epoch: 2, batch: 153149, loss: 0.73212, acc: 0.92690, time: Sun Jan 23 12:15:30 2022
epoch 2 training:  78%|███████▊  | 153248/196351 [21:50:23<6:02:46,  1.98it/s]global step 349600, epoch: 2, batch: 153249, loss: 1.62529, acc: 0.92690, time: Sun Jan 23 12:16:21 2022
epoch 2 training:  78%|███████▊  | 153348/196351 [21:51:14<6:04:58,  1.96it/s]global step 349700, epoch: 2, batch: 153349, loss: 2.95774, acc: 0.92690, time: Sun Jan 23 12:17:11 2022
epoch 2 training:  78%|███████▊  | 153448/196351 [21:52:05<5:58:39,  1.99it/s]global step 349800, epoch: 2, batch: 153449, loss: 0.46289, acc: 0.92691, time: Sun Jan 23 12:18:03 2022
epoch 2 training:  78%|███████▊  | 153548/196351 [21:52:56<6:06:04,  1.95it/s]global step 349900, epoch: 2, batch: 153549, loss: 0.61444, acc: 0.92689, time: Sun Jan 23 12:18:54 2022
epoch 2 training:  78%|███████▊  | 153648/196351 [21:53:47<6:03:07,  1.96it/s]global step 350000, epoch: 2, batch: 153649, loss: 0.65764, acc: 0.92687, time: Sun Jan 23 12:19:45 2022
epoch 2 training:  78%|███████▊  | 153748/196351 [21:54:38<6:00:06,  1.97it/s]global step 350100, epoch: 2, batch: 153749, loss: 1.00109, acc: 0.92686, time: Sun Jan 23 12:20:35 2022
epoch 2 training:  78%|███████▊  | 153848/196351 [21:55:29<6:01:06,  1.96it/s]global step 350200, epoch: 2, batch: 153849, loss: 0.72735, acc: 0.92688, time: Sun Jan 23 12:21:26 2022
epoch 2 training:  78%|███████▊  | 153948/196351 [21:56:20<5:57:58,  1.97it/s]global step 350300, epoch: 2, batch: 153949, loss: 2.11427, acc: 0.92689, time: Sun Jan 23 12:22:17 2022
epoch 2 training:  78%|███████▊  | 154048/196351 [21:57:10<6:03:29,  1.94it/s]global step 350400, epoch: 2, batch: 154049, loss: 2.28076, acc: 0.92688, time: Sun Jan 23 12:23:08 2022
epoch 2 training:  79%|███████▊  | 154148/196351 [21:58:01<5:58:42,  1.96it/s]global step 350500, epoch: 2, batch: 154149, loss: 0.83638, acc: 0.92687, time: Sun Jan 23 12:23:59 2022
epoch 2 training:  79%|███████▊  | 154248/196351 [21:58:52<5:56:59,  1.97it/s]global step 350600, epoch: 2, batch: 154249, loss: 1.21549, acc: 0.92687, time: Sun Jan 23 12:24:49 2022
epoch 2 training:  79%|███████▊  | 154348/196351 [21:59:43<5:51:17,  1.99it/s]global step 350700, epoch: 2, batch: 154349, loss: 2.07156, acc: 0.92688, time: Sun Jan 23 12:25:40 2022
epoch 2 training:  79%|███████▊  | 154448/196351 [22:00:33<5:50:48,  1.99it/s]global step 350800, epoch: 2, batch: 154449, loss: 1.37930, acc: 0.92688, time: Sun Jan 23 12:26:31 2022
epoch 2 training:  79%|███████▊  | 154548/196351 [22:01:24<5:50:57,  1.99it/s]global step 350900, epoch: 2, batch: 154549, loss: 2.57623, acc: 0.92687, time: Sun Jan 23 12:27:21 2022
epoch 2 training:  79%|███████▉  | 154648/196351 [22:02:15<5:50:49,  1.98it/s]global step 351000, epoch: 2, batch: 154649, loss: 0.96319, acc: 0.92689, time: Sun Jan 23 12:28:12 2022
epoch 2 training:  79%|███████▉  | 154748/196351 [22:03:05<5:48:05,  1.99it/s]global step 351100, epoch: 2, batch: 154749, loss: 0.89936, acc: 0.92689, time: Sun Jan 23 12:29:02 2022
epoch 2 training:  79%|███████▉  | 154848/196351 [22:03:56<5:47:44,  1.99it/s]global step 351200, epoch: 2, batch: 154849, loss: 2.46179, acc: 0.92689, time: Sun Jan 23 12:29:53 2022
epoch 2 training:  79%|███████▉  | 154948/196351 [22:04:46<5:47:06,  1.99it/s]global step 351300, epoch: 2, batch: 154949, loss: 2.31686, acc: 0.92690, time: Sun Jan 23 12:30:44 2022
epoch 2 training:  79%|███████▉  | 155048/196351 [22:05:37<5:49:37,  1.97it/s]global step 351400, epoch: 2, batch: 155049, loss: 2.21223, acc: 0.92691, time: Sun Jan 23 12:31:35 2022
epoch 2 training:  79%|███████▉  | 155148/196351 [22:06:28<5:47:29,  1.98it/s]global step 351500, epoch: 2, batch: 155149, loss: 2.38394, acc: 0.92687, time: Sun Jan 23 12:32:25 2022
epoch 2 training:  79%|███████▉  | 155248/196351 [22:07:19<5:53:04,  1.94it/s]global step 351600, epoch: 2, batch: 155249, loss: 1.10701, acc: 0.92688, time: Sun Jan 23 12:33:17 2022
epoch 2 training:  79%|███████▉  | 155348/196351 [22:08:10<5:47:36,  1.97it/s]global step 351700, epoch: 2, batch: 155349, loss: 1.02202, acc: 0.92688, time: Sun Jan 23 12:34:08 2022
epoch 2 training:  79%|███████▉  | 155448/196351 [22:09:01<5:48:15,  1.96it/s]global step 351800, epoch: 2, batch: 155449, loss: 1.88296, acc: 0.92687, time: Sun Jan 23 12:34:58 2022
epoch 2 training:  79%|███████▉  | 155548/196351 [22:09:52<5:44:56,  1.97it/s]global step 351900, epoch: 2, batch: 155549, loss: 2.10544, acc: 0.92688, time: Sun Jan 23 12:35:49 2022
epoch 2 training:  79%|███████▉  | 155648/196351 [22:10:43<5:43:01,  1.98it/s]global step 352000, epoch: 2, batch: 155649, loss: 3.51207, acc: 0.92688, time: Sun Jan 23 12:36:40 2022
epoch 2 training:  79%|███████▉  | 155748/196351 [22:11:34<5:46:43,  1.95it/s]global step 352100, epoch: 2, batch: 155749, loss: 0.23023, acc: 0.92688, time: Sun Jan 23 12:37:32 2022
epoch 2 training:  79%|███████▉  | 155848/196351 [22:12:25<5:44:30,  1.96it/s]global step 352200, epoch: 2, batch: 155849, loss: 2.00363, acc: 0.92689, time: Sun Jan 23 12:38:23 2022
epoch 2 training:  79%|███████▉  | 155948/196351 [22:13:16<5:37:21,  2.00it/s]global step 352300, epoch: 2, batch: 155949, loss: 1.13922, acc: 0.92690, time: Sun Jan 23 12:39:13 2022
epoch 2 training:  79%|███████▉  | 156048/196351 [22:14:07<5:41:03,  1.97it/s]global step 352400, epoch: 2, batch: 156049, loss: 1.01253, acc: 0.92690, time: Sun Jan 23 12:40:04 2022
epoch 2 training:  80%|███████▉  | 156148/196351 [22:14:57<5:40:13,  1.97it/s]global step 352500, epoch: 2, batch: 156149, loss: 0.91695, acc: 0.92688, time: Sun Jan 23 12:40:55 2022
epoch 2 training:  80%|███████▉  | 156248/196351 [22:15:49<5:47:09,  1.93it/s]global step 352600, epoch: 2, batch: 156249, loss: 4.01173, acc: 0.92687, time: Sun Jan 23 12:41:46 2022
epoch 2 training:  80%|███████▉  | 156348/196351 [22:16:40<5:40:18,  1.96it/s]global step 352700, epoch: 2, batch: 156349, loss: 1.14204, acc: 0.92688, time: Sun Jan 23 12:42:37 2022
epoch 2 training:  80%|███████▉  | 156448/196351 [22:17:31<5:35:11,  1.98it/s]global step 352800, epoch: 2, batch: 156449, loss: 1.06432, acc: 0.92686, time: Sun Jan 23 12:43:28 2022
epoch 2 training:  80%|███████▉  | 156548/196351 [22:18:22<5:35:58,  1.97it/s]global step 352900, epoch: 2, batch: 156549, loss: 1.06655, acc: 0.92687, time: Sun Jan 23 12:44:19 2022
epoch 2 training:  80%|███████▉  | 156648/196351 [22:19:13<5:37:48,  1.96it/s]global step 353000, epoch: 2, batch: 156649, loss: 0.54540, acc: 0.92688, time: Sun Jan 23 12:45:10 2022
epoch 2 training:  80%|███████▉  | 156748/196351 [22:20:04<5:31:21,  1.99it/s]global step 353100, epoch: 2, batch: 156749, loss: 0.67129, acc: 0.92687, time: Sun Jan 23 12:46:01 2022
epoch 2 training:  80%|███████▉  | 156848/196351 [22:20:54<5:30:24,  1.99it/s]global step 353200, epoch: 2, batch: 156849, loss: 2.19508, acc: 0.92688, time: Sun Jan 23 12:46:52 2022
epoch 2 training:  80%|███████▉  | 156948/196351 [22:21:45<5:30:38,  1.99it/s]global step 353300, epoch: 2, batch: 156949, loss: 2.41217, acc: 0.92689, time: Sun Jan 23 12:47:42 2022
epoch 2 training:  80%|███████▉  | 157048/196351 [22:22:36<5:31:57,  1.97it/s]global step 353400, epoch: 2, batch: 157049, loss: 0.51891, acc: 0.92689, time: Sun Jan 23 12:48:33 2022
epoch 2 training:  80%|████████  | 157148/196351 [22:23:27<5:29:42,  1.98it/s]global step 353500, epoch: 2, batch: 157149, loss: 2.50462, acc: 0.92689, time: Sun Jan 23 12:49:24 2022
epoch 2 training:  80%|████████  | 157248/196351 [22:24:18<5:34:19,  1.95it/s]global step 353600, epoch: 2, batch: 157249, loss: 1.67441, acc: 0.92688, time: Sun Jan 23 12:50:15 2022
epoch 2 training:  80%|████████  | 157348/196351 [22:25:09<5:31:09,  1.96it/s]global step 353700, epoch: 2, batch: 157349, loss: 1.01913, acc: 0.92689, time: Sun Jan 23 12:51:06 2022
epoch 2 training:  80%|████████  | 157448/196351 [22:26:00<5:27:16,  1.98it/s]global step 353800, epoch: 2, batch: 157449, loss: 1.01172, acc: 0.92688, time: Sun Jan 23 12:51:57 2022
epoch 2 training:  80%|████████  | 157548/196351 [22:26:51<5:32:01,  1.95it/s]global step 353900, epoch: 2, batch: 157549, loss: 1.08211, acc: 0.92688, time: Sun Jan 23 12:52:49 2022
epoch 2 training:  80%|████████  | 157648/196351 [22:27:42<5:35:00,  1.93it/s]global step 354000, epoch: 2, batch: 157649, loss: 0.98854, acc: 0.92690, time: Sun Jan 23 12:53:39 2022
epoch 2 training:  80%|████████  | 157748/196351 [22:28:33<5:23:08,  1.99it/s]global step 354100, epoch: 2, batch: 157749, loss: 0.62955, acc: 0.92690, time: Sun Jan 23 12:54:30 2022
epoch 2 training:  80%|████████  | 157848/196351 [22:29:24<5:22:10,  1.99it/s]global step 354200, epoch: 2, batch: 157849, loss: 1.82624, acc: 0.92690, time: Sun Jan 23 12:55:21 2022
epoch 2 training:  80%|████████  | 157948/196351 [22:30:15<5:22:42,  1.98it/s]global step 354300, epoch: 2, batch: 157949, loss: 0.81344, acc: 0.92692, time: Sun Jan 23 12:56:12 2022
epoch 2 training:  80%|████████  | 158048/196351 [22:31:06<5:30:25,  1.93it/s]global step 354400, epoch: 2, batch: 158049, loss: 2.48559, acc: 0.92691, time: Sun Jan 23 12:57:03 2022
epoch 2 training:  81%|████████  | 158148/196351 [22:31:57<5:25:35,  1.96it/s]global step 354500, epoch: 2, batch: 158149, loss: 0.24381, acc: 0.92690, time: Sun Jan 23 12:57:54 2022
epoch 2 training:  81%|████████  | 158248/196351 [22:32:48<5:23:36,  1.96it/s]global step 354600, epoch: 2, batch: 158249, loss: 1.41854, acc: 0.92691, time: Sun Jan 23 12:58:45 2022
epoch 2 training:  81%|████████  | 158348/196351 [22:33:38<5:17:16,  2.00it/s]global step 354700, epoch: 2, batch: 158349, loss: 2.42498, acc: 0.92690, time: Sun Jan 23 12:59:36 2022
epoch 2 training:  81%|████████  | 158448/196351 [22:34:29<5:20:23,  1.97it/s]global step 354800, epoch: 2, batch: 158449, loss: 2.26454, acc: 0.92690, time: Sun Jan 23 13:00:26 2022
epoch 2 training:  81%|████████  | 158548/196351 [22:35:20<5:23:38,  1.95it/s]global step 354900, epoch: 2, batch: 158549, loss: 2.53840, acc: 0.92691, time: Sun Jan 23 13:01:17 2022
epoch 2 training:  81%|████████  | 158648/196351 [22:36:10<5:21:40,  1.95it/s]global step 355000, epoch: 2, batch: 158649, loss: 0.31162, acc: 0.92691, time: Sun Jan 23 13:02:08 2022
epoch 2 training:  81%|████████  | 158748/196351 [22:37:01<5:19:13,  1.96it/s]global step 355100, epoch: 2, batch: 158749, loss: 1.17496, acc: 0.92691, time: Sun Jan 23 13:02:58 2022
epoch 2 training:  81%|████████  | 158848/196351 [22:37:51<5:13:36,  1.99it/s]global step 355200, epoch: 2, batch: 158849, loss: 0.88600, acc: 0.92688, time: Sun Jan 23 13:03:49 2022
epoch 2 training:  81%|████████  | 158948/196351 [22:38:42<5:15:03,  1.98it/s]global step 355300, epoch: 2, batch: 158949, loss: 2.70222, acc: 0.92688, time: Sun Jan 23 13:04:39 2022
epoch 2 training:  81%|████████  | 159048/196351 [22:39:33<5:12:43,  1.99it/s]global step 355400, epoch: 2, batch: 159049, loss: 1.08979, acc: 0.92688, time: Sun Jan 23 13:05:30 2022
epoch 2 training:  81%|████████  | 159148/196351 [22:40:24<5:15:02,  1.97it/s]global step 355500, epoch: 2, batch: 159149, loss: 3.51970, acc: 0.92687, time: Sun Jan 23 13:06:21 2022
epoch 2 training:  81%|████████  | 159248/196351 [22:41:14<5:11:34,  1.98it/s]global step 355600, epoch: 2, batch: 159249, loss: 2.96951, acc: 0.92688, time: Sun Jan 23 13:07:12 2022
epoch 2 training:  81%|████████  | 159348/196351 [22:42:05<5:10:10,  1.99it/s]global step 355700, epoch: 2, batch: 159349, loss: 0.91806, acc: 0.92688, time: Sun Jan 23 13:08:02 2022
epoch 2 training:  81%|████████  | 159448/196351 [22:42:55<5:11:28,  1.97it/s]global step 355800, epoch: 2, batch: 159449, loss: 1.72253, acc: 0.92688, time: Sun Jan 23 13:08:53 2022
epoch 2 training:  81%|████████▏ | 159548/196351 [22:43:46<5:12:40,  1.96it/s]global step 355900, epoch: 2, batch: 159549, loss: 2.08732, acc: 0.92687, time: Sun Jan 23 13:09:43 2022
epoch 2 training:  81%|████████▏ | 159648/196351 [22:44:37<5:06:48,  1.99it/s]global step 356000, epoch: 2, batch: 159649, loss: 1.66580, acc: 0.92689, time: Sun Jan 23 13:10:34 2022
epoch 2 training:  81%|████████▏ | 159748/196351 [22:45:28<5:07:46,  1.98it/s]global step 356100, epoch: 2, batch: 159749, loss: 0.52259, acc: 0.92690, time: Sun Jan 23 13:11:25 2022
epoch 2 training:  81%|████████▏ | 159848/196351 [22:46:19<5:08:36,  1.97it/s]global step 356200, epoch: 2, batch: 159849, loss: 1.93198, acc: 0.92691, time: Sun Jan 23 13:12:16 2022
epoch 2 training:  81%|████████▏ | 159948/196351 [22:47:10<5:10:37,  1.95it/s]global step 356300, epoch: 2, batch: 159949, loss: 1.53531, acc: 0.92694, time: Sun Jan 23 13:13:07 2022
epoch 2 training:  82%|████████▏ | 160048/196351 [22:48:01<5:07:16,  1.97it/s]global step 356400, epoch: 2, batch: 160049, loss: 0.91774, acc: 0.92694, time: Sun Jan 23 13:13:59 2022
epoch 2 training:  82%|████████▏ | 160148/196351 [22:48:53<5:08:14,  1.96it/s]global step 356500, epoch: 2, batch: 160149, loss: 1.53974, acc: 0.92693, time: Sun Jan 23 13:14:50 2022
epoch 2 training:  82%|████████▏ | 160248/196351 [22:49:44<5:05:32,  1.97it/s]global step 356600, epoch: 2, batch: 160249, loss: 2.09236, acc: 0.92693, time: Sun Jan 23 13:15:41 2022
epoch 2 training:  82%|████████▏ | 160348/196351 [22:50:35<5:02:43,  1.98it/s]global step 356700, epoch: 2, batch: 160349, loss: 2.66961, acc: 0.92694, time: Sun Jan 23 13:16:32 2022
epoch 2 training:  82%|████████▏ | 160448/196351 [22:51:26<5:02:26,  1.98it/s]global step 356800, epoch: 2, batch: 160449, loss: 0.85826, acc: 0.92695, time: Sun Jan 23 13:17:23 2022
epoch 2 training:  82%|████████▏ | 160548/196351 [22:52:17<5:01:44,  1.98it/s]global step 356900, epoch: 2, batch: 160549, loss: 0.31510, acc: 0.92695, time: Sun Jan 23 13:18:14 2022
epoch 2 training:  82%|████████▏ | 160648/196351 [22:53:07<5:03:11,  1.96it/s]global step 357000, epoch: 2, batch: 160649, loss: 3.25802, acc: 0.92696, time: Sun Jan 23 13:19:05 2022
epoch 2 training:  82%|████████▏ | 160748/196351 [22:53:58<5:00:55,  1.97it/s]global step 357100, epoch: 2, batch: 160749, loss: 0.84029, acc: 0.92695, time: Sun Jan 23 13:19:55 2022
epoch 2 training:  82%|████████▏ | 160848/196351 [22:54:48<5:00:17,  1.97it/s]global step 357200, epoch: 2, batch: 160849, loss: 1.11528, acc: 0.92697, time: Sun Jan 23 13:20:46 2022
epoch 2 training:  82%|████████▏ | 160948/196351 [22:55:39<4:56:40,  1.99it/s]global step 357300, epoch: 2, batch: 160949, loss: 0.75920, acc: 0.92696, time: Sun Jan 23 13:21:36 2022
epoch 2 training:  82%|████████▏ | 161048/196351 [22:56:30<4:55:27,  1.99it/s]global step 357400, epoch: 2, batch: 161049, loss: 2.09645, acc: 0.92694, time: Sun Jan 23 13:22:27 2022
epoch 2 training:  82%|████████▏ | 161148/196351 [22:57:21<4:58:16,  1.97it/s]global step 357500, epoch: 2, batch: 161149, loss: 2.96630, acc: 0.92693, time: Sun Jan 23 13:23:18 2022
epoch 2 training:  82%|████████▏ | 161248/196351 [22:58:12<4:52:40,  2.00it/s]global step 357600, epoch: 2, batch: 161249, loss: 1.80434, acc: 0.92692, time: Sun Jan 23 13:24:09 2022
epoch 2 training:  82%|████████▏ | 161348/196351 [22:59:02<4:55:29,  1.97it/s]global step 357700, epoch: 2, batch: 161349, loss: 0.45201, acc: 0.92693, time: Sun Jan 23 13:25:00 2022
epoch 2 training:  82%|████████▏ | 161448/196351 [22:59:53<5:05:27,  1.90it/s]global step 357800, epoch: 2, batch: 161449, loss: 0.12436, acc: 0.92696, time: Sun Jan 23 13:25:51 2022
epoch 2 training:  82%|████████▏ | 161548/196351 [23:00:45<5:00:39,  1.93it/s]global step 357900, epoch: 2, batch: 161549, loss: 2.24215, acc: 0.92696, time: Sun Jan 23 13:26:42 2022
epoch 2 training:  82%|████████▏ | 161648/196351 [23:01:36<4:53:45,  1.97it/s]global step 358000, epoch: 2, batch: 161649, loss: 1.57367, acc: 0.92695, time: Sun Jan 23 13:27:33 2022
epoch 2 training:  82%|████████▏ | 161748/196351 [23:02:27<4:52:34,  1.97it/s]global step 358100, epoch: 2, batch: 161749, loss: 0.68892, acc: 0.92693, time: Sun Jan 23 13:28:24 2022
epoch 2 training:  82%|████████▏ | 161848/196351 [23:03:17<4:51:30,  1.97it/s]global step 358200, epoch: 2, batch: 161849, loss: 0.18561, acc: 0.92694, time: Sun Jan 23 13:29:15 2022
epoch 2 training:  82%|████████▏ | 161948/196351 [23:04:08<4:52:36,  1.96it/s]global step 358300, epoch: 2, batch: 161949, loss: 2.65364, acc: 0.92694, time: Sun Jan 23 13:30:05 2022
epoch 2 training:  83%|████████▎ | 162048/196351 [23:04:59<4:46:04,  2.00it/s]global step 358400, epoch: 2, batch: 162049, loss: 4.93532, acc: 0.92695, time: Sun Jan 23 13:30:56 2022
epoch 2 training:  83%|████████▎ | 162148/196351 [23:05:49<4:47:07,  1.99it/s]global step 358500, epoch: 2, batch: 162149, loss: 1.09165, acc: 0.92695, time: Sun Jan 23 13:31:47 2022
epoch 2 training:  83%|████████▎ | 162248/196351 [23:06:40<4:52:59,  1.94it/s]global step 358600, epoch: 2, batch: 162249, loss: 3.74508, acc: 0.92697, time: Sun Jan 23 13:32:37 2022
epoch 2 training:  83%|████████▎ | 162348/196351 [23:07:31<4:47:42,  1.97it/s]global step 358700, epoch: 2, batch: 162349, loss: 0.22645, acc: 0.92696, time: Sun Jan 23 13:33:29 2022
epoch 2 training:  83%|████████▎ | 162448/196351 [23:08:22<4:45:38,  1.98it/s]global step 358800, epoch: 2, batch: 162449, loss: 1.40982, acc: 0.92697, time: Sun Jan 23 13:34:19 2022
epoch 2 training:  83%|████████▎ | 162548/196351 [23:09:13<4:48:31,  1.95it/s]global step 358900, epoch: 2, batch: 162549, loss: 0.69940, acc: 0.92698, time: Sun Jan 23 13:35:10 2022
epoch 2 training:  83%|████████▎ | 162648/196351 [23:10:04<4:46:36,  1.96it/s]global step 359000, epoch: 2, batch: 162649, loss: 1.80969, acc: 0.92698, time: Sun Jan 23 13:36:01 2022
epoch 2 training:  83%|████████▎ | 162748/196351 [23:10:55<4:47:37,  1.95it/s]global step 359100, epoch: 2, batch: 162749, loss: 3.29393, acc: 0.92699, time: Sun Jan 23 13:36:52 2022
epoch 2 training:  83%|████████▎ | 162848/196351 [23:11:46<4:46:07,  1.95it/s]global step 359200, epoch: 2, batch: 162849, loss: 3.00928, acc: 0.92698, time: Sun Jan 23 13:37:43 2022
epoch 2 training:  83%|████████▎ | 162948/196351 [23:12:37<4:44:53,  1.95it/s]global step 359300, epoch: 2, batch: 162949, loss: 2.73017, acc: 0.92700, time: Sun Jan 23 13:38:35 2022
epoch 2 training:  83%|████████▎ | 163048/196351 [23:13:28<4:42:46,  1.96it/s]global step 359400, epoch: 2, batch: 163049, loss: 0.63362, acc: 0.92702, time: Sun Jan 23 13:39:26 2022
epoch 2 training:  83%|████████▎ | 163148/196351 [23:14:19<4:42:48,  1.96it/s]global step 359500, epoch: 2, batch: 163149, loss: 2.74093, acc: 0.92701, time: Sun Jan 23 13:40:16 2022
epoch 2 training:  83%|████████▎ | 163248/196351 [23:15:10<4:39:42,  1.97it/s]global step 359600, epoch: 2, batch: 163249, loss: 2.61400, acc: 0.92702, time: Sun Jan 23 13:41:07 2022
epoch 2 training:  83%|████████▎ | 163348/196351 [23:16:00<4:38:42,  1.97it/s]global step 359700, epoch: 2, batch: 163349, loss: 0.39005, acc: 0.92700, time: Sun Jan 23 13:41:58 2022
epoch 2 training:  83%|████████▎ | 163448/196351 [23:16:51<4:38:14,  1.97it/s]global step 359800, epoch: 2, batch: 163449, loss: 0.26465, acc: 0.92700, time: Sun Jan 23 13:42:49 2022
epoch 2 training:  83%|████████▎ | 163548/196351 [23:17:42<4:41:22,  1.94it/s]global step 359900, epoch: 2, batch: 163549, loss: 0.57307, acc: 0.92702, time: Sun Jan 23 13:43:39 2022
epoch 2 training:  83%|████████▎ | 163648/196351 [23:18:33<4:35:56,  1.98it/s]global step 360000, epoch: 2, batch: 163649, loss: 0.77634, acc: 0.92701, time: Sun Jan 23 13:44:30 2022
epoch 2 training:  83%|████████▎ | 163748/196351 [23:19:23<4:36:50,  1.96it/s]global step 360100, epoch: 2, batch: 163749, loss: 3.88349, acc: 0.92701, time: Sun Jan 23 13:45:21 2022
epoch 2 training:  83%|████████▎ | 163848/196351 [23:20:15<4:38:25,  1.95it/s]global step 360200, epoch: 2, batch: 163849, loss: 3.89877, acc: 0.92702, time: Sun Jan 23 13:46:13 2022
epoch 2 training:  83%|████████▎ | 163948/196351 [23:21:07<4:35:11,  1.96it/s]global step 360300, epoch: 2, batch: 163949, loss: 0.41307, acc: 0.92701, time: Sun Jan 23 13:47:04 2022
epoch 2 training:  84%|████████▎ | 164048/196351 [23:21:59<4:36:07,  1.95it/s]global step 360400, epoch: 2, batch: 164049, loss: 1.49524, acc: 0.92703, time: Sun Jan 23 13:47:56 2022
epoch 2 training:  84%|████████▎ | 164148/196351 [23:22:50<4:31:24,  1.98it/s]global step 360500, epoch: 2, batch: 164149, loss: 1.46135, acc: 0.92702, time: Sun Jan 23 13:48:47 2022
epoch 2 training:  84%|████████▎ | 164248/196351 [23:23:41<4:33:19,  1.96it/s]global step 360600, epoch: 2, batch: 164249, loss: 0.51049, acc: 0.92701, time: Sun Jan 23 13:49:38 2022
epoch 2 training:  84%|████████▎ | 164348/196351 [23:24:33<4:35:19,  1.94it/s]global step 360700, epoch: 2, batch: 164349, loss: 0.33238, acc: 0.92702, time: Sun Jan 23 13:50:30 2022
epoch 2 training:  84%|████████▍ | 164448/196351 [23:25:24<4:35:21,  1.93it/s]global step 360800, epoch: 2, batch: 164449, loss: 2.64468, acc: 0.92703, time: Sun Jan 23 13:51:21 2022
epoch 2 training:  84%|████████▍ | 164548/196351 [23:26:15<4:29:33,  1.97it/s]global step 360900, epoch: 2, batch: 164549, loss: 2.59484, acc: 0.92703, time: Sun Jan 23 13:52:13 2022
epoch 2 training:  84%|████████▍ | 164648/196351 [23:27:07<4:32:07,  1.94it/s]global step 361000, epoch: 2, batch: 164649, loss: 0.21483, acc: 0.92704, time: Sun Jan 23 13:53:04 2022
epoch 2 training:  84%|████████▍ | 164748/196351 [23:27:58<4:29:12,  1.96it/s]global step 361100, epoch: 2, batch: 164749, loss: 1.71052, acc: 0.92703, time: Sun Jan 23 13:53:55 2022
epoch 2 training:  84%|████████▍ | 164848/196351 [23:28:49<4:32:37,  1.93it/s]global step 361200, epoch: 2, batch: 164849, loss: 0.94540, acc: 0.92704, time: Sun Jan 23 13:54:47 2022
epoch 2 training:  84%|████████▍ | 164948/196351 [23:29:41<5:02:35,  1.73it/s]global step 361300, epoch: 2, batch: 164949, loss: 0.43432, acc: 0.92704, time: Sun Jan 23 13:55:38 2022
epoch 2 training:  84%|████████▍ | 165048/196351 [23:30:33<4:28:55,  1.94it/s]global step 361400, epoch: 2, batch: 165049, loss: 2.68858, acc: 0.92702, time: Sun Jan 23 13:56:30 2022
epoch 2 training:  84%|████████▍ | 165148/196351 [23:31:24<4:24:54,  1.96it/s]global step 361500, epoch: 2, batch: 165149, loss: 0.19692, acc: 0.92702, time: Sun Jan 23 13:57:22 2022
epoch 2 training:  84%|████████▍ | 165248/196351 [23:32:15<4:26:15,  1.95it/s]global step 361600, epoch: 2, batch: 165249, loss: 0.38102, acc: 0.92701, time: Sun Jan 23 13:58:13 2022
epoch 2 training:  84%|████████▍ | 165348/196351 [23:33:06<4:21:38,  1.97it/s]global step 361700, epoch: 2, batch: 165349, loss: 3.18088, acc: 0.92702, time: Sun Jan 23 13:59:04 2022
epoch 2 training:  84%|████████▍ | 165448/196351 [23:33:58<4:25:57,  1.94it/s]global step 361800, epoch: 2, batch: 165449, loss: 1.87929, acc: 0.92702, time: Sun Jan 23 13:59:55 2022
epoch 2 training:  84%|████████▍ | 165548/196351 [23:34:49<4:20:48,  1.97it/s]global step 361900, epoch: 2, batch: 165549, loss: 0.73292, acc: 0.92703, time: Sun Jan 23 14:00:46 2022
epoch 2 training:  84%|████████▍ | 165648/196351 [23:35:40<4:18:29,  1.98it/s]global step 362000, epoch: 2, batch: 165649, loss: 0.51754, acc: 0.92704, time: Sun Jan 23 14:01:37 2022
epoch 2 training:  84%|████████▍ | 165748/196351 [23:36:32<4:25:01,  1.92it/s]global step 362100, epoch: 2, batch: 165749, loss: 1.06168, acc: 0.92705, time: Sun Jan 23 14:02:29 2022
epoch 2 training:  84%|████████▍ | 165848/196351 [23:37:23<4:18:21,  1.97it/s]global step 362200, epoch: 2, batch: 165849, loss: 2.35038, acc: 0.92705, time: Sun Jan 23 14:03:20 2022
epoch 2 training:  85%|████████▍ | 165948/196351 [23:38:14<4:21:30,  1.94it/s]global step 362300, epoch: 2, batch: 165949, loss: 2.24675, acc: 0.92705, time: Sun Jan 23 14:04:11 2022
epoch 2 training:  85%|████████▍ | 166048/196351 [23:39:05<4:14:02,  1.99it/s]global step 362400, epoch: 2, batch: 166049, loss: 3.19443, acc: 0.92705, time: Sun Jan 23 14:05:02 2022
epoch 2 training:  85%|████████▍ | 166148/196351 [23:39:56<4:16:30,  1.96it/s]global step 362500, epoch: 2, batch: 166149, loss: 1.67953, acc: 0.92705, time: Sun Jan 23 14:05:53 2022
epoch 2 training:  85%|████████▍ | 166248/196351 [23:40:46<4:13:47,  1.98it/s]global step 362600, epoch: 2, batch: 166249, loss: 1.40585, acc: 0.92706, time: Sun Jan 23 14:06:44 2022
epoch 2 training:  85%|████████▍ | 166348/196351 [23:41:37<4:11:29,  1.99it/s]global step 362700, epoch: 2, batch: 166349, loss: 0.93029, acc: 0.92705, time: Sun Jan 23 14:07:34 2022
epoch 2 training:  85%|████████▍ | 166448/196351 [23:42:28<4:15:43,  1.95it/s]global step 362800, epoch: 2, batch: 166449, loss: 2.24149, acc: 0.92703, time: Sun Jan 23 14:08:25 2022
epoch 2 training:  85%|████████▍ | 166548/196351 [23:43:19<4:13:25,  1.96it/s]global step 362900, epoch: 2, batch: 166549, loss: 0.49956, acc: 0.92704, time: Sun Jan 23 14:09:16 2022
epoch 2 training:  85%|████████▍ | 166648/196351 [23:44:10<4:12:54,  1.96it/s]global step 363000, epoch: 2, batch: 166649, loss: 1.90112, acc: 0.92704, time: Sun Jan 23 14:10:07 2022
epoch 2 training:  85%|████████▍ | 166748/196351 [23:45:01<4:10:40,  1.97it/s]global step 363100, epoch: 2, batch: 166749, loss: 0.86428, acc: 0.92704, time: Sun Jan 23 14:10:58 2022
epoch 2 training:  85%|████████▍ | 166848/196351 [23:45:53<4:10:20,  1.96it/s]global step 363200, epoch: 2, batch: 166849, loss: 1.86750, acc: 0.92702, time: Sun Jan 23 14:11:50 2022
epoch 2 training:  85%|████████▌ | 166948/196351 [23:46:44<4:09:13,  1.97it/s]global step 363300, epoch: 2, batch: 166949, loss: 1.68816, acc: 0.92701, time: Sun Jan 23 14:12:41 2022
epoch 2 training:  85%|████████▌ | 167048/196351 [23:47:35<4:09:01,  1.96it/s]global step 363400, epoch: 2, batch: 167049, loss: 4.11929, acc: 0.92701, time: Sun Jan 23 14:13:32 2022
epoch 2 training:  85%|████████▌ | 167148/196351 [23:48:26<4:05:45,  1.98it/s]global step 363500, epoch: 2, batch: 167149, loss: 0.52012, acc: 0.92701, time: Sun Jan 23 14:14:23 2022
epoch 2 training:  85%|████████▌ | 167248/196351 [23:49:17<4:14:17,  1.91it/s]global step 363600, epoch: 2, batch: 167249, loss: 1.09594, acc: 0.92701, time: Sun Jan 23 14:15:15 2022
epoch 2 training:  85%|████████▌ | 167348/196351 [23:50:09<4:10:34,  1.93it/s]global step 363700, epoch: 2, batch: 167349, loss: 2.07205, acc: 0.92702, time: Sun Jan 23 14:16:06 2022
epoch 2 training:  85%|████████▌ | 167448/196351 [23:51:00<4:05:34,  1.96it/s]global step 363800, epoch: 2, batch: 167449, loss: 1.66992, acc: 0.92704, time: Sun Jan 23 14:16:57 2022
epoch 2 training:  85%|████████▌ | 167548/196351 [23:51:51<4:09:13,  1.93it/s]global step 363900, epoch: 2, batch: 167549, loss: 0.13153, acc: 0.92705, time: Sun Jan 23 14:17:49 2022
epoch 2 training:  85%|████████▌ | 167648/196351 [23:52:42<4:04:12,  1.96it/s]global step 364000, epoch: 2, batch: 167649, loss: 0.98720, acc: 0.92706, time: Sun Jan 23 14:18:40 2022
epoch 2 training:  85%|████████▌ | 167748/196351 [23:53:33<4:00:00,  1.99it/s]global step 364100, epoch: 2, batch: 167749, loss: 2.73123, acc: 0.92705, time: Sun Jan 23 14:19:31 2022
epoch 2 training:  85%|████████▌ | 167848/196351 [23:54:25<3:59:10,  1.99it/s]global step 364200, epoch: 2, batch: 167849, loss: 0.19642, acc: 0.92705, time: Sun Jan 23 14:20:22 2022
epoch 2 training:  86%|████████▌ | 167948/196351 [23:55:15<3:59:40,  1.98it/s]global step 364300, epoch: 2, batch: 167949, loss: 1.28842, acc: 0.92704, time: Sun Jan 23 14:21:13 2022
epoch 2 training:  86%|████████▌ | 168048/196351 [23:56:07<3:59:09,  1.97it/s]global step 364400, epoch: 2, batch: 168049, loss: 1.04069, acc: 0.92706, time: Sun Jan 23 14:22:04 2022
epoch 2 training:  86%|████████▌ | 168148/196351 [23:56:58<4:02:37,  1.94it/s]global step 364500, epoch: 2, batch: 168149, loss: 0.73289, acc: 0.92706, time: Sun Jan 23 14:22:56 2022
epoch 2 training:  86%|████████▌ | 168248/196351 [23:57:50<4:02:35,  1.93it/s]global step 364600, epoch: 2, batch: 168249, loss: 2.04694, acc: 0.92706, time: Sun Jan 23 14:23:47 2022
epoch 2 training:  86%|████████▌ | 168348/196351 [23:58:41<3:58:19,  1.96it/s]global step 364700, epoch: 2, batch: 168349, loss: 3.22823, acc: 0.92708, time: Sun Jan 23 14:24:39 2022
epoch 2 training:  86%|████████▌ | 168448/196351 [23:59:32<3:57:27,  1.96it/s]global step 364800, epoch: 2, batch: 168449, loss: 2.49890, acc: 0.92710, time: Sun Jan 23 14:25:30 2022
epoch 2 training:  86%|████████▌ | 168548/196351 [24:00:24<3:57:32,  1.95it/s]global step 364900, epoch: 2, batch: 168549, loss: 1.11126, acc: 0.92709, time: Sun Jan 23 14:26:21 2022
epoch 2 training:  86%|████████▌ | 168648/196351 [24:01:15<3:58:15,  1.94it/s]global step 365000, epoch: 2, batch: 168649, loss: 1.83775, acc: 0.92707, time: Sun Jan 23 14:27:13 2022
epoch 2 training:  86%|████████▌ | 168748/196351 [24:02:06<3:51:42,  1.99it/s]global step 365100, epoch: 2, batch: 168749, loss: 0.45523, acc: 0.92707, time: Sun Jan 23 14:28:04 2022
epoch 2 training:  86%|████████▌ | 168848/196351 [24:02:58<3:54:12,  1.96it/s]global step 365200, epoch: 2, batch: 168849, loss: 1.82504, acc: 0.92707, time: Sun Jan 23 14:28:56 2022
epoch 2 training:  86%|████████▌ | 168948/196351 [24:03:49<3:47:14,  2.01it/s]global step 365300, epoch: 2, batch: 168949, loss: 2.45751, acc: 0.92707, time: Sun Jan 23 14:29:47 2022
epoch 2 training:  86%|████████▌ | 169048/196351 [24:04:40<3:54:03,  1.94it/s]global step 365400, epoch: 2, batch: 169049, loss: 0.82560, acc: 0.92707, time: Sun Jan 23 14:30:38 2022
epoch 2 training:  86%|████████▌ | 169148/196351 [24:05:32<3:49:44,  1.97it/s]global step 365500, epoch: 2, batch: 169149, loss: 1.14988, acc: 0.92705, time: Sun Jan 23 14:31:29 2022
epoch 2 training:  86%|████████▌ | 169248/196351 [24:06:22<3:51:56,  1.95it/s]global step 365600, epoch: 2, batch: 169249, loss: 1.36374, acc: 0.92706, time: Sun Jan 23 14:32:20 2022
epoch 2 training:  86%|████████▌ | 169348/196351 [24:07:13<3:46:36,  1.99it/s]global step 365700, epoch: 2, batch: 169349, loss: 0.60256, acc: 0.92704, time: Sun Jan 23 14:33:11 2022
epoch 2 training:  86%|████████▋ | 169448/196351 [24:08:04<3:46:07,  1.98it/s]global step 365800, epoch: 2, batch: 169449, loss: 5.12810, acc: 0.92705, time: Sun Jan 23 14:34:01 2022
epoch 2 training:  86%|████████▋ | 169548/196351 [24:08:55<3:53:15,  1.92it/s]global step 365900, epoch: 2, batch: 169549, loss: 1.47505, acc: 0.92704, time: Sun Jan 23 14:34:53 2022
epoch 2 training:  86%|████████▋ | 169648/196351 [24:09:47<3:47:07,  1.96it/s]global step 366000, epoch: 2, batch: 169649, loss: 1.00227, acc: 0.92706, time: Sun Jan 23 14:35:44 2022
epoch 2 training:  86%|████████▋ | 169748/196351 [24:10:38<3:49:32,  1.93it/s]global step 366100, epoch: 2, batch: 169749, loss: 0.19556, acc: 0.92706, time: Sun Jan 23 14:36:35 2022
epoch 2 training:  87%|████████▋ | 169848/196351 [24:11:29<3:43:51,  1.97it/s]global step 366200, epoch: 2, batch: 169849, loss: 2.36756, acc: 0.92707, time: Sun Jan 23 14:37:27 2022
epoch 2 training:  87%|████████▋ | 169948/196351 [24:12:21<3:47:41,  1.93it/s]global step 366300, epoch: 2, batch: 169949, loss: 2.34224, acc: 0.92708, time: Sun Jan 23 14:38:18 2022
epoch 2 training:  87%|████████▋ | 170048/196351 [24:13:12<3:46:38,  1.93it/s]global step 366400, epoch: 2, batch: 170049, loss: 0.68551, acc: 0.92709, time: Sun Jan 23 14:39:10 2022
epoch 2 training:  87%|████████▋ | 170148/196351 [24:14:04<3:42:22,  1.96it/s]global step 366500, epoch: 2, batch: 170149, loss: 1.57178, acc: 0.92708, time: Sun Jan 23 14:40:01 2022
epoch 2 training:  87%|████████▋ | 170248/196351 [24:14:55<3:47:46,  1.91it/s]global step 366600, epoch: 2, batch: 170249, loss: 2.18016, acc: 0.92707, time: Sun Jan 23 14:40:53 2022
epoch 2 training:  87%|████████▋ | 170348/196351 [24:15:46<3:44:50,  1.93it/s]global step 366700, epoch: 2, batch: 170349, loss: 0.44550, acc: 0.92708, time: Sun Jan 23 14:41:44 2022
epoch 2 training:  87%|████████▋ | 170448/196351 [24:16:38<3:38:49,  1.97it/s]global step 366800, epoch: 2, batch: 170449, loss: 3.22496, acc: 0.92709, time: Sun Jan 23 14:42:36 2022
epoch 2 training:  87%|████████▋ | 170548/196351 [24:17:30<3:44:30,  1.92it/s]global step 366900, epoch: 2, batch: 170549, loss: 1.18031, acc: 0.92709, time: Sun Jan 23 14:43:27 2022
epoch 2 training:  87%|████████▋ | 170648/196351 [24:18:22<3:43:33,  1.92it/s]global step 367000, epoch: 2, batch: 170649, loss: 2.59524, acc: 0.92709, time: Sun Jan 23 14:44:19 2022
epoch 2 training:  87%|████████▋ | 170748/196351 [24:19:13<3:35:57,  1.98it/s]global step 367100, epoch: 2, batch: 170749, loss: 0.70177, acc: 0.92710, time: Sun Jan 23 14:45:10 2022
epoch 2 training:  87%|████████▋ | 170848/196351 [24:20:04<3:33:32,  1.99it/s]global step 367200, epoch: 2, batch: 170849, loss: 0.64711, acc: 0.92708, time: Sun Jan 23 14:46:01 2022
epoch 2 training:  87%|████████▋ | 170948/196351 [24:20:55<3:38:33,  1.94it/s]global step 367300, epoch: 2, batch: 170949, loss: 1.13500, acc: 0.92708, time: Sun Jan 23 14:46:52 2022
epoch 2 training:  87%|████████▋ | 171048/196351 [24:21:46<3:35:14,  1.96it/s]global step 367400, epoch: 2, batch: 171049, loss: 0.24240, acc: 0.92709, time: Sun Jan 23 14:47:44 2022
epoch 2 training:  87%|████████▋ | 171148/196351 [24:22:38<3:38:04,  1.93it/s]global step 367500, epoch: 2, batch: 171149, loss: 1.35593, acc: 0.92709, time: Sun Jan 23 14:48:35 2022
epoch 2 training:  87%|████████▋ | 171248/196351 [24:23:29<3:34:50,  1.95it/s]global step 367600, epoch: 2, batch: 171249, loss: 3.29676, acc: 0.92709, time: Sun Jan 23 14:49:26 2022
epoch 2 training:  87%|████████▋ | 171348/196351 [24:24:20<3:35:44,  1.93it/s]global step 367700, epoch: 2, batch: 171349, loss: 3.20110, acc: 0.92710, time: Sun Jan 23 14:50:17 2022
epoch 2 training:  87%|████████▋ | 171448/196351 [24:25:11<3:32:22,  1.95it/s]global step 367800, epoch: 2, batch: 171449, loss: 1.96095, acc: 0.92708, time: Sun Jan 23 14:51:09 2022
epoch 2 training:  87%|████████▋ | 171548/196351 [24:26:02<3:29:39,  1.97it/s]global step 367900, epoch: 2, batch: 171549, loss: 0.15929, acc: 0.92708, time: Sun Jan 23 14:52:00 2022
epoch 2 training:  87%|████████▋ | 171648/196351 [24:26:53<3:31:06,  1.95it/s]global step 368000, epoch: 2, batch: 171649, loss: 0.37364, acc: 0.92708, time: Sun Jan 23 14:52:51 2022
epoch 2 training:  87%|████████▋ | 171748/196351 [24:27:45<3:28:34,  1.97it/s]global step 368100, epoch: 2, batch: 171749, loss: 2.41222, acc: 0.92708, time: Sun Jan 23 14:53:43 2022
epoch 2 training:  88%|████████▊ | 171848/196351 [24:28:36<3:28:19,  1.96it/s]global step 368200, epoch: 2, batch: 171849, loss: 1.07534, acc: 0.92709, time: Sun Jan 23 14:54:34 2022
epoch 2 training:  88%|████████▊ | 171948/196351 [24:29:27<3:33:39,  1.90it/s]global step 368300, epoch: 2, batch: 171949, loss: 2.06286, acc: 0.92708, time: Sun Jan 23 14:55:25 2022
epoch 2 training:  88%|████████▊ | 172048/196351 [24:30:18<3:24:42,  1.98it/s]global step 368400, epoch: 2, batch: 172049, loss: 3.29296, acc: 0.92708, time: Sun Jan 23 14:56:16 2022
epoch 2 training:  88%|████████▊ | 172148/196351 [24:31:09<3:24:52,  1.97it/s]global step 368500, epoch: 2, batch: 172149, loss: 2.90819, acc: 0.92706, time: Sun Jan 23 14:57:07 2022
epoch 2 training:  88%|████████▊ | 172248/196351 [24:32:00<3:26:46,  1.94it/s]global step 368600, epoch: 2, batch: 172249, loss: 0.34284, acc: 0.92708, time: Sun Jan 23 14:57:58 2022
epoch 2 training:  88%|████████▊ | 172348/196351 [24:32:51<3:20:58,  1.99it/s]global step 368700, epoch: 2, batch: 172349, loss: 2.04532, acc: 0.92710, time: Sun Jan 23 14:58:49 2022
epoch 2 training:  88%|████████▊ | 172448/196351 [24:33:43<3:26:59,  1.92it/s]global step 368800, epoch: 2, batch: 172449, loss: 0.42689, acc: 0.92711, time: Sun Jan 23 14:59:40 2022
epoch 2 training:  88%|████████▊ | 172548/196351 [24:34:34<3:22:34,  1.96it/s]global step 368900, epoch: 2, batch: 172549, loss: 1.37752, acc: 0.92713, time: Sun Jan 23 15:00:32 2022
epoch 2 training:  88%|████████▊ | 172648/196351 [24:35:25<3:20:15,  1.97it/s]global step 369000, epoch: 2, batch: 172649, loss: 1.08306, acc: 0.92713, time: Sun Jan 23 15:01:23 2022
epoch 2 training:  88%|████████▊ | 172748/196351 [24:36:17<3:19:20,  1.97it/s]global step 369100, epoch: 2, batch: 172749, loss: 3.13374, acc: 0.92714, time: Sun Jan 23 15:02:14 2022
epoch 2 training:  88%|████████▊ | 172848/196351 [24:37:08<3:20:04,  1.96it/s]global step 369200, epoch: 2, batch: 172849, loss: 0.51606, acc: 0.92715, time: Sun Jan 23 15:03:06 2022
epoch 2 training:  88%|████████▊ | 172948/196351 [24:37:59<3:16:20,  1.99it/s]global step 369300, epoch: 2, batch: 172949, loss: 2.01548, acc: 0.92714, time: Sun Jan 23 15:03:57 2022
epoch 2 training:  88%|████████▊ | 173048/196351 [24:38:50<3:18:11,  1.96it/s]global step 369400, epoch: 2, batch: 173049, loss: 0.90932, acc: 0.92716, time: Sun Jan 23 15:04:48 2022
epoch 2 training:  88%|████████▊ | 173148/196351 [24:39:42<3:17:41,  1.96it/s]global step 369500, epoch: 2, batch: 173149, loss: 2.62809, acc: 0.92717, time: Sun Jan 23 15:05:39 2022
epoch 2 training:  88%|████████▊ | 173248/196351 [24:40:33<3:14:46,  1.98it/s]global step 369600, epoch: 2, batch: 173249, loss: 1.12445, acc: 0.92717, time: Sun Jan 23 15:06:30 2022
epoch 2 training:  88%|████████▊ | 173348/196351 [24:41:24<3:15:34,  1.96it/s]global step 369700, epoch: 2, batch: 173349, loss: 0.44655, acc: 0.92718, time: Sun Jan 23 15:07:21 2022
epoch 2 training:  88%|████████▊ | 173448/196351 [24:42:15<3:13:20,  1.97it/s]global step 369800, epoch: 2, batch: 173449, loss: 3.43761, acc: 0.92718, time: Sun Jan 23 15:08:12 2022
epoch 2 training:  88%|████████▊ | 173548/196351 [24:43:06<3:17:16,  1.93it/s]global step 369900, epoch: 2, batch: 173549, loss: 2.94630, acc: 0.92719, time: Sun Jan 23 15:09:03 2022
epoch 2 training:  88%|████████▊ | 173648/196351 [24:43:57<3:13:54,  1.95it/s]global step 370000, epoch: 2, batch: 173649, loss: 2.33221, acc: 0.92719, time: Sun Jan 23 15:09:55 2022
epoch 2 training:  88%|████████▊ | 173748/196351 [24:44:49<3:11:50,  1.96it/s]global step 370100, epoch: 2, batch: 173749, loss: 0.81915, acc: 0.92719, time: Sun Jan 23 15:10:46 2022
epoch 2 training:  89%|████████▊ | 173848/196351 [24:45:40<3:12:03,  1.95it/s]global step 370200, epoch: 2, batch: 173849, loss: 2.71089, acc: 0.92719, time: Sun Jan 23 15:11:37 2022
epoch 2 training:  89%|████████▊ | 173948/196351 [24:46:31<3:08:14,  1.98it/s]global step 370300, epoch: 2, batch: 173949, loss: 0.29593, acc: 0.92720, time: Sun Jan 23 15:12:29 2022
epoch 2 training:  89%|████████▊ | 174048/196351 [24:47:22<3:08:45,  1.97it/s]global step 370400, epoch: 2, batch: 174049, loss: 0.72931, acc: 0.92720, time: Sun Jan 23 15:13:20 2022
epoch 2 training:  89%|████████▊ | 174148/196351 [24:48:13<3:06:33,  1.98it/s]global step 370500, epoch: 2, batch: 174149, loss: 0.37497, acc: 0.92720, time: Sun Jan 23 15:14:11 2022
epoch 2 training:  89%|████████▊ | 174248/196351 [24:49:04<3:08:07,  1.96it/s]global step 370600, epoch: 2, batch: 174249, loss: 3.18044, acc: 0.92719, time: Sun Jan 23 15:15:02 2022
epoch 2 training:  89%|████████▉ | 174348/196351 [24:49:55<3:07:11,  1.96it/s]global step 370700, epoch: 2, batch: 174349, loss: 2.49924, acc: 0.92720, time: Sun Jan 23 15:15:53 2022
epoch 2 training:  89%|████████▉ | 174448/196351 [24:50:46<3:06:17,  1.96it/s]global step 370800, epoch: 2, batch: 174449, loss: 0.45463, acc: 0.92721, time: Sun Jan 23 15:16:44 2022
epoch 2 training:  89%|████████▉ | 174548/196351 [24:51:38<3:03:04,  1.98it/s]global step 370900, epoch: 2, batch: 174549, loss: 2.57993, acc: 0.92721, time: Sun Jan 23 15:17:35 2022
epoch 2 training:  89%|████████▉ | 174648/196351 [24:52:29<3:06:58,  1.93it/s]global step 371000, epoch: 2, batch: 174649, loss: 1.24873, acc: 0.92720, time: Sun Jan 23 15:18:26 2022
epoch 2 training:  89%|████████▉ | 174748/196351 [24:53:20<3:03:50,  1.96it/s]global step 371100, epoch: 2, batch: 174749, loss: 0.87444, acc: 0.92719, time: Sun Jan 23 15:19:17 2022
epoch 2 training:  89%|████████▉ | 174848/196351 [24:54:11<3:04:16,  1.94it/s]global step 371200, epoch: 2, batch: 174849, loss: 0.89666, acc: 0.92720, time: Sun Jan 23 15:20:08 2022
epoch 2 training:  89%|████████▉ | 174948/196351 [24:55:02<3:01:38,  1.96it/s]global step 371300, epoch: 2, batch: 174949, loss: 1.20461, acc: 0.92720, time: Sun Jan 23 15:20:59 2022
epoch 2 training:  89%|████████▉ | 175048/196351 [24:55:53<3:01:47,  1.95it/s]global step 371400, epoch: 2, batch: 175049, loss: 0.22583, acc: 0.92721, time: Sun Jan 23 15:21:50 2022
epoch 2 training:  89%|████████▉ | 175148/196351 [24:56:44<3:00:05,  1.96it/s]global step 371500, epoch: 2, batch: 175149, loss: 0.45869, acc: 0.92720, time: Sun Jan 23 15:22:42 2022
epoch 2 training:  89%|████████▉ | 175248/196351 [24:57:35<2:58:18,  1.97it/s]global step 371600, epoch: 2, batch: 175249, loss: 2.30041, acc: 0.92721, time: Sun Jan 23 15:23:33 2022
epoch 2 training:  89%|████████▉ | 175348/196351 [24:58:26<2:58:48,  1.96it/s]global step 371700, epoch: 2, batch: 175349, loss: 1.47954, acc: 0.92723, time: Sun Jan 23 15:24:24 2022
epoch 2 training:  89%|████████▉ | 175448/196351 [24:59:18<3:00:14,  1.93it/s]global step 371800, epoch: 2, batch: 175449, loss: 0.57508, acc: 0.92723, time: Sun Jan 23 15:25:15 2022
epoch 2 training:  89%|████████▉ | 175548/196351 [25:00:09<3:00:30,  1.92it/s]global step 371900, epoch: 2, batch: 175549, loss: 2.60134, acc: 0.92721, time: Sun Jan 23 15:26:07 2022
epoch 2 training:  89%|████████▉ | 175648/196351 [25:01:01<2:58:23,  1.93it/s]global step 372000, epoch: 2, batch: 175649, loss: 4.18010, acc: 0.92720, time: Sun Jan 23 15:26:59 2022
epoch 2 training:  90%|████████▉ | 175748/196351 [25:01:53<2:55:54,  1.95it/s]global step 372100, epoch: 2, batch: 175749, loss: 5.15133, acc: 0.92719, time: Sun Jan 23 15:27:50 2022
epoch 2 training:  90%|████████▉ | 175848/196351 [25:02:44<2:53:22,  1.97it/s]global step 372200, epoch: 2, batch: 175849, loss: 2.12759, acc: 0.92719, time: Sun Jan 23 15:28:41 2022
epoch 2 training:  90%|████████▉ | 175948/196351 [25:03:35<2:53:06,  1.96it/s]global step 372300, epoch: 2, batch: 175949, loss: 0.90430, acc: 0.92718, time: Sun Jan 23 15:29:33 2022
epoch 2 training:  90%|████████▉ | 176048/196351 [25:04:27<2:50:58,  1.98it/s]global step 372400, epoch: 2, batch: 176049, loss: 4.23470, acc: 0.92720, time: Sun Jan 23 15:30:24 2022
epoch 2 training:  90%|████████▉ | 176148/196351 [25:05:19<2:57:41,  1.89it/s]global step 372500, epoch: 2, batch: 176149, loss: 0.58102, acc: 0.92720, time: Sun Jan 23 15:31:16 2022
epoch 2 training:  90%|████████▉ | 176248/196351 [25:06:11<2:54:38,  1.92it/s]global step 372600, epoch: 2, batch: 176249, loss: 0.82305, acc: 0.92719, time: Sun Jan 23 15:32:08 2022
epoch 2 training:  90%|████████▉ | 176348/196351 [25:07:02<2:52:28,  1.93it/s]global step 372700, epoch: 2, batch: 176349, loss: 2.96316, acc: 0.92721, time: Sun Jan 23 15:33:00 2022
epoch 2 training:  90%|████████▉ | 176448/196351 [25:07:54<2:48:19,  1.97it/s]global step 372800, epoch: 2, batch: 176449, loss: 1.77941, acc: 0.92721, time: Sun Jan 23 15:33:51 2022
epoch 2 training:  90%|████████▉ | 176548/196351 [25:08:46<2:53:32,  1.90it/s]global step 372900, epoch: 2, batch: 176549, loss: 0.58341, acc: 0.92722, time: Sun Jan 23 15:34:43 2022
epoch 2 training:  90%|████████▉ | 176648/196351 [25:09:37<2:49:55,  1.93it/s]global step 373000, epoch: 2, batch: 176649, loss: 1.02388, acc: 0.92722, time: Sun Jan 23 15:35:34 2022
epoch 2 training:  90%|█████████ | 176748/196351 [25:10:28<2:48:47,  1.94it/s]global step 373100, epoch: 2, batch: 176749, loss: 0.48649, acc: 0.92723, time: Sun Jan 23 15:36:25 2022
epoch 2 training:  90%|█████████ | 176848/196351 [25:11:19<2:45:55,  1.96it/s]global step 373200, epoch: 2, batch: 176849, loss: 0.41030, acc: 0.92720, time: Sun Jan 23 15:37:16 2022
epoch 2 training:  90%|█████████ | 176948/196351 [25:12:10<2:45:13,  1.96it/s]global step 373300, epoch: 2, batch: 176949, loss: 0.48279, acc: 0.92720, time: Sun Jan 23 15:38:07 2022
epoch 2 training:  90%|█████████ | 177048/196351 [25:13:00<2:43:24,  1.97it/s]global step 373400, epoch: 2, batch: 177049, loss: 0.62890, acc: 0.92720, time: Sun Jan 23 15:38:58 2022
epoch 2 training:  90%|█████████ | 177148/196351 [25:13:51<2:42:20,  1.97it/s]global step 373500, epoch: 2, batch: 177149, loss: 1.83849, acc: 0.92720, time: Sun Jan 23 15:39:49 2022
epoch 2 training:  90%|█████████ | 177248/196351 [25:14:43<2:41:59,  1.97it/s]global step 373600, epoch: 2, batch: 177249, loss: 0.24792, acc: 0.92722, time: Sun Jan 23 15:40:40 2022
epoch 2 training:  90%|█████████ | 177348/196351 [25:15:34<2:45:03,  1.92it/s]global step 373700, epoch: 2, batch: 177349, loss: 2.96023, acc: 0.92723, time: Sun Jan 23 15:41:32 2022
epoch 2 training:  90%|█████████ | 177448/196351 [25:16:26<2:43:30,  1.93it/s]global step 373800, epoch: 2, batch: 177449, loss: 0.11038, acc: 0.92724, time: Sun Jan 23 15:42:23 2022
epoch 2 training:  90%|█████████ | 177548/196351 [25:17:17<2:41:48,  1.94it/s]global step 373900, epoch: 2, batch: 177549, loss: 1.45980, acc: 0.92725, time: Sun Jan 23 15:43:15 2022
epoch 2 training:  90%|█████████ | 177648/196351 [25:18:09<2:41:45,  1.93it/s]global step 374000, epoch: 2, batch: 177649, loss: 1.38042, acc: 0.92725, time: Sun Jan 23 15:44:06 2022
epoch 2 training:  91%|█████████ | 177748/196351 [25:19:00<2:39:49,  1.94it/s]global step 374100, epoch: 2, batch: 177749, loss: 1.24504, acc: 0.92726, time: Sun Jan 23 15:44:58 2022
epoch 2 training:  91%|█████████ | 177848/196351 [25:19:52<2:35:45,  1.98it/s]global step 374200, epoch: 2, batch: 177849, loss: 1.79616, acc: 0.92725, time: Sun Jan 23 15:45:49 2022
epoch 2 training:  91%|█████████ | 177948/196351 [25:20:43<2:39:15,  1.93it/s]global step 374300, epoch: 2, batch: 177949, loss: 1.78780, acc: 0.92725, time: Sun Jan 23 15:46:41 2022
epoch 2 training:  91%|█████████ | 178048/196351 [25:21:34<2:35:26,  1.96it/s]global step 374400, epoch: 2, batch: 178049, loss: 1.00250, acc: 0.92725, time: Sun Jan 23 15:47:32 2022
epoch 2 training:  91%|█████████ | 178148/196351 [25:22:25<2:33:04,  1.98it/s]global step 374500, epoch: 2, batch: 178149, loss: 1.25554, acc: 0.92726, time: Sun Jan 23 15:48:23 2022
epoch 2 training:  91%|█████████ | 178248/196351 [25:23:17<2:32:41,  1.98it/s]global step 374600, epoch: 2, batch: 178249, loss: 1.69034, acc: 0.92725, time: Sun Jan 23 15:49:14 2022
epoch 2 training:  91%|█████████ | 178348/196351 [25:24:08<2:34:51,  1.94it/s]global step 374700, epoch: 2, batch: 178349, loss: 0.91959, acc: 0.92727, time: Sun Jan 23 15:50:05 2022
epoch 2 training:  91%|█████████ | 178448/196351 [25:25:00<2:32:49,  1.95it/s]global step 374800, epoch: 2, batch: 178449, loss: 0.53884, acc: 0.92726, time: Sun Jan 23 15:50:57 2022
epoch 2 training:  91%|█████████ | 178548/196351 [25:25:51<2:31:41,  1.96it/s]global step 374900, epoch: 2, batch: 178549, loss: 0.33110, acc: 0.92726, time: Sun Jan 23 15:51:48 2022
epoch 2 training:  91%|█████████ | 178648/196351 [25:26:42<2:28:31,  1.99it/s]global step 375000, epoch: 2, batch: 178649, loss: 0.39917, acc: 0.92725, time: Sun Jan 23 15:52:39 2022
epoch 2 training:  91%|█████████ | 178748/196351 [25:27:33<2:30:12,  1.95it/s]global step 375100, epoch: 2, batch: 178749, loss: 3.49998, acc: 0.92725, time: Sun Jan 23 15:53:30 2022
epoch 2 training:  91%|█████████ | 178848/196351 [25:28:25<2:29:48,  1.95it/s]global step 375200, epoch: 2, batch: 178849, loss: 0.55028, acc: 0.92725, time: Sun Jan 23 15:54:22 2022
epoch 2 training:  91%|█████████ | 178948/196351 [25:29:16<2:29:13,  1.94it/s]global step 375300, epoch: 2, batch: 178949, loss: 0.37314, acc: 0.92725, time: Sun Jan 23 15:55:13 2022
epoch 2 training:  91%|█████████ | 179048/196351 [25:30:08<2:28:57,  1.94it/s]global step 375400, epoch: 2, batch: 179049, loss: 1.54680, acc: 0.92724, time: Sun Jan 23 15:56:05 2022
epoch 2 training:  91%|█████████ | 179148/196351 [25:30:59<2:29:33,  1.92it/s]global step 375500, epoch: 2, batch: 179149, loss: 3.52428, acc: 0.92723, time: Sun Jan 23 15:56:57 2022
epoch 2 training:  91%|█████████▏| 179248/196351 [25:31:51<2:26:31,  1.95it/s]global step 375600, epoch: 2, batch: 179249, loss: 0.65207, acc: 0.92721, time: Sun Jan 23 15:57:48 2022
epoch 2 training:  91%|█████████▏| 179348/196351 [25:32:41<2:23:21,  1.98it/s]global step 375700, epoch: 2, batch: 179349, loss: 1.09143, acc: 0.92721, time: Sun Jan 23 15:58:39 2022
epoch 2 training:  91%|█████████▏| 179448/196351 [25:33:33<2:22:17,  1.98it/s]global step 375800, epoch: 2, batch: 179449, loss: 3.54703, acc: 0.92722, time: Sun Jan 23 15:59:31 2022
epoch 2 training:  91%|█████████▏| 179548/196351 [25:34:24<2:24:28,  1.94it/s]global step 375900, epoch: 2, batch: 179549, loss: 2.52168, acc: 0.92722, time: Sun Jan 23 16:00:22 2022
epoch 2 training:  91%|█████████▏| 179648/196351 [25:35:16<2:24:07,  1.93it/s]global step 376000, epoch: 2, batch: 179649, loss: 1.88649, acc: 0.92722, time: Sun Jan 23 16:01:13 2022
epoch 2 training:  92%|█████████▏| 179748/196351 [25:36:07<2:20:50,  1.96it/s]global step 376100, epoch: 2, batch: 179749, loss: 2.15563, acc: 0.92722, time: Sun Jan 23 16:02:04 2022
epoch 2 training:  92%|█████████▏| 179848/196351 [25:36:58<2:18:24,  1.99it/s]global step 376200, epoch: 2, batch: 179849, loss: 1.73314, acc: 0.92724, time: Sun Jan 23 16:02:56 2022
epoch 2 training:  92%|█████████▏| 179948/196351 [25:37:49<2:17:54,  1.98it/s]global step 376300, epoch: 2, batch: 179949, loss: 0.71771, acc: 0.92723, time: Sun Jan 23 16:03:46 2022
epoch 2 training:  92%|█████████▏| 180048/196351 [25:38:40<2:18:49,  1.96it/s]global step 376400, epoch: 2, batch: 180049, loss: 1.89528, acc: 0.92722, time: Sun Jan 23 16:04:38 2022
epoch 2 training:  92%|█████████▏| 180148/196351 [25:39:31<2:18:18,  1.95it/s]global step 376500, epoch: 2, batch: 180149, loss: 0.68190, acc: 0.92724, time: Sun Jan 23 16:05:29 2022
epoch 2 training:  92%|█████████▏| 180248/196351 [25:40:23<2:16:35,  1.96it/s]global step 376600, epoch: 2, batch: 180249, loss: 1.85170, acc: 0.92724, time: Sun Jan 23 16:06:20 2022
epoch 2 training:  92%|█████████▏| 180348/196351 [25:41:14<2:14:10,  1.99it/s]global step 376700, epoch: 2, batch: 180349, loss: 0.24506, acc: 0.92724, time: Sun Jan 23 16:07:12 2022
epoch 2 training:  92%|█████████▏| 180448/196351 [25:42:06<2:16:21,  1.94it/s]global step 376800, epoch: 2, batch: 180449, loss: 3.29883, acc: 0.92725, time: Sun Jan 23 16:08:03 2022
epoch 2 training:  92%|█████████▏| 180548/196351 [25:42:57<2:14:30,  1.96it/s]global step 376900, epoch: 2, batch: 180549, loss: 3.22880, acc: 0.92726, time: Sun Jan 23 16:08:55 2022
epoch 2 training:  92%|█████████▏| 180648/196351 [25:43:48<2:14:19,  1.95it/s]global step 377000, epoch: 2, batch: 180649, loss: 0.89578, acc: 0.92727, time: Sun Jan 23 16:09:46 2022
epoch 2 training:  92%|█████████▏| 180748/196351 [25:44:40<2:14:00,  1.94it/s]global step 377100, epoch: 2, batch: 180749, loss: 1.25283, acc: 0.92727, time: Sun Jan 23 16:10:37 2022
epoch 2 training:  92%|█████████▏| 180848/196351 [25:45:31<2:13:01,  1.94it/s]global step 377200, epoch: 2, batch: 180849, loss: 1.28112, acc: 0.92729, time: Sun Jan 23 16:11:29 2022
epoch 2 training:  92%|█████████▏| 180948/196351 [25:46:23<2:13:31,  1.92it/s]global step 377300, epoch: 2, batch: 180949, loss: 0.81639, acc: 0.92729, time: Sun Jan 23 16:12:20 2022
epoch 2 training:  92%|█████████▏| 181048/196351 [25:47:14<2:11:52,  1.93it/s]global step 377400, epoch: 2, batch: 181049, loss: 1.67299, acc: 0.92730, time: Sun Jan 23 16:13:12 2022
epoch 2 training:  92%|█████████▏| 181148/196351 [25:48:06<2:10:15,  1.95it/s]global step 377500, epoch: 2, batch: 181149, loss: 0.52600, acc: 0.92730, time: Sun Jan 23 16:14:03 2022
epoch 2 training:  92%|█████████▏| 181248/196351 [25:48:57<2:08:45,  1.95it/s]global step 377600, epoch: 2, batch: 181249, loss: 2.69771, acc: 0.92729, time: Sun Jan 23 16:14:54 2022
epoch 2 training:  92%|█████████▏| 181348/196351 [25:49:48<2:09:47,  1.93it/s]global step 377700, epoch: 2, batch: 181349, loss: 1.74117, acc: 0.92729, time: Sun Jan 23 16:15:46 2022
epoch 2 training:  92%|█████████▏| 181448/196351 [25:50:40<2:10:40,  1.90it/s]global step 377800, epoch: 2, batch: 181449, loss: 2.02342, acc: 0.92730, time: Sun Jan 23 16:16:37 2022
epoch 2 training:  92%|█████████▏| 181548/196351 [25:51:31<2:06:19,  1.95it/s]global step 377900, epoch: 2, batch: 181549, loss: 0.97353, acc: 0.92731, time: Sun Jan 23 16:17:29 2022
epoch 2 training:  93%|█████████▎| 181648/196351 [25:52:23<2:05:32,  1.95it/s]global step 378000, epoch: 2, batch: 181649, loss: 3.59610, acc: 0.92732, time: Sun Jan 23 16:18:20 2022
epoch 2 training:  93%|█████████▎| 181748/196351 [25:53:15<2:03:37,  1.97it/s]global step 378100, epoch: 2, batch: 181749, loss: 2.26745, acc: 0.92732, time: Sun Jan 23 16:19:12 2022
epoch 2 training:  93%|█████████▎| 181848/196351 [25:54:06<2:07:09,  1.90it/s]global step 378200, epoch: 2, batch: 181849, loss: 1.07434, acc: 0.92733, time: Sun Jan 23 16:20:03 2022
epoch 2 training:  93%|█████████▎| 181948/196351 [25:54:57<2:05:54,  1.91it/s]global step 378300, epoch: 2, batch: 181949, loss: 2.00161, acc: 0.92734, time: Sun Jan 23 16:20:55 2022
epoch 2 training:  93%|█████████▎| 182048/196351 [25:55:48<2:01:17,  1.97it/s]global step 378400, epoch: 2, batch: 182049, loss: 1.72307, acc: 0.92734, time: Sun Jan 23 16:21:46 2022
epoch 2 training:  93%|█████████▎| 182148/196351 [25:56:39<2:00:46,  1.96it/s]global step 378500, epoch: 2, batch: 182149, loss: 3.13371, acc: 0.92733, time: Sun Jan 23 16:22:37 2022
epoch 2 training:  93%|█████████▎| 182248/196351 [25:57:31<2:00:34,  1.95it/s]global step 378600, epoch: 2, batch: 182249, loss: 0.54398, acc: 0.92732, time: Sun Jan 23 16:23:28 2022
epoch 2 training:  93%|█████████▎| 182348/196351 [25:58:22<2:00:03,  1.94it/s]global step 378700, epoch: 2, batch: 182349, loss: 4.03316, acc: 0.92732, time: Sun Jan 23 16:24:19 2022
epoch 2 training:  93%|█████████▎| 182448/196351 [25:59:13<1:58:39,  1.95it/s]global step 378800, epoch: 2, batch: 182449, loss: 0.70287, acc: 0.92734, time: Sun Jan 23 16:25:10 2022
epoch 2 training:  93%|█████████▎| 182548/196351 [26:00:03<1:57:38,  1.96it/s]global step 378900, epoch: 2, batch: 182549, loss: 0.92874, acc: 0.92734, time: Sun Jan 23 16:26:01 2022
epoch 2 training:  93%|█████████▎| 182648/196351 [26:00:55<1:58:08,  1.93it/s]global step 379000, epoch: 2, batch: 182649, loss: 0.61247, acc: 0.92734, time: Sun Jan 23 16:26:52 2022
epoch 2 training:  93%|█████████▎| 182748/196351 [26:01:46<1:55:34,  1.96it/s]global step 379100, epoch: 2, batch: 182749, loss: 2.07771, acc: 0.92733, time: Sun Jan 23 16:27:44 2022
epoch 2 training:  93%|█████████▎| 182848/196351 [26:02:37<1:54:18,  1.97it/s]global step 379200, epoch: 2, batch: 182849, loss: 3.28268, acc: 0.92733, time: Sun Jan 23 16:28:35 2022
epoch 2 training:  93%|█████████▎| 182948/196351 [26:03:29<1:54:21,  1.95it/s]global step 379300, epoch: 2, batch: 182949, loss: 0.34222, acc: 0.92734, time: Sun Jan 23 16:29:26 2022
epoch 2 training:  93%|█████████▎| 183048/196351 [26:04:20<1:56:36,  1.90it/s]global step 379400, epoch: 2, batch: 183049, loss: 0.93651, acc: 0.92733, time: Sun Jan 23 16:30:18 2022
epoch 2 training:  93%|█████████▎| 183148/196351 [26:05:12<1:53:13,  1.94it/s]global step 379500, epoch: 2, batch: 183149, loss: 1.61519, acc: 0.92733, time: Sun Jan 23 16:31:09 2022
epoch 2 training:  93%|█████████▎| 183248/196351 [26:06:03<1:51:09,  1.96it/s]global step 379600, epoch: 2, batch: 183249, loss: 0.44475, acc: 0.92732, time: Sun Jan 23 16:32:01 2022
epoch 2 training:  93%|█████████▎| 183348/196351 [26:06:55<1:53:26,  1.91it/s]global step 379700, epoch: 2, batch: 183349, loss: 0.64942, acc: 0.92732, time: Sun Jan 23 16:32:53 2022
epoch 2 training:  93%|█████████▎| 183448/196351 [26:07:47<1:50:00,  1.95it/s]global step 379800, epoch: 2, batch: 183449, loss: 4.64962, acc: 0.92733, time: Sun Jan 23 16:33:44 2022
epoch 2 training:  93%|█████████▎| 183548/196351 [26:08:38<1:49:58,  1.94it/s]global step 379900, epoch: 2, batch: 183549, loss: 3.07107, acc: 0.92732, time: Sun Jan 23 16:34:36 2022
epoch 2 training:  94%|█████████▎| 183648/196351 [26:09:30<1:49:00,  1.94it/s]global step 380000, epoch: 2, batch: 183649, loss: 0.39015, acc: 0.92730, time: Sun Jan 23 16:35:27 2022
epoch 2 training:  94%|█████████▎| 183748/196351 [26:10:21<1:48:41,  1.93it/s]global step 380100, epoch: 2, batch: 183749, loss: 0.24469, acc: 0.92730, time: Sun Jan 23 16:36:19 2022
epoch 2 training:  94%|█████████▎| 183848/196351 [26:11:13<1:45:49,  1.97it/s]global step 380200, epoch: 2, batch: 183849, loss: 1.93310, acc: 0.92729, time: Sun Jan 23 16:37:10 2022
epoch 2 training:  94%|█████████▎| 183948/196351 [26:12:03<1:44:33,  1.98it/s]global step 380300, epoch: 2, batch: 183949, loss: 2.77867, acc: 0.92730, time: Sun Jan 23 16:38:01 2022
epoch 2 training:  94%|█████████▎| 184048/196351 [26:12:54<1:45:08,  1.95it/s]global step 380400, epoch: 2, batch: 184049, loss: 1.61642, acc: 0.92730, time: Sun Jan 23 16:38:52 2022
epoch 2 training:  94%|█████████▍| 184148/196351 [26:13:45<1:43:41,  1.96it/s]global step 380500, epoch: 2, batch: 184149, loss: 3.96037, acc: 0.92731, time: Sun Jan 23 16:39:42 2022
epoch 2 training:  94%|█████████▍| 184248/196351 [26:14:36<1:43:41,  1.95it/s]global step 380600, epoch: 2, batch: 184249, loss: 1.76273, acc: 0.92731, time: Sun Jan 23 16:40:34 2022
epoch 2 training:  94%|█████████▍| 184348/196351 [26:15:28<1:43:25,  1.93it/s]global step 380700, epoch: 2, batch: 184349, loss: 0.07074, acc: 0.92731, time: Sun Jan 23 16:41:26 2022
epoch 2 training:  94%|█████████▍| 184448/196351 [26:16:19<1:40:44,  1.97it/s]global step 380800, epoch: 2, batch: 184449, loss: 0.58881, acc: 0.92733, time: Sun Jan 23 16:42:17 2022
epoch 2 training:  94%|█████████▍| 184548/196351 [26:17:10<1:39:11,  1.98it/s]global step 380900, epoch: 2, batch: 184549, loss: 3.20486, acc: 0.92732, time: Sun Jan 23 16:43:07 2022
epoch 2 training:  94%|█████████▍| 184648/196351 [26:18:01<1:40:24,  1.94it/s]global step 381000, epoch: 2, batch: 184649, loss: 0.35821, acc: 0.92732, time: Sun Jan 23 16:43:59 2022
epoch 2 training:  94%|█████████▍| 184748/196351 [26:18:52<1:38:27,  1.96it/s]global step 381100, epoch: 2, batch: 184749, loss: 1.01517, acc: 0.92732, time: Sun Jan 23 16:44:50 2022
epoch 2 training:  94%|█████████▍| 184848/196351 [26:19:44<1:37:44,  1.96it/s]global step 381200, epoch: 2, batch: 184849, loss: 3.12538, acc: 0.92730, time: Sun Jan 23 16:45:41 2022
epoch 2 training:  94%|█████████▍| 184948/196351 [26:20:35<1:36:04,  1.98it/s]global step 381300, epoch: 2, batch: 184949, loss: 1.02331, acc: 0.92731, time: Sun Jan 23 16:46:33 2022
epoch 2 training:  94%|█████████▍| 185048/196351 [26:21:26<1:36:38,  1.95it/s]global step 381400, epoch: 2, batch: 185049, loss: 0.20890, acc: 0.92732, time: Sun Jan 23 16:47:24 2022
epoch 2 training:  94%|█████████▍| 185148/196351 [26:22:18<1:35:43,  1.95it/s]global step 381500, epoch: 2, batch: 185149, loss: 2.56505, acc: 0.92734, time: Sun Jan 23 16:48:16 2022
epoch 2 training:  94%|█████████▍| 185248/196351 [26:23:10<1:34:46,  1.95it/s]global step 381600, epoch: 2, batch: 185249, loss: 0.33961, acc: 0.92734, time: Sun Jan 23 16:49:07 2022
epoch 2 training:  94%|█████████▍| 185348/196351 [26:24:01<1:33:22,  1.96it/s]global step 381700, epoch: 2, batch: 185349, loss: 1.42248, acc: 0.92734, time: Sun Jan 23 16:49:58 2022
epoch 2 training:  94%|█████████▍| 185448/196351 [26:24:52<1:32:57,  1.95it/s]global step 381800, epoch: 2, batch: 185449, loss: 1.39522, acc: 0.92735, time: Sun Jan 23 16:50:49 2022
epoch 2 training:  94%|█████████▍| 185548/196351 [26:25:43<1:32:43,  1.94it/s]global step 381900, epoch: 2, batch: 185549, loss: 2.20892, acc: 0.92735, time: Sun Jan 23 16:51:41 2022
epoch 2 training:  95%|█████████▍| 185648/196351 [26:26:35<1:32:26,  1.93it/s]global step 382000, epoch: 2, batch: 185649, loss: 3.88920, acc: 0.92735, time: Sun Jan 23 16:52:32 2022
epoch 2 training:  95%|█████████▍| 185748/196351 [26:27:26<1:31:39,  1.93it/s]global step 382100, epoch: 2, batch: 185749, loss: 2.84444, acc: 0.92735, time: Sun Jan 23 16:53:24 2022
epoch 2 training:  95%|█████████▍| 185848/196351 [26:28:18<1:29:10,  1.96it/s]global step 382200, epoch: 2, batch: 185849, loss: 1.19145, acc: 0.92735, time: Sun Jan 23 16:54:15 2022
epoch 2 training:  95%|█████████▍| 185948/196351 [26:29:09<1:29:14,  1.94it/s]global step 382300, epoch: 2, batch: 185949, loss: 1.13828, acc: 0.92736, time: Sun Jan 23 16:55:07 2022
epoch 2 training:  95%|█████████▍| 186048/196351 [26:30:01<1:29:38,  1.92it/s]global step 382400, epoch: 2, batch: 186049, loss: 1.00905, acc: 0.92736, time: Sun Jan 23 16:55:58 2022
epoch 2 training:  95%|█████████▍| 186148/196351 [26:30:52<1:27:31,  1.94it/s]global step 382500, epoch: 2, batch: 186149, loss: 0.73069, acc: 0.92735, time: Sun Jan 23 16:56:50 2022
epoch 2 training:  95%|█████████▍| 186248/196351 [26:31:44<1:27:03,  1.93it/s]global step 382600, epoch: 2, batch: 186249, loss: 1.54009, acc: 0.92734, time: Sun Jan 23 16:57:41 2022
epoch 2 training:  95%|█████████▍| 186348/196351 [26:32:35<1:24:50,  1.96it/s]global step 382700, epoch: 2, batch: 186349, loss: 2.15136, acc: 0.92733, time: Sun Jan 23 16:58:33 2022
epoch 2 training:  95%|█████████▍| 186448/196351 [26:33:27<1:25:07,  1.94it/s]global step 382800, epoch: 2, batch: 186449, loss: 0.52774, acc: 0.92733, time: Sun Jan 23 16:59:24 2022
epoch 2 training:  95%|█████████▌| 186548/196351 [26:34:18<1:23:57,  1.95it/s]global step 382900, epoch: 2, batch: 186549, loss: 1.65420, acc: 0.92733, time: Sun Jan 23 17:00:16 2022
epoch 2 training:  95%|█████████▌| 186648/196351 [26:35:10<1:24:00,  1.92it/s]global step 383000, epoch: 2, batch: 186649, loss: 0.71072, acc: 0.92734, time: Sun Jan 23 17:01:07 2022
epoch 2 training:  95%|█████████▌| 186748/196351 [26:36:01<1:22:04,  1.95it/s]global step 383100, epoch: 2, batch: 186749, loss: 1.71279, acc: 0.92733, time: Sun Jan 23 17:01:58 2022
epoch 2 training:  95%|█████████▌| 186848/196351 [26:36:52<1:21:28,  1.94it/s]global step 383200, epoch: 2, batch: 186849, loss: 0.52845, acc: 0.92733, time: Sun Jan 23 17:02:49 2022
epoch 2 training:  95%|█████████▌| 186948/196351 [26:37:43<1:19:58,  1.96it/s]global step 383300, epoch: 2, batch: 186949, loss: 2.33402, acc: 0.92733, time: Sun Jan 23 17:03:41 2022
epoch 2 training:  95%|█████████▌| 187048/196351 [26:38:34<1:20:12,  1.93it/s]global step 383400, epoch: 2, batch: 187049, loss: 1.11701, acc: 0.92733, time: Sun Jan 23 17:04:32 2022
epoch 2 training:  95%|█████████▌| 187148/196351 [26:39:26<1:33:04,  1.65it/s]global step 383500, epoch: 2, batch: 187149, loss: 2.34116, acc: 0.92732, time: Sun Jan 23 17:05:23 2022
epoch 2 training:  95%|█████████▌| 187248/196351 [26:40:17<1:17:12,  1.96it/s]global step 383600, epoch: 2, batch: 187249, loss: 2.14919, acc: 0.92731, time: Sun Jan 23 17:06:14 2022
epoch 2 training:  95%|█████████▌| 187348/196351 [26:41:08<1:15:23,  1.99it/s]global step 383700, epoch: 2, batch: 187349, loss: 1.92847, acc: 0.92731, time: Sun Jan 23 17:07:05 2022
epoch 2 training:  95%|█████████▌| 187448/196351 [26:41:59<1:15:59,  1.95it/s]global step 383800, epoch: 2, batch: 187449, loss: 0.88696, acc: 0.92732, time: Sun Jan 23 17:07:56 2022
epoch 2 training:  96%|█████████▌| 187548/196351 [26:42:50<1:17:35,  1.89it/s]global step 383900, epoch: 2, batch: 187549, loss: 3.00663, acc: 0.92731, time: Sun Jan 23 17:08:47 2022
epoch 2 training:  96%|█████████▌| 187648/196351 [26:43:41<1:13:19,  1.98it/s]global step 384000, epoch: 2, batch: 187649, loss: 2.25876, acc: 0.92733, time: Sun Jan 23 17:09:39 2022
epoch 2 training:  96%|█████████▌| 187748/196351 [26:44:33<1:13:34,  1.95it/s]global step 384100, epoch: 2, batch: 187749, loss: 1.75856, acc: 0.92733, time: Sun Jan 23 17:10:30 2022
epoch 2 training:  96%|█████████▌| 187848/196351 [26:45:24<1:12:44,  1.95it/s]global step 384200, epoch: 2, batch: 187849, loss: 2.48732, acc: 0.92733, time: Sun Jan 23 17:11:22 2022
epoch 2 training:  96%|█████████▌| 187948/196351 [26:46:16<1:11:50,  1.95it/s]global step 384300, epoch: 2, batch: 187949, loss: 0.94491, acc: 0.92733, time: Sun Jan 23 17:12:13 2022
epoch 2 training:  96%|█████████▌| 188048/196351 [26:47:07<1:10:58,  1.95it/s]global step 384400, epoch: 2, batch: 188049, loss: 2.44452, acc: 0.92732, time: Sun Jan 23 17:13:05 2022
epoch 2 training:  96%|█████████▌| 188148/196351 [26:47:59<1:10:56,  1.93it/s]global step 384500, epoch: 2, batch: 188149, loss: 3.21681, acc: 0.92733, time: Sun Jan 23 17:13:57 2022
epoch 2 training:  96%|█████████▌| 188248/196351 [26:48:51<1:09:13,  1.95it/s]global step 384600, epoch: 2, batch: 188249, loss: 0.32664, acc: 0.92734, time: Sun Jan 23 17:14:48 2022
epoch 2 training:  96%|█████████▌| 188348/196351 [26:49:42<1:09:06,  1.93it/s]global step 384700, epoch: 2, batch: 188349, loss: 1.40924, acc: 0.92735, time: Sun Jan 23 17:15:39 2022
epoch 2 training:  96%|█████████▌| 188448/196351 [26:50:33<1:07:13,  1.96it/s]global step 384800, epoch: 2, batch: 188449, loss: 1.80379, acc: 0.92735, time: Sun Jan 23 17:16:31 2022
epoch 2 training:  96%|█████████▌| 188548/196351 [26:51:25<1:07:01,  1.94it/s]global step 384900, epoch: 2, batch: 188549, loss: 1.69412, acc: 0.92734, time: Sun Jan 23 17:17:22 2022
epoch 2 training:  96%|█████████▌| 188648/196351 [26:52:16<1:04:24,  1.99it/s]global step 385000, epoch: 2, batch: 188649, loss: 1.04520, acc: 0.92734, time: Sun Jan 23 17:18:13 2022
epoch 2 training:  96%|█████████▌| 188748/196351 [26:53:07<1:04:08,  1.98it/s]global step 385100, epoch: 2, batch: 188749, loss: 1.34442, acc: 0.92734, time: Sun Jan 23 17:19:04 2022
epoch 2 training:  96%|█████████▌| 188848/196351 [26:53:58<1:04:41,  1.93it/s]global step 385200, epoch: 2, batch: 188849, loss: 1.61308, acc: 0.92735, time: Sun Jan 23 17:19:55 2022
epoch 2 training:  96%|█████████▌| 188948/196351 [26:54:49<1:02:43,  1.97it/s]global step 385300, epoch: 2, batch: 188949, loss: 2.25031, acc: 0.92736, time: Sun Jan 23 17:20:46 2022
epoch 2 training:  96%|█████████▋| 189048/196351 [26:55:40<1:02:51,  1.94it/s]global step 385400, epoch: 2, batch: 189049, loss: 0.26572, acc: 0.92737, time: Sun Jan 23 17:21:37 2022
epoch 2 training:  96%|█████████▋| 189148/196351 [26:56:31<1:01:17,  1.96it/s]global step 385500, epoch: 2, batch: 189149, loss: 2.31607, acc: 0.92739, time: Sun Jan 23 17:22:29 2022
epoch 2 training:  96%|█████████▋| 189248/196351 [26:57:22<1:00:43,  1.95it/s]global step 385600, epoch: 2, batch: 189249, loss: 0.25914, acc: 0.92739, time: Sun Jan 23 17:23:20 2022
epoch 2 training:  96%|█████████▋| 189348/196351 [26:58:13<58:16,  2.00it/s]global step 385700, epoch: 2, batch: 189349, loss: 0.78522, acc: 0.92740, time: Sun Jan 23 17:24:11 2022
epoch 2 training:  96%|█████████▋| 189448/196351 [26:59:04<58:30,  1.97it/s]global step 385800, epoch: 2, batch: 189449, loss: 0.84554, acc: 0.92739, time: Sun Jan 23 17:25:02 2022
epoch 2 training:  97%|█████████▋| 189548/196351 [26:59:55<57:52,  1.96it/s]global step 385900, epoch: 2, batch: 189549, loss: 0.61505, acc: 0.92740, time: Sun Jan 23 17:25:53 2022
epoch 2 training:  97%|█████████▋| 189648/196351 [27:00:47<57:52,  1.93it/s]global step 386000, epoch: 2, batch: 189649, loss: 0.88477, acc: 0.92740, time: Sun Jan 23 17:26:44 2022
epoch 2 training:  97%|█████████▋| 189748/196351 [27:01:38<55:51,  1.97it/s]global step 386100, epoch: 2, batch: 189749, loss: 1.83696, acc: 0.92741, time: Sun Jan 23 17:27:36 2022
epoch 2 training:  97%|█████████▋| 189848/196351 [27:02:29<54:50,  1.98it/s]global step 386200, epoch: 2, batch: 189849, loss: 1.84790, acc: 0.92741, time: Sun Jan 23 17:28:27 2022
epoch 2 training:  97%|█████████▋| 189948/196351 [27:03:21<54:18,  1.97it/s]global step 386300, epoch: 2, batch: 189949, loss: 1.47423, acc: 0.92741, time: Sun Jan 23 17:29:18 2022
epoch 2 training:  97%|█████████▋| 190048/196351 [27:04:13<53:38,  1.96it/s]global step 386400, epoch: 2, batch: 190049, loss: 2.37582, acc: 0.92740, time: Sun Jan 23 17:30:10 2022
epoch 2 training:  97%|█████████▋| 190148/196351 [27:05:04<54:09,  1.91it/s]global step 386500, epoch: 2, batch: 190149, loss: 0.61167, acc: 0.92741, time: Sun Jan 23 17:31:01 2022
epoch 2 training:  97%|█████████▋| 190248/196351 [27:05:55<51:10,  1.99it/s]global step 386600, epoch: 2, batch: 190249, loss: 3.76602, acc: 0.92740, time: Sun Jan 23 17:31:52 2022
epoch 2 training:  97%|█████████▋| 190348/196351 [27:06:46<50:33,  1.98it/s]global step 386700, epoch: 2, batch: 190349, loss: 0.38775, acc: 0.92740, time: Sun Jan 23 17:32:43 2022
epoch 2 training:  97%|█████████▋| 190448/196351 [27:07:37<49:40,  1.98it/s]global step 386800, epoch: 2, batch: 190449, loss: 2.05871, acc: 0.92740, time: Sun Jan 23 17:33:34 2022
epoch 2 training:  97%|█████████▋| 190548/196351 [27:08:28<48:58,  1.97it/s]global step 386900, epoch: 2, batch: 190549, loss: 0.91648, acc: 0.92740, time: Sun Jan 23 17:34:25 2022
epoch 2 training:  97%|█████████▋| 190648/196351 [27:09:19<48:53,  1.94it/s]global step 387000, epoch: 2, batch: 190649, loss: 0.43075, acc: 0.92739, time: Sun Jan 23 17:35:16 2022
epoch 2 training:  97%|█████████▋| 190748/196351 [27:10:10<48:10,  1.94it/s]global step 387100, epoch: 2, batch: 190749, loss: 0.14901, acc: 0.92740, time: Sun Jan 23 17:36:08 2022
epoch 2 training:  97%|█████████▋| 190848/196351 [27:11:01<47:18,  1.94it/s]global step 387200, epoch: 2, batch: 190849, loss: 2.38880, acc: 0.92741, time: Sun Jan 23 17:36:59 2022
epoch 2 training:  97%|█████████▋| 190948/196351 [27:11:52<45:20,  1.99it/s]global step 387300, epoch: 2, batch: 190949, loss: 0.40647, acc: 0.92741, time: Sun Jan 23 17:37:50 2022
epoch 2 training:  97%|█████████▋| 191048/196351 [27:12:44<45:05,  1.96it/s]global step 387400, epoch: 2, batch: 191049, loss: 0.76752, acc: 0.92741, time: Sun Jan 23 17:38:41 2022
epoch 2 training:  97%|█████████▋| 191148/196351 [27:13:36<45:01,  1.93it/s]global step 387500, epoch: 2, batch: 191149, loss: 1.86418, acc: 0.92739, time: Sun Jan 23 17:39:33 2022
epoch 2 training:  97%|█████████▋| 191248/196351 [27:14:27<43:46,  1.94it/s]global step 387600, epoch: 2, batch: 191249, loss: 4.17181, acc: 0.92740, time: Sun Jan 23 17:40:24 2022
epoch 2 training:  97%|█████████▋| 191348/196351 [27:15:18<42:30,  1.96it/s]global step 387700, epoch: 2, batch: 191349, loss: 0.42209, acc: 0.92741, time: Sun Jan 23 17:41:16 2022
epoch 2 training:  98%|█████████▊| 191448/196351 [27:16:10<41:59,  1.95it/s]global step 387800, epoch: 2, batch: 191449, loss: 1.06116, acc: 0.92742, time: Sun Jan 23 17:42:08 2022
epoch 2 training:  98%|█████████▊| 191548/196351 [27:17:02<41:25,  1.93it/s]global step 387900, epoch: 2, batch: 191549, loss: 0.33997, acc: 0.92742, time: Sun Jan 23 17:42:59 2022
epoch 2 training:  98%|█████████▊| 191648/196351 [27:17:53<40:19,  1.94it/s]global step 388000, epoch: 2, batch: 191649, loss: 2.60915, acc: 0.92742, time: Sun Jan 23 17:43:51 2022
epoch 2 training:  98%|█████████▊| 191748/196351 [27:18:44<39:22,  1.95it/s]global step 388100, epoch: 2, batch: 191749, loss: 2.13333, acc: 0.92742, time: Sun Jan 23 17:44:42 2022
epoch 2 training:  98%|█████████▊| 191848/196351 [27:19:35<38:27,  1.95it/s]global step 388200, epoch: 2, batch: 191849, loss: 1.18813, acc: 0.92741, time: Sun Jan 23 17:45:33 2022
epoch 2 training:  98%|█████████▊| 191948/196351 [27:20:26<37:27,  1.96it/s]global step 388300, epoch: 2, batch: 191949, loss: 2.77662, acc: 0.92740, time: Sun Jan 23 17:46:24 2022
epoch 2 training:  98%|█████████▊| 192048/196351 [27:21:17<36:03,  1.99it/s]global step 388400, epoch: 2, batch: 192049, loss: 1.61952, acc: 0.92740, time: Sun Jan 23 17:47:15 2022
epoch 2 training:  98%|█████████▊| 192148/196351 [27:22:08<36:17,  1.93it/s]global step 388500, epoch: 2, batch: 192149, loss: 4.01006, acc: 0.92740, time: Sun Jan 23 17:48:06 2022
epoch 2 training:  98%|█████████▊| 192248/196351 [27:23:00<34:46,  1.97it/s]global step 388600, epoch: 2, batch: 192249, loss: 2.46184, acc: 0.92740, time: Sun Jan 23 17:48:57 2022
epoch 2 training:  98%|█████████▊| 192348/196351 [27:23:51<33:49,  1.97it/s]global step 388700, epoch: 2, batch: 192349, loss: 4.13187, acc: 0.92739, time: Sun Jan 23 17:49:49 2022
epoch 2 training:  98%|█████████▊| 192448/196351 [27:24:43<33:24,  1.95it/s]global step 388800, epoch: 2, batch: 192449, loss: 0.81786, acc: 0.92740, time: Sun Jan 23 17:50:40 2022
epoch 2 training:  98%|█████████▊| 192548/196351 [27:25:34<32:30,  1.95it/s]global step 388900, epoch: 2, batch: 192549, loss: 3.80853, acc: 0.92740, time: Sun Jan 23 17:51:31 2022
epoch 2 training:  98%|█████████▊| 192648/196351 [27:26:25<31:25,  1.96it/s]global step 389000, epoch: 2, batch: 192649, loss: 1.15276, acc: 0.92740, time: Sun Jan 23 17:52:22 2022
epoch 2 training:  98%|█████████▊| 192748/196351 [27:27:16<30:57,  1.94it/s]global step 389100, epoch: 2, batch: 192749, loss: 0.61672, acc: 0.92742, time: Sun Jan 23 17:53:14 2022
epoch 2 training:  98%|█████████▊| 192848/196351 [27:28:08<29:45,  1.96it/s]global step 389200, epoch: 2, batch: 192849, loss: 0.95559, acc: 0.92742, time: Sun Jan 23 17:54:05 2022
epoch 2 training:  98%|█████████▊| 192948/196351 [27:28:59<29:21,  1.93it/s]global step 389300, epoch: 2, batch: 192949, loss: 0.85311, acc: 0.92741, time: Sun Jan 23 17:54:57 2022
epoch 2 training:  98%|█████████▊| 193048/196351 [27:29:50<28:04,  1.96it/s]global step 389400, epoch: 2, batch: 193049, loss: 0.78111, acc: 0.92741, time: Sun Jan 23 17:55:48 2022
epoch 2 training:  98%|█████████▊| 193148/196351 [27:30:41<27:24,  1.95it/s]global step 389500, epoch: 2, batch: 193149, loss: 1.21358, acc: 0.92740, time: Sun Jan 23 17:56:39 2022
epoch 2 training:  98%|█████████▊| 193248/196351 [27:31:32<26:46,  1.93it/s]global step 389600, epoch: 2, batch: 193249, loss: 1.76145, acc: 0.92740, time: Sun Jan 23 17:57:30 2022
epoch 2 training:  98%|█████████▊| 193348/196351 [27:32:23<25:19,  1.98it/s]global step 389700, epoch: 2, batch: 193349, loss: 0.14547, acc: 0.92740, time: Sun Jan 23 17:58:20 2022
epoch 2 training:  99%|█████████▊| 193448/196351 [27:33:14<24:29,  1.98it/s]global step 389800, epoch: 2, batch: 193449, loss: 1.04851, acc: 0.92739, time: Sun Jan 23 17:59:12 2022
epoch 2 training:  99%|█████████▊| 193548/196351 [27:34:06<24:00,  1.95it/s]global step 389900, epoch: 2, batch: 193549, loss: 2.71991, acc: 0.92738, time: Sun Jan 23 18:00:03 2022
epoch 2 training:  99%|█████████▊| 193648/196351 [27:34:57<22:54,  1.97it/s]global step 390000, epoch: 2, batch: 193649, loss: 0.36071, acc: 0.92738, time: Sun Jan 23 18:00:54 2022
epoch 2 training:  99%|█████████▊| 193748/196351 [27:35:48<22:23,  1.94it/s]global step 390100, epoch: 2, batch: 193749, loss: 1.58682, acc: 0.92737, time: Sun Jan 23 18:01:46 2022
epoch 2 training:  99%|█████████▊| 193848/196351 [27:36:40<21:39,  1.93it/s]global step 390200, epoch: 2, batch: 193849, loss: 2.44975, acc: 0.92737, time: Sun Jan 23 18:02:37 2022
epoch 2 training:  99%|█████████▉| 193948/196351 [27:37:31<20:39,  1.94it/s]global step 390300, epoch: 2, batch: 193949, loss: 1.53837, acc: 0.92738, time: Sun Jan 23 18:03:28 2022
epoch 2 training:  99%|█████████▉| 194048/196351 [27:38:22<19:16,  1.99it/s]global step 390400, epoch: 2, batch: 194049, loss: 0.35969, acc: 0.92739, time: Sun Jan 23 18:04:19 2022
epoch 2 training:  99%|█████████▉| 194148/196351 [27:39:13<18:43,  1.96it/s]global step 390500, epoch: 2, batch: 194149, loss: 1.88626, acc: 0.92740, time: Sun Jan 23 18:05:10 2022
epoch 2 training:  99%|█████████▉| 194248/196351 [27:40:03<17:47,  1.97it/s]global step 390600, epoch: 2, batch: 194249, loss: 3.48367, acc: 0.92739, time: Sun Jan 23 18:06:01 2022
epoch 2 training:  99%|█████████▉| 194348/196351 [27:40:54<16:56,  1.97it/s]global step 390700, epoch: 2, batch: 194349, loss: 1.51948, acc: 0.92740, time: Sun Jan 23 18:06:51 2022
epoch 2 training:  99%|█████████▉| 194448/196351 [27:41:45<15:48,  2.01it/s]global step 390800, epoch: 2, batch: 194449, loss: 0.36547, acc: 0.92742, time: Sun Jan 23 18:07:42 2022
epoch 2 training:  99%|█████████▉| 194548/196351 [27:42:35<15:05,  1.99it/s]global step 390900, epoch: 2, batch: 194549, loss: 2.21978, acc: 0.92742, time: Sun Jan 23 18:08:33 2022
epoch 2 training:  99%|█████████▉| 194648/196351 [27:43:26<14:22,  1.97it/s]global step 391000, epoch: 2, batch: 194649, loss: 2.45998, acc: 0.92742, time: Sun Jan 23 18:09:23 2022
epoch 2 training:  99%|█████████▉| 194748/196351 [27:44:17<13:34,  1.97it/s]global step 391100, epoch: 2, batch: 194749, loss: 0.76790, acc: 0.92741, time: Sun Jan 23 18:10:14 2022
epoch 2 training:  99%|█████████▉| 194848/196351 [27:45:08<12:38,  1.98it/s]global step 391200, epoch: 2, batch: 194849, loss: 0.40210, acc: 0.92742, time: Sun Jan 23 18:11:05 2022
epoch 2 training:  99%|█████████▉| 194948/196351 [27:45:59<11:59,  1.95it/s]global step 391300, epoch: 2, batch: 194949, loss: 1.85710, acc: 0.92742, time: Sun Jan 23 18:11:57 2022
epoch 2 training:  99%|█████████▉| 195048/196351 [27:46:50<11:02,  1.97it/s]global step 391400, epoch: 2, batch: 195049, loss: 0.84395, acc: 0.92743, time: Sun Jan 23 18:12:48 2022
epoch 2 training:  99%|█████████▉| 195148/196351 [27:47:41<10:11,  1.97it/s]global step 391500, epoch: 2, batch: 195149, loss: 0.36382, acc: 0.92743, time: Sun Jan 23 18:13:39 2022
epoch 2 training:  99%|█████████▉| 195248/196351 [27:48:33<09:20,  1.97it/s]global step 391600, epoch: 2, batch: 195249, loss: 1.78421, acc: 0.92742, time: Sun Jan 23 18:14:30 2022
epoch 2 training:  99%|█████████▉| 195348/196351 [27:49:24<08:34,  1.95it/s]global step 391700, epoch: 2, batch: 195349, loss: 0.14805, acc: 0.92743, time: Sun Jan 23 18:15:21 2022
epoch 2 training: 100%|█████████▉| 195448/196351 [27:50:15<07:38,  1.97it/s]global step 391800, epoch: 2, batch: 195449, loss: 4.54752, acc: 0.92742, time: Sun Jan 23 18:16:12 2022
epoch 2 training: 100%|█████████▉| 195548/196351 [27:51:05<06:47,  1.97it/s]global step 391900, epoch: 2, batch: 195549, loss: 0.32619, acc: 0.92742, time: Sun Jan 23 18:17:03 2022
epoch 2 training: 100%|█████████▉| 195648/196351 [27:51:56<06:03,  1.93it/s]global step 392000, epoch: 2, batch: 195649, loss: 1.75913, acc: 0.92744, time: Sun Jan 23 18:17:54 2022
epoch 2 training: 100%|█████████▉| 195748/196351 [27:52:47<05:10,  1.94it/s]global step 392100, epoch: 2, batch: 195749, loss: 1.06779, acc: 0.92744, time: Sun Jan 23 18:18:45 2022
epoch 2 training: 100%|█████████▉| 195848/196351 [27:53:38<04:13,  1.98it/s]global step 392200, epoch: 2, batch: 195849, loss: 4.07623, acc: 0.92744, time: Sun Jan 23 18:19:36 2022
epoch 2 training: 100%|█████████▉| 195948/196351 [27:54:29<03:21,  2.00it/s]global step 392300, epoch: 2, batch: 195949, loss: 0.63791, acc: 0.92744, time: Sun Jan 23 18:20:27 2022
epoch 2 training: 100%|█████████▉| 196048/196351 [27:55:20<02:34,  1.96it/s]global step 392400, epoch: 2, batch: 196049, loss: 0.92976, acc: 0.92745, time: Sun Jan 23 18:21:17 2022
epoch 2 training: 100%|█████████▉| 196148/196351 [27:56:11<01:42,  1.98it/s]global step 392500, epoch: 2, batch: 196149, loss: 2.27806, acc: 0.92746, time: Sun Jan 23 18:22:08 2022
epoch 2 training: 100%|█████████▉| 196248/196351 [27:57:01<00:52,  1.96it/s]global step 392600, epoch: 2, batch: 196249, loss: 2.31345, acc: 0.92746, time: Sun Jan 23 18:22:59 2022
epoch 2 training: 100%|█████████▉| 196348/196351 [27:57:52<00:01,  1.97it/s]global step 392700, epoch: 2, batch: 196349, loss: 1.02984, acc: 0.92747, time: Sun Jan 23 18:23:49 2022
epoch 2 training: 100%|██████████| 196351/196351 [27:57:54<00:00,  1.95it/s]
epoch 2 dev: 100%|██████████| 307/307 [01:03<00:00,  4.85it/s]
epoch 3 training:   0%|          | 0/196351 [00:00<?, ?it/s]epoch: 2, dev accuracy: 0.91044, time: Sun Jan 23 18:24:54 2022
epoch 3 training:   0%|          | 97/196351 [00:49<27:23:15,  1.99it/s]global step 392800, epoch: 3, batch: 98, loss: 1.14920, acc: 0.96939, time: Sun Jan 23 18:25:43 2022
epoch 3 training:   0%|          | 197/196351 [01:39<27:25:26,  1.99it/s]global step 392900, epoch: 3, batch: 198, loss: 1.28677, acc: 0.95707, time: Sun Jan 23 18:26:34 2022
epoch 3 training:   0%|          | 297/196351 [02:30<28:02:47,  1.94it/s]global step 393000, epoch: 3, batch: 298, loss: 1.16980, acc: 0.96477, time: Sun Jan 23 18:27:25 2022
epoch 3 training:   0%|          | 397/196351 [03:21<27:14:34,  2.00it/s]global step 393100, epoch: 3, batch: 398, loss: 0.99210, acc: 0.96357, time: Sun Jan 23 18:28:16 2022
epoch 3 training:   0%|          | 497/196351 [04:12<27:17:00,  1.99it/s]global step 393200, epoch: 3, batch: 498, loss: 0.85881, acc: 0.96084, time: Sun Jan 23 18:29:07 2022
epoch 3 training:   0%|          | 597/196351 [05:03<27:42:03,  1.96it/s]global step 393300, epoch: 3, batch: 598, loss: 0.38020, acc: 0.96237, time: Sun Jan 23 18:29:58 2022
epoch 3 training:   0%|          | 697/196351 [05:54<27:21:39,  1.99it/s]global step 393400, epoch: 3, batch: 698, loss: 0.43752, acc: 0.95989, time: Sun Jan 23 18:30:49 2022
epoch 3 training:   0%|          | 797/196351 [06:45<27:34:14,  1.97it/s]global step 393500, epoch: 3, batch: 798, loss: 0.90658, acc: 0.96115, time: Sun Jan 23 18:31:40 2022
epoch 3 training:   0%|          | 897/196351 [07:36<27:39:53,  1.96it/s]global step 393600, epoch: 3, batch: 898, loss: 0.22574, acc: 0.95880, time: Sun Jan 23 18:32:31 2022
epoch 3 training:   1%|          | 997/196351 [08:27<27:45:16,  1.96it/s]global step 393700, epoch: 3, batch: 998, loss: 1.09752, acc: 0.95641, time: Sun Jan 23 18:33:22 2022
epoch 3 training:   1%|          | 1097/196351 [09:18<27:27:28,  1.98it/s]global step 393800, epoch: 3, batch: 1098, loss: 1.40476, acc: 0.95537, time: Sun Jan 23 18:34:13 2022
epoch 3 training:   1%|          | 1197/196351 [10:10<27:32:24,  1.97it/s]global step 393900, epoch: 3, batch: 1198, loss: 1.25838, acc: 0.95701, time: Sun Jan 23 18:35:04 2022
epoch 3 training:   1%|          | 1297/196351 [11:00<27:29:21,  1.97it/s]global step 394000, epoch: 3, batch: 1298, loss: 0.51468, acc: 0.95609, time: Sun Jan 23 18:35:55 2022
epoch 3 training:   1%|          | 1397/196351 [11:51<27:12:23,  1.99it/s]global step 394100, epoch: 3, batch: 1398, loss: 0.97826, acc: 0.95672, time: Sun Jan 23 18:36:46 2022
epoch 3 training:   1%|          | 1497/196351 [12:42<27:20:16,  1.98it/s]global step 394200, epoch: 3, batch: 1498, loss: 0.66312, acc: 0.95661, time: Sun Jan 23 18:37:37 2022
epoch 3 training:   1%|          | 1597/196351 [13:33<27:28:36,  1.97it/s]global step 394300, epoch: 3, batch: 1598, loss: 0.43429, acc: 0.95557, time: Sun Jan 23 18:38:28 2022
epoch 3 training:   1%|          | 1697/196351 [14:24<27:58:24,  1.93it/s]global step 394400, epoch: 3, batch: 1698, loss: 1.40823, acc: 0.95495, time: Sun Jan 23 18:39:18 2022
epoch 3 training:   1%|          | 1797/196351 [15:14<27:31:04,  1.96it/s]global step 394500, epoch: 3, batch: 1798, loss: 1.23521, acc: 0.95551, time: Sun Jan 23 18:40:09 2022
epoch 3 training:   1%|          | 1897/196351 [16:05<27:33:26,  1.96it/s]global step 394600, epoch: 3, batch: 1898, loss: 1.52157, acc: 0.95548, time: Sun Jan 23 18:41:00 2022
epoch 3 training:   1%|          | 1997/196351 [16:56<27:57:29,  1.93it/s]global step 394700, epoch: 3, batch: 1998, loss: 1.94158, acc: 0.95521, time: Sun Jan 23 18:41:51 2022
epoch 3 training:   1%|          | 2097/196351 [17:47<27:16:50,  1.98it/s]global step 394800, epoch: 3, batch: 2098, loss: 2.09555, acc: 0.95496, time: Sun Jan 23 18:42:42 2022
epoch 3 training:   1%|          | 2197/196351 [18:38<27:10:27,  1.98it/s]global step 394900, epoch: 3, batch: 2198, loss: 0.41810, acc: 0.95450, time: Sun Jan 23 18:43:33 2022
epoch 3 training:   1%|          | 2297/196351 [19:29<27:43:00,  1.94it/s]global step 395000, epoch: 3, batch: 2298, loss: 2.30695, acc: 0.95431, time: Sun Jan 23 18:44:24 2022
epoch 3 training:   1%|          | 2397/196351 [20:20<27:17:16,  1.97it/s]global step 395100, epoch: 3, batch: 2398, loss: 2.94445, acc: 0.95413, time: Sun Jan 23 18:45:15 2022
epoch 3 training:   1%|▏         | 2497/196351 [21:11<27:08:52,  1.98it/s]global step 395200, epoch: 3, batch: 2498, loss: 2.10382, acc: 0.95336, time: Sun Jan 23 18:46:06 2022
epoch 3 training:   1%|▏         | 2597/196351 [22:02<27:13:09,  1.98it/s]global step 395300, epoch: 3, batch: 2598, loss: 1.10361, acc: 0.95362, time: Sun Jan 23 18:46:57 2022
epoch 3 training:   1%|▏         | 2697/196351 [22:53<27:28:40,  1.96it/s]global step 395400, epoch: 3, batch: 2698, loss: 0.11820, acc: 0.95330, time: Sun Jan 23 18:47:48 2022
epoch 3 training:   1%|▏         | 2797/196351 [23:44<27:20:30,  1.97it/s]global step 395500, epoch: 3, batch: 2798, loss: 2.90647, acc: 0.95264, time: Sun Jan 23 18:48:38 2022
epoch 3 training:   1%|▏         | 2897/196351 [24:35<27:21:54,  1.96it/s]global step 395600, epoch: 3, batch: 2898, loss: 2.09719, acc: 0.95238, time: Sun Jan 23 18:49:29 2022
epoch 3 training:   2%|▏         | 2997/196351 [25:25<27:20:05,  1.96it/s]global step 395700, epoch: 3, batch: 2998, loss: 0.58144, acc: 0.95297, time: Sun Jan 23 18:50:20 2022
epoch 3 training:   2%|▏         | 3097/196351 [26:16<26:42:55,  2.01it/s]global step 395800, epoch: 3, batch: 3098, loss: 0.30488, acc: 0.95320, time: Sun Jan 23 18:51:11 2022
epoch 3 training:   2%|▏         | 3197/196351 [27:07<26:59:11,  1.99it/s]global step 395900, epoch: 3, batch: 3198, loss: 1.11103, acc: 0.95310, time: Sun Jan 23 18:52:01 2022
epoch 3 training:   2%|▏         | 3297/196351 [27:58<27:30:47,  1.95it/s]global step 396000, epoch: 3, batch: 3298, loss: 2.34400, acc: 0.95285, time: Sun Jan 23 18:52:52 2022
epoch 3 training:   2%|▏         | 3397/196351 [28:48<27:08:39,  1.97it/s]global step 396100, epoch: 3, batch: 3398, loss: 1.62232, acc: 0.95321, time: Sun Jan 23 18:53:43 2022
epoch 3 training:   2%|▏         | 3497/196351 [29:39<27:02:26,  1.98it/s]global step 396200, epoch: 3, batch: 3498, loss: 0.71352, acc: 0.95254, time: Sun Jan 23 18:54:34 2022
epoch 3 training:   2%|▏         | 3597/196351 [30:30<26:48:17,  2.00it/s]global step 396300, epoch: 3, batch: 3598, loss: 2.46439, acc: 0.95289, time: Sun Jan 23 18:55:25 2022
epoch 3 training:   2%|▏         | 3697/196351 [31:21<27:45:49,  1.93it/s]global step 396400, epoch: 3, batch: 3698, loss: 2.10899, acc: 0.95349, time: Sun Jan 23 18:56:16 2022
epoch 3 training:   2%|▏         | 3797/196351 [32:12<26:57:09,  1.98it/s]global step 396500, epoch: 3, batch: 3798, loss: 0.46388, acc: 0.95379, time: Sun Jan 23 18:57:07 2022
epoch 3 training:   2%|▏         | 3897/196351 [33:03<27:07:31,  1.97it/s]global step 396600, epoch: 3, batch: 3898, loss: 2.51719, acc: 0.95408, time: Sun Jan 23 18:57:58 2022
epoch 3 training:   2%|▏         | 3997/196351 [33:54<27:45:08,  1.93it/s]global step 396700, epoch: 3, batch: 3998, loss: 0.84856, acc: 0.95410, time: Sun Jan 23 18:58:48 2022
epoch 3 training:   2%|▏         | 4097/196351 [34:45<26:57:40,  1.98it/s]global step 396800, epoch: 3, batch: 4098, loss: 1.06972, acc: 0.95425, time: Sun Jan 23 18:59:39 2022
epoch 3 training:   2%|▏         | 4197/196351 [35:36<27:22:03,  1.95it/s]global step 396900, epoch: 3, batch: 4198, loss: 0.23145, acc: 0.95403, time: Sun Jan 23 19:00:31 2022
epoch 3 training:   2%|▏         | 4297/196351 [36:27<27:13:45,  1.96it/s]global step 397000, epoch: 3, batch: 4298, loss: 1.06556, acc: 0.95405, time: Sun Jan 23 19:01:22 2022
epoch 3 training:   2%|▏         | 4397/196351 [37:18<27:17:24,  1.95it/s]global step 397100, epoch: 3, batch: 4398, loss: 0.19611, acc: 0.95452, time: Sun Jan 23 19:02:13 2022
epoch 3 training:   2%|▏         | 4497/196351 [38:09<27:09:37,  1.96it/s]global step 397200, epoch: 3, batch: 4498, loss: 0.16900, acc: 0.95420, time: Sun Jan 23 19:03:04 2022
epoch 3 training:   2%|▏         | 4597/196351 [39:01<27:30:36,  1.94it/s]global step 397300, epoch: 3, batch: 4598, loss: 0.42103, acc: 0.95422, time: Sun Jan 23 19:03:56 2022
epoch 3 training:   2%|▏         | 4697/196351 [39:52<27:14:37,  1.95it/s]global step 397400, epoch: 3, batch: 4698, loss: 1.70354, acc: 0.95477, time: Sun Jan 23 19:04:46 2022
epoch 3 training:   2%|▏         | 4797/196351 [40:42<27:06:23,  1.96it/s]global step 397500, epoch: 3, batch: 4798, loss: 0.85840, acc: 0.95540, time: Sun Jan 23 19:05:37 2022
epoch 3 training:   2%|▏         | 4897/196351 [41:33<27:07:53,  1.96it/s]global step 397600, epoch: 3, batch: 4898, loss: 0.88445, acc: 0.95478, time: Sun Jan 23 19:06:28 2022
epoch 3 training:   3%|▎         | 4997/196351 [42:24<26:55:19,  1.97it/s]global step 397700, epoch: 3, batch: 4998, loss: 0.69445, acc: 0.95468, time: Sun Jan 23 19:07:19 2022
epoch 3 training:   3%|▎         | 5097/196351 [43:15<26:44:09,  1.99it/s]global step 397800, epoch: 3, batch: 5098, loss: 1.93911, acc: 0.95449, time: Sun Jan 23 19:08:09 2022
epoch 3 training:   3%|▎         | 5197/196351 [44:05<26:41:27,  1.99it/s]global step 397900, epoch: 3, batch: 5198, loss: 2.83605, acc: 0.95450, time: Sun Jan 23 19:09:00 2022
epoch 3 training:   3%|▎         | 5297/196351 [44:56<26:43:42,  1.99it/s]global step 398000, epoch: 3, batch: 5298, loss: 1.84034, acc: 0.95413, time: Sun Jan 23 19:09:51 2022
epoch 3 training:   3%|▎         | 5397/196351 [45:47<27:00:37,  1.96it/s]global step 398100, epoch: 3, batch: 5398, loss: 1.12336, acc: 0.95424, time: Sun Jan 23 19:10:41 2022
epoch 3 training:   3%|▎         | 5497/196351 [46:37<26:46:25,  1.98it/s]global step 398200, epoch: 3, batch: 5498, loss: 0.56172, acc: 0.95471, time: Sun Jan 23 19:11:32 2022
epoch 3 training:   3%|▎         | 5597/196351 [47:28<26:58:36,  1.96it/s]global step 398300, epoch: 3, batch: 5598, loss: 2.72274, acc: 0.95472, time: Sun Jan 23 19:12:23 2022
epoch 3 training:   3%|▎         | 5697/196351 [48:19<26:42:34,  1.98it/s]global step 398400, epoch: 3, batch: 5698, loss: 1.10729, acc: 0.95507, time: Sun Jan 23 19:13:14 2022
epoch 3 training:   3%|▎         | 5797/196351 [49:10<26:24:39,  2.00it/s]global step 398500, epoch: 3, batch: 5798, loss: 2.76697, acc: 0.95490, time: Sun Jan 23 19:14:05 2022
epoch 3 training:   3%|▎         | 5897/196351 [50:00<26:56:00,  1.96it/s]global step 398600, epoch: 3, batch: 5898, loss: 0.85663, acc: 0.95456, time: Sun Jan 23 19:14:55 2022
epoch 3 training:   3%|▎         | 5997/196351 [50:51<26:42:09,  1.98it/s]global step 398700, epoch: 3, batch: 5998, loss: 0.83922, acc: 0.95448, time: Sun Jan 23 19:15:46 2022
epoch 3 training:   3%|▎         | 6097/196351 [51:42<26:43:20,  1.98it/s]global step 398800, epoch: 3, batch: 6098, loss: 3.28363, acc: 0.95458, time: Sun Jan 23 19:16:37 2022
epoch 3 training:   3%|▎         | 6197/196351 [52:32<26:48:21,  1.97it/s]global step 398900, epoch: 3, batch: 6198, loss: 0.91463, acc: 0.95474, time: Sun Jan 23 19:17:27 2022
epoch 3 training:   3%|▎         | 6297/196351 [53:23<26:39:29,  1.98it/s]global step 399000, epoch: 3, batch: 6298, loss: 2.50312, acc: 0.95475, time: Sun Jan 23 19:18:18 2022
epoch 3 training:   3%|▎         | 6397/196351 [54:14<26:59:37,  1.95it/s]global step 399100, epoch: 3, batch: 6398, loss: 0.38158, acc: 0.95491, time: Sun Jan 23 19:19:09 2022
epoch 3 training:   3%|▎         | 6497/196351 [55:05<26:44:49,  1.97it/s]global step 399200, epoch: 3, batch: 6498, loss: 0.77967, acc: 0.95514, time: Sun Jan 23 19:20:00 2022
epoch 3 training:   3%|▎         | 6597/196351 [55:55<26:32:37,  1.99it/s]global step 399300, epoch: 3, batch: 6598, loss: 2.46362, acc: 0.95514, time: Sun Jan 23 19:20:50 2022
epoch 3 training:   3%|▎         | 6697/196351 [56:46<26:29:02,  1.99it/s]global step 399400, epoch: 3, batch: 6698, loss: 0.65752, acc: 0.95521, time: Sun Jan 23 19:21:41 2022
epoch 3 training:   3%|▎         | 6797/196351 [57:37<26:53:18,  1.96it/s]global step 399500, epoch: 3, batch: 6798, loss: 0.49904, acc: 0.95506, time: Sun Jan 23 19:22:32 2022
epoch 3 training:   4%|▎         | 6897/196351 [58:28<26:30:30,  1.99it/s]global step 399600, epoch: 3, batch: 6898, loss: 1.26943, acc: 0.95484, time: Sun Jan 23 19:23:22 2022
epoch 3 training:   4%|▎         | 6997/196351 [59:18<26:50:13,  1.96it/s]global step 399700, epoch: 3, batch: 6998, loss: 1.28851, acc: 0.95506, time: Sun Jan 23 19:24:13 2022
epoch 3 training:   4%|▎         | 7097/196351 [1:00:09<27:06:38,  1.94it/s]global step 399800, epoch: 3, batch: 7098, loss: 0.99446, acc: 0.95513, time: Sun Jan 23 19:25:04 2022
epoch 3 training:   4%|▎         | 7197/196351 [1:01:01<26:47:47,  1.96it/s]global step 399900, epoch: 3, batch: 7198, loss: 0.53211, acc: 0.95520, time: Sun Jan 23 19:25:55 2022
epoch 3 training:   4%|▎         | 7297/196351 [1:01:52<26:43:41,  1.96it/s]global step 400000, epoch: 3, batch: 7298, loss: 0.85059, acc: 0.95526, time: Sun Jan 23 19:26:47 2022
epoch 3 training:   4%|▍         | 7397/196351 [1:02:43<26:42:05,  1.97it/s]global step 400100, epoch: 3, batch: 7398, loss: 1.89437, acc: 0.95506, time: Sun Jan 23 19:27:38 2022
epoch 3 training:   4%|▍         | 7497/196351 [1:03:34<26:42:08,  1.96it/s]global step 400200, epoch: 3, batch: 7498, loss: 1.36638, acc: 0.95519, time: Sun Jan 23 19:28:29 2022
epoch 3 training:   4%|▍         | 7597/196351 [1:04:25<26:47:51,  1.96it/s]global step 400300, epoch: 3, batch: 7598, loss: 1.00224, acc: 0.95532, time: Sun Jan 23 19:29:20 2022
epoch 3 training:   4%|▍         | 7697/196351 [1:05:16<26:13:28,  2.00it/s]global step 400400, epoch: 3, batch: 7698, loss: 1.37357, acc: 0.95512, time: Sun Jan 23 19:30:11 2022
epoch 3 training:   4%|▍         | 7797/196351 [1:06:07<26:33:28,  1.97it/s]global step 400500, epoch: 3, batch: 7798, loss: 0.11359, acc: 0.95505, time: Sun Jan 23 19:31:02 2022
epoch 3 training:   4%|▍         | 7897/196351 [1:06:58<26:15:04,  1.99it/s]global step 400600, epoch: 3, batch: 7898, loss: 2.13346, acc: 0.95537, time: Sun Jan 23 19:31:53 2022
epoch 3 training:   4%|▍         | 7997/196351 [1:07:49<26:40:56,  1.96it/s]global step 400700, epoch: 3, batch: 7998, loss: 2.34064, acc: 0.95543, time: Sun Jan 23 19:32:44 2022
epoch 3 training:   4%|▍         | 8097/196351 [1:08:40<26:33:39,  1.97it/s]global step 400800, epoch: 3, batch: 8098, loss: 0.26222, acc: 0.95511, time: Sun Jan 23 19:33:35 2022
epoch 3 training:   4%|▍         | 8197/196351 [1:09:31<26:24:03,  1.98it/s]global step 400900, epoch: 3, batch: 8198, loss: 2.21283, acc: 0.95499, time: Sun Jan 23 19:34:26 2022
epoch 3 training:   4%|▍         | 8297/196351 [1:10:22<26:29:02,  1.97it/s]global step 401000, epoch: 3, batch: 8298, loss: 0.78148, acc: 0.95499, time: Sun Jan 23 19:35:16 2022
epoch 3 training:   4%|▍         | 8397/196351 [1:11:13<26:29:51,  1.97it/s]global step 401100, epoch: 3, batch: 8398, loss: 0.53773, acc: 0.95511, time: Sun Jan 23 19:36:08 2022
epoch 3 training:   4%|▍         | 8497/196351 [1:12:04<26:10:38,  1.99it/s]global step 401200, epoch: 3, batch: 8498, loss: 2.47617, acc: 0.95540, time: Sun Jan 23 19:36:59 2022
epoch 3 training:   4%|▍         | 8597/196351 [1:12:55<26:23:41,  1.98it/s]global step 401300, epoch: 3, batch: 8598, loss: 2.28963, acc: 0.95551, time: Sun Jan 23 19:37:49 2022
epoch 3 training:   4%|▍         | 8697/196351 [1:13:45<26:48:14,  1.94it/s]global step 401400, epoch: 3, batch: 8698, loss: 0.97800, acc: 0.95551, time: Sun Jan 23 19:38:40 2022
epoch 3 training:   4%|▍         | 8797/196351 [1:14:36<26:50:43,  1.94it/s]global step 401500, epoch: 3, batch: 8798, loss: 0.47196, acc: 0.95561, time: Sun Jan 23 19:39:31 2022
epoch 3 training:   5%|▍         | 8897/196351 [1:15:28<26:51:18,  1.94it/s]global step 401600, epoch: 3, batch: 8898, loss: 0.24502, acc: 0.95555, time: Sun Jan 23 19:40:23 2022
epoch 3 training:   5%|▍         | 8997/196351 [1:16:19<26:29:06,  1.96it/s]global step 401700, epoch: 3, batch: 8998, loss: 1.96564, acc: 0.95566, time: Sun Jan 23 19:41:14 2022
epoch 3 training:   5%|▍         | 9097/196351 [1:17:10<26:22:07,  1.97it/s]global step 401800, epoch: 3, batch: 9098, loss: 0.33314, acc: 0.95592, time: Sun Jan 23 19:42:05 2022
epoch 3 training:   5%|▍         | 9197/196351 [1:18:01<26:12:03,  1.98it/s]global step 401900, epoch: 3, batch: 9198, loss: 1.00170, acc: 0.95581, time: Sun Jan 23 19:42:56 2022
epoch 3 training:   5%|▍         | 9297/196351 [1:18:52<25:57:50,  2.00it/s]global step 402000, epoch: 3, batch: 9298, loss: 2.04851, acc: 0.95596, time: Sun Jan 23 19:43:46 2022
epoch 3 training:   5%|▍         | 9397/196351 [1:19:42<26:21:21,  1.97it/s]global step 402100, epoch: 3, batch: 9398, loss: 2.21717, acc: 0.95595, time: Sun Jan 23 19:44:37 2022
epoch 3 training:   5%|▍         | 9497/196351 [1:20:33<26:36:25,  1.95it/s]global step 402200, epoch: 3, batch: 9498, loss: 0.04706, acc: 0.95610, time: Sun Jan 23 19:45:28 2022
epoch 3 training:   5%|▍         | 9597/196351 [1:21:24<26:17:27,  1.97it/s]global step 402300, epoch: 3, batch: 9598, loss: 0.54153, acc: 0.95619, time: Sun Jan 23 19:46:19 2022
epoch 3 training:   5%|▍         | 9697/196351 [1:22:14<26:20:04,  1.97it/s]global step 402400, epoch: 3, batch: 9698, loss: 1.89484, acc: 0.95618, time: Sun Jan 23 19:47:09 2022
epoch 3 training:   5%|▍         | 9797/196351 [1:23:05<26:07:03,  1.98it/s]global step 402500, epoch: 3, batch: 9798, loss: 1.15914, acc: 0.95616, time: Sun Jan 23 19:48:00 2022
epoch 3 training:   5%|▌         | 9897/196351 [1:23:56<26:35:11,  1.95it/s]global step 402600, epoch: 3, batch: 9898, loss: 0.51136, acc: 0.95600, time: Sun Jan 23 19:48:51 2022
epoch 3 training:   5%|▌         | 9997/196351 [1:24:47<25:47:08,  2.01it/s]global step 402700, epoch: 3, batch: 9998, loss: 0.44251, acc: 0.95614, time: Sun Jan 23 19:49:42 2022
epoch 3 training:   5%|▌         | 10097/196351 [1:25:37<25:59:31,  1.99it/s]global step 402800, epoch: 3, batch: 10098, loss: 0.53768, acc: 0.95608, time: Sun Jan 23 19:50:32 2022
epoch 3 training:   5%|▌         | 10197/196351 [1:26:28<26:18:01,  1.97it/s]global step 402900, epoch: 3, batch: 10198, loss: 0.58040, acc: 0.95607, time: Sun Jan 23 19:51:23 2022
epoch 3 training:   5%|▌         | 10297/196351 [1:27:19<26:46:01,  1.93it/s]global step 403000, epoch: 3, batch: 10298, loss: 0.71301, acc: 0.95601, time: Sun Jan 23 19:52:14 2022
epoch 3 training:   5%|▌         | 10397/196351 [1:28:10<26:00:50,  1.99it/s]global step 403100, epoch: 3, batch: 10398, loss: 1.97635, acc: 0.95600, time: Sun Jan 23 19:53:05 2022
epoch 3 training:   5%|▌         | 10497/196351 [1:29:01<26:13:11,  1.97it/s]global step 403200, epoch: 3, batch: 10498, loss: 1.25390, acc: 0.95590, time: Sun Jan 23 19:53:56 2022
epoch 3 training:   5%|▌         | 10597/196351 [1:29:52<26:22:08,  1.96it/s]global step 403300, epoch: 3, batch: 10598, loss: 0.29287, acc: 0.95608, time: Sun Jan 23 19:54:46 2022
epoch 3 training:   5%|▌         | 10697/196351 [1:30:43<26:23:34,  1.95it/s]global step 403400, epoch: 3, batch: 10698, loss: 0.30074, acc: 0.95621, time: Sun Jan 23 19:55:37 2022
epoch 3 training:   5%|▌         | 10797/196351 [1:31:33<26:03:18,  1.98it/s]global step 403500, epoch: 3, batch: 10798, loss: 2.21653, acc: 0.95615, time: Sun Jan 23 19:56:28 2022
epoch 3 training:   6%|▌         | 10897/196351 [1:32:24<25:52:42,  1.99it/s]global step 403600, epoch: 3, batch: 10898, loss: 3.53060, acc: 0.95632, time: Sun Jan 23 19:57:19 2022
epoch 3 training:   6%|▌         | 10997/196351 [1:33:15<26:32:57,  1.94it/s]global step 403700, epoch: 3, batch: 10998, loss: 1.46379, acc: 0.95622, time: Sun Jan 23 19:58:09 2022
epoch 3 training:   6%|▌         | 11097/196351 [1:34:05<25:51:09,  1.99it/s]global step 403800, epoch: 3, batch: 11098, loss: 1.90885, acc: 0.95630, time: Sun Jan 23 19:59:00 2022
epoch 3 training:   6%|▌         | 11197/196351 [1:34:56<26:06:56,  1.97it/s]global step 403900, epoch: 3, batch: 11198, loss: 0.23896, acc: 0.95629, time: Sun Jan 23 19:59:51 2022
epoch 3 training:   6%|▌         | 11297/196351 [1:35:47<25:59:29,  1.98it/s]global step 404000, epoch: 3, batch: 11298, loss: 0.62452, acc: 0.95628, time: Sun Jan 23 20:00:42 2022
epoch 3 training:   6%|▌         | 11397/196351 [1:36:38<26:21:16,  1.95it/s]global step 404100, epoch: 3, batch: 11398, loss: 1.65045, acc: 0.95635, time: Sun Jan 23 20:01:33 2022
epoch 3 training:   6%|▌         | 11497/196351 [1:37:29<26:15:57,  1.95it/s]global step 404200, epoch: 3, batch: 11498, loss: 2.77957, acc: 0.95638, time: Sun Jan 23 20:02:24 2022
epoch 3 training:   6%|▌         | 11597/196351 [1:38:20<26:21:24,  1.95it/s]global step 404300, epoch: 3, batch: 11598, loss: 1.20471, acc: 0.95646, time: Sun Jan 23 20:03:15 2022
epoch 3 training:   6%|▌         | 11697/196351 [1:39:12<26:09:32,  1.96it/s]global step 404400, epoch: 3, batch: 11698, loss: 2.21864, acc: 0.95645, time: Sun Jan 23 20:04:06 2022
epoch 3 training:   6%|▌         | 11797/196351 [1:40:03<25:58:08,  1.97it/s]global step 404500, epoch: 3, batch: 11798, loss: 0.73146, acc: 0.95618, time: Sun Jan 23 20:04:58 2022
epoch 3 training:   6%|▌         | 11897/196351 [1:40:54<26:10:09,  1.96it/s]global step 404600, epoch: 3, batch: 11898, loss: 1.31146, acc: 0.95609, time: Sun Jan 23 20:05:49 2022
epoch 3 training:   6%|▌         | 11997/196351 [1:41:44<26:06:38,  1.96it/s]global step 404700, epoch: 3, batch: 11998, loss: 0.81308, acc: 0.95608, time: Sun Jan 23 20:06:39 2022
epoch 3 training:   6%|▌         | 12097/196351 [1:42:36<26:22:23,  1.94it/s]global step 404800, epoch: 3, batch: 12098, loss: 1.75065, acc: 0.95594, time: Sun Jan 23 20:07:31 2022
epoch 3 training:   6%|▌         | 12197/196351 [1:43:27<26:04:08,  1.96it/s]global step 404900, epoch: 3, batch: 12198, loss: 1.44836, acc: 0.95598, time: Sun Jan 23 20:08:22 2022
epoch 3 training:   6%|▋         | 12297/196351 [1:44:18<25:49:51,  1.98it/s]global step 405000, epoch: 3, batch: 12298, loss: 2.28782, acc: 0.95605, time: Sun Jan 23 20:09:13 2022
epoch 3 training:   6%|▋         | 12397/196351 [1:45:09<25:48:33,  1.98it/s]global step 405100, epoch: 3, batch: 12398, loss: 1.08700, acc: 0.95608, time: Sun Jan 23 20:10:04 2022
epoch 3 training:   6%|▋         | 12497/196351 [1:46:00<25:49:21,  1.98it/s]global step 405200, epoch: 3, batch: 12498, loss: 0.12628, acc: 0.95599, time: Sun Jan 23 20:10:55 2022
epoch 3 training:   6%|▋         | 12597/196351 [1:46:50<25:49:26,  1.98it/s]global step 405300, epoch: 3, batch: 12598, loss: 1.23460, acc: 0.95599, time: Sun Jan 23 20:11:45 2022
epoch 3 training:   6%|▋         | 12697/196351 [1:47:41<26:09:38,  1.95it/s]global step 405400, epoch: 3, batch: 12698, loss: 0.94415, acc: 0.95598, time: Sun Jan 23 20:12:36 2022
epoch 3 training:   7%|▋         | 12797/196351 [1:48:31<25:43:13,  1.98it/s]global step 405500, epoch: 3, batch: 12798, loss: 1.45292, acc: 0.95605, time: Sun Jan 23 20:13:26 2022
epoch 3 training:   7%|▋         | 12897/196351 [1:49:23<26:35:32,  1.92it/s]global step 405600, epoch: 3, batch: 12898, loss: 0.36667, acc: 0.95600, time: Sun Jan 23 20:14:18 2022
epoch 3 training:   7%|▋         | 12997/196351 [1:50:15<26:13:55,  1.94it/s]global step 405700, epoch: 3, batch: 12998, loss: 0.71367, acc: 0.95611, time: Sun Jan 23 20:15:09 2022
epoch 3 training:   7%|▋         | 13097/196351 [1:51:06<25:39:12,  1.98it/s]global step 405800, epoch: 3, batch: 13098, loss: 3.36548, acc: 0.95633, time: Sun Jan 23 20:16:01 2022
epoch 3 training:   7%|▋         | 13197/196351 [1:51:57<26:10:09,  1.94it/s]global step 405900, epoch: 3, batch: 13198, loss: 0.43375, acc: 0.95628, time: Sun Jan 23 20:16:52 2022
epoch 3 training:   7%|▋         | 13297/196351 [1:52:48<25:44:55,  1.97it/s]global step 406000, epoch: 3, batch: 13298, loss: 0.82331, acc: 0.95627, time: Sun Jan 23 20:17:43 2022
epoch 3 training:   7%|▋         | 13397/196351 [1:53:39<26:01:11,  1.95it/s]global step 406100, epoch: 3, batch: 13398, loss: 0.39368, acc: 0.95622, time: Sun Jan 23 20:18:34 2022
epoch 3 training:   7%|▋         | 13497/196351 [1:54:31<25:48:41,  1.97it/s]global step 406200, epoch: 3, batch: 13498, loss: 1.41445, acc: 0.95618, time: Sun Jan 23 20:19:26 2022
epoch 3 training:   7%|▋         | 13597/196351 [1:55:22<25:58:58,  1.95it/s]global step 406300, epoch: 3, batch: 13598, loss: 1.27191, acc: 0.95624, time: Sun Jan 23 20:20:17 2022
epoch 3 training:   7%|▋         | 13697/196351 [1:56:13<25:50:16,  1.96it/s]global step 406400, epoch: 3, batch: 13698, loss: 1.44997, acc: 0.95631, time: Sun Jan 23 20:21:08 2022
epoch 3 training:   7%|▋         | 13797/196351 [1:57:05<26:18:11,  1.93it/s]global step 406500, epoch: 3, batch: 13798, loss: 1.19586, acc: 0.95644, time: Sun Jan 23 20:22:00 2022
epoch 3 training:   7%|▋         | 13897/196351 [1:57:56<26:08:37,  1.94it/s]global step 406600, epoch: 3, batch: 13898, loss: 2.31150, acc: 0.95661, time: Sun Jan 23 20:22:51 2022
epoch 3 training:   7%|▋         | 13997/196351 [1:58:47<26:00:31,  1.95it/s]global step 406700, epoch: 3, batch: 13998, loss: 0.64984, acc: 0.95671, time: Sun Jan 23 20:23:42 2022
epoch 3 training:   7%|▋         | 14097/196351 [1:59:38<25:54:30,  1.95it/s]global step 406800, epoch: 3, batch: 14098, loss: 0.49226, acc: 0.95670, time: Sun Jan 23 20:24:33 2022
epoch 3 training:   7%|▋         | 14197/196351 [2:00:30<26:09:38,  1.93it/s]global step 406900, epoch: 3, batch: 14198, loss: 0.23191, acc: 0.95675, time: Sun Jan 23 20:25:24 2022
epoch 3 training:   7%|▋         | 14297/196351 [2:01:21<26:10:08,  1.93it/s]global step 407000, epoch: 3, batch: 14298, loss: 0.48113, acc: 0.95667, time: Sun Jan 23 20:26:16 2022
epoch 3 training:   7%|▋         | 14397/196351 [2:02:12<25:41:49,  1.97it/s]global step 407100, epoch: 3, batch: 14398, loss: 3.64475, acc: 0.95645, time: Sun Jan 23 20:27:07 2022
epoch 3 training:   7%|▋         | 14497/196351 [2:03:04<26:16:41,  1.92it/s]global step 407200, epoch: 3, batch: 14498, loss: 1.45757, acc: 0.95641, time: Sun Jan 23 20:27:59 2022
epoch 3 training:   7%|▋         | 14597/196351 [2:03:56<26:08:40,  1.93it/s]global step 407300, epoch: 3, batch: 14598, loss: 0.67598, acc: 0.95647, time: Sun Jan 23 20:28:50 2022
epoch 3 training:   7%|▋         | 14697/196351 [2:04:47<25:39:11,  1.97it/s]global step 407400, epoch: 3, batch: 14698, loss: 0.60423, acc: 0.95666, time: Sun Jan 23 20:29:41 2022
epoch 3 training:   8%|▊         | 14797/196351 [2:05:38<25:41:06,  1.96it/s]global step 407500, epoch: 3, batch: 14798, loss: 0.65697, acc: 0.95675, time: Sun Jan 23 20:30:33 2022
epoch 3 training:   8%|▊         | 14897/196351 [2:06:29<25:32:23,  1.97it/s]global step 407600, epoch: 3, batch: 14898, loss: 0.38127, acc: 0.95674, time: Sun Jan 23 20:31:24 2022
epoch 3 training:   8%|▊         | 14997/196351 [2:07:20<25:45:42,  1.96it/s]global step 407700, epoch: 3, batch: 14998, loss: 0.30839, acc: 0.95669, time: Sun Jan 23 20:32:15 2022
epoch 3 training:   8%|▊         | 15097/196351 [2:08:11<25:54:32,  1.94it/s]global step 407800, epoch: 3, batch: 15098, loss: 2.48973, acc: 0.95672, time: Sun Jan 23 20:33:06 2022
epoch 3 training:   8%|▊         | 15197/196351 [2:09:03<28:11:43,  1.78it/s]global step 407900, epoch: 3, batch: 15198, loss: 2.07607, acc: 0.95670, time: Sun Jan 23 20:33:58 2022
epoch 3 training:   8%|▊         | 15297/196351 [2:09:54<25:22:36,  1.98it/s]global step 408000, epoch: 3, batch: 15298, loss: 0.28849, acc: 0.95663, time: Sun Jan 23 20:34:49 2022
epoch 3 training:   8%|▊         | 15397/196351 [2:10:45<25:32:28,  1.97it/s]global step 408100, epoch: 3, batch: 15398, loss: 0.75393, acc: 0.95678, time: Sun Jan 23 20:35:40 2022
epoch 3 training:   8%|▊         | 15497/196351 [2:11:36<25:46:02,  1.95it/s]global step 408200, epoch: 3, batch: 15498, loss: 0.29887, acc: 0.95677, time: Sun Jan 23 20:36:31 2022
epoch 3 training:   8%|▊         | 15597/196351 [2:12:28<25:50:44,  1.94it/s]global step 408300, epoch: 3, batch: 15598, loss: 1.41900, acc: 0.95685, time: Sun Jan 23 20:37:23 2022
epoch 3 training:   8%|▊         | 15697/196351 [2:13:20<25:30:39,  1.97it/s]global step 408400, epoch: 3, batch: 15698, loss: 0.14675, acc: 0.95691, time: Sun Jan 23 20:38:14 2022
epoch 3 training:   8%|▊         | 15797/196351 [2:14:11<26:03:09,  1.93it/s]global step 408500, epoch: 3, batch: 15798, loss: 0.83195, acc: 0.95696, time: Sun Jan 23 20:39:06 2022
epoch 3 training:   8%|▊         | 15897/196351 [2:15:03<25:50:40,  1.94it/s]global step 408600, epoch: 3, batch: 15898, loss: 1.19453, acc: 0.95701, time: Sun Jan 23 20:39:58 2022
epoch 3 training:   8%|▊         | 15997/196351 [2:15:55<25:47:20,  1.94it/s]global step 408700, epoch: 3, batch: 15998, loss: 0.33956, acc: 0.95715, time: Sun Jan 23 20:40:49 2022
epoch 3 training:   8%|▊         | 16097/196351 [2:16:46<25:43:50,  1.95it/s]global step 408800, epoch: 3, batch: 16098, loss: 0.04805, acc: 0.95714, time: Sun Jan 23 20:41:41 2022
epoch 3 training:   8%|▊         | 16197/196351 [2:17:37<25:27:55,  1.97it/s]global step 408900, epoch: 3, batch: 16198, loss: 1.10273, acc: 0.95706, time: Sun Jan 23 20:42:32 2022
epoch 3 training:   8%|▊         | 16297/196351 [2:18:29<25:41:19,  1.95it/s]global step 409000, epoch: 3, batch: 16298, loss: 0.99838, acc: 0.95708, time: Sun Jan 23 20:43:23 2022
epoch 3 training:   8%|▊         | 16397/196351 [2:19:20<25:21:04,  1.97it/s]global step 409100, epoch: 3, batch: 16398, loss: 1.30381, acc: 0.95701, time: Sun Jan 23 20:44:15 2022
epoch 3 training:   8%|▊         | 16497/196351 [2:20:11<25:38:06,  1.95it/s]global step 409200, epoch: 3, batch: 16498, loss: 0.08060, acc: 0.95709, time: Sun Jan 23 20:45:06 2022
epoch 3 training:   8%|▊         | 16597/196351 [2:21:02<25:40:08,  1.95it/s]global step 409300, epoch: 3, batch: 16598, loss: 1.13223, acc: 0.95713, time: Sun Jan 23 20:45:57 2022
epoch 3 training:   9%|▊         | 16697/196351 [2:21:54<25:35:10,  1.95it/s]global step 409400, epoch: 3, batch: 16698, loss: 0.22118, acc: 0.95718, time: Sun Jan 23 20:46:49 2022
epoch 3 training:   9%|▊         | 16797/196351 [2:22:46<26:10:06,  1.91it/s]global step 409500, epoch: 3, batch: 16798, loss: 1.14521, acc: 0.95726, time: Sun Jan 23 20:47:41 2022
epoch 3 training:   9%|▊         | 16897/196351 [2:23:37<25:57:21,  1.92it/s]global step 409600, epoch: 3, batch: 16898, loss: 0.80898, acc: 0.95727, time: Sun Jan 23 20:48:32 2022
epoch 3 training:   9%|▊         | 16997/196351 [2:24:29<25:57:09,  1.92it/s]global step 409700, epoch: 3, batch: 16998, loss: 2.24913, acc: 0.95726, time: Sun Jan 23 20:49:24 2022
epoch 3 training:   9%|▊         | 17097/196351 [2:25:21<25:55:11,  1.92it/s]global step 409800, epoch: 3, batch: 17098, loss: 0.49657, acc: 0.95728, time: Sun Jan 23 20:50:15 2022
epoch 3 training:   9%|▉         | 17197/196351 [2:26:13<25:40:00,  1.94it/s]global step 409900, epoch: 3, batch: 17198, loss: 3.35971, acc: 0.95715, time: Sun Jan 23 20:51:07 2022
epoch 3 training:   9%|▉         | 17297/196351 [2:27:04<25:40:10,  1.94it/s]global step 410000, epoch: 3, batch: 17298, loss: 2.78032, acc: 0.95702, time: Sun Jan 23 20:51:59 2022
epoch 3 training:   9%|▉         | 17397/196351 [2:27:56<25:51:00,  1.92it/s]global step 410100, epoch: 3, batch: 17398, loss: 1.03922, acc: 0.95709, time: Sun Jan 23 20:52:51 2022
epoch 3 training:   9%|▉         | 17497/196351 [2:28:48<25:48:06,  1.93it/s]global step 410200, epoch: 3, batch: 17498, loss: 1.11805, acc: 0.95700, time: Sun Jan 23 20:53:43 2022
epoch 3 training:   9%|▉         | 17597/196351 [2:29:40<25:42:39,  1.93it/s]global step 410300, epoch: 3, batch: 17598, loss: 0.25806, acc: 0.95693, time: Sun Jan 23 20:54:34 2022
epoch 3 training:   9%|▉         | 17697/196351 [2:30:31<25:06:19,  1.98it/s]global step 410400, epoch: 3, batch: 17698, loss: 0.25422, acc: 0.95683, time: Sun Jan 23 20:55:26 2022
epoch 3 training:   9%|▉         | 17797/196351 [2:31:22<25:21:58,  1.96it/s]global step 410500, epoch: 3, batch: 17798, loss: 1.67679, acc: 0.95676, time: Sun Jan 23 20:56:17 2022
epoch 3 training:   9%|▉         | 17897/196351 [2:32:13<24:57:57,  1.99it/s]global step 410600, epoch: 3, batch: 17898, loss: 1.82420, acc: 0.95684, time: Sun Jan 23 20:57:08 2022
epoch 3 training:   9%|▉         | 17997/196351 [2:33:04<24:55:53,  1.99it/s]global step 410700, epoch: 3, batch: 17998, loss: 2.13125, acc: 0.95688, time: Sun Jan 23 20:57:58 2022
epoch 3 training:   9%|▉         | 18097/196351 [2:33:55<25:10:26,  1.97it/s]global step 410800, epoch: 3, batch: 18098, loss: 1.50252, acc: 0.95690, time: Sun Jan 23 20:58:50 2022
epoch 3 training:   9%|▉         | 18197/196351 [2:34:46<25:06:10,  1.97it/s]global step 410900, epoch: 3, batch: 18198, loss: 3.46945, acc: 0.95675, time: Sun Jan 23 20:59:41 2022
epoch 3 training:   9%|▉         | 18297/196351 [2:35:38<25:15:27,  1.96it/s]global step 411000, epoch: 3, batch: 18298, loss: 0.78112, acc: 0.95680, time: Sun Jan 23 21:00:32 2022
epoch 3 training:   9%|▉         | 18397/196351 [2:36:28<25:19:21,  1.95it/s]global step 411100, epoch: 3, batch: 18398, loss: 0.79752, acc: 0.95665, time: Sun Jan 23 21:01:23 2022
epoch 3 training:   9%|▉         | 18497/196351 [2:37:19<25:30:52,  1.94it/s]global step 411200, epoch: 3, batch: 18498, loss: 1.52929, acc: 0.95656, time: Sun Jan 23 21:02:14 2022
epoch 3 training:   9%|▉         | 18597/196351 [2:38:10<25:20:25,  1.95it/s]global step 411300, epoch: 3, batch: 18598, loss: 1.01142, acc: 0.95653, time: Sun Jan 23 21:03:05 2022
epoch 3 training:  10%|▉         | 18697/196351 [2:39:02<26:00:46,  1.90it/s]global step 411400, epoch: 3, batch: 18698, loss: 0.66094, acc: 0.95660, time: Sun Jan 23 21:03:57 2022
epoch 3 training:  10%|▉         | 18797/196351 [2:39:54<25:29:15,  1.94it/s]global step 411500, epoch: 3, batch: 18798, loss: 0.64146, acc: 0.95643, time: Sun Jan 23 21:04:49 2022
epoch 3 training:  10%|▉         | 18897/196351 [2:40:45<25:14:04,  1.95it/s]global step 411600, epoch: 3, batch: 18898, loss: 0.29075, acc: 0.95629, time: Sun Jan 23 21:05:40 2022
epoch 3 training:  10%|▉         | 18997/196351 [2:41:37<25:07:14,  1.96it/s]global step 411700, epoch: 3, batch: 18998, loss: 0.74244, acc: 0.95615, time: Sun Jan 23 21:06:32 2022
epoch 3 training:  10%|▉         | 19097/196351 [2:42:29<25:23:39,  1.94it/s]global step 411800, epoch: 3, batch: 19098, loss: 1.38971, acc: 0.95615, time: Sun Jan 23 21:07:24 2022
epoch 3 training:  10%|▉         | 19197/196351 [2:43:21<25:22:13,  1.94it/s]global step 411900, epoch: 3, batch: 19198, loss: 0.68969, acc: 0.95617, time: Sun Jan 23 21:08:16 2022
epoch 3 training:  10%|▉         | 19297/196351 [2:44:12<25:04:46,  1.96it/s]global step 412000, epoch: 3, batch: 19298, loss: 3.20447, acc: 0.95626, time: Sun Jan 23 21:09:07 2022
epoch 3 training:  10%|▉         | 19397/196351 [2:45:04<25:18:51,  1.94it/s]global step 412100, epoch: 3, batch: 19398, loss: 0.64674, acc: 0.95628, time: Sun Jan 23 21:09:59 2022
epoch 3 training:  10%|▉         | 19497/196351 [2:45:55<24:55:53,  1.97it/s]global step 412200, epoch: 3, batch: 19498, loss: 0.32235, acc: 0.95623, time: Sun Jan 23 21:10:50 2022
epoch 3 training:  10%|▉         | 19597/196351 [2:46:47<24:47:37,  1.98it/s]global step 412300, epoch: 3, batch: 19598, loss: 0.69443, acc: 0.95619, time: Sun Jan 23 21:11:42 2022
epoch 3 training:  10%|█         | 19697/196351 [2:47:38<24:53:13,  1.97it/s]global step 412400, epoch: 3, batch: 19698, loss: 2.82607, acc: 0.95626, time: Sun Jan 23 21:12:33 2022
epoch 3 training:  10%|█         | 19797/196351 [2:48:29<25:08:44,  1.95it/s]global step 412500, epoch: 3, batch: 19798, loss: 0.38375, acc: 0.95621, time: Sun Jan 23 21:13:24 2022
epoch 3 training:  10%|█         | 19897/196351 [2:49:21<24:50:32,  1.97it/s]global step 412600, epoch: 3, batch: 19898, loss: 0.89987, acc: 0.95623, time: Sun Jan 23 21:14:16 2022
epoch 3 training:  10%|█         | 19997/196351 [2:50:12<24:51:00,  1.97it/s]global step 412700, epoch: 3, batch: 19998, loss: 1.27416, acc: 0.95632, time: Sun Jan 23 21:15:07 2022
epoch 3 training:  10%|█         | 20097/196351 [2:51:04<24:55:41,  1.96it/s]global step 412800, epoch: 3, batch: 20098, loss: 0.50951, acc: 0.95629, time: Sun Jan 23 21:15:59 2022
epoch 3 training:  10%|█         | 20197/196351 [2:51:55<25:19:19,  1.93it/s]global step 412900, epoch: 3, batch: 20198, loss: 2.21770, acc: 0.95641, time: Sun Jan 23 21:16:50 2022
epoch 3 training:  10%|█         | 20297/196351 [2:52:46<24:50:14,  1.97it/s]global step 413000, epoch: 3, batch: 20298, loss: 0.44805, acc: 0.95645, time: Sun Jan 23 21:17:41 2022
epoch 3 training:  10%|█         | 20397/196351 [2:53:37<24:50:29,  1.97it/s]global step 413100, epoch: 3, batch: 20398, loss: 0.96887, acc: 0.95652, time: Sun Jan 23 21:18:32 2022
epoch 3 training:  10%|█         | 20497/196351 [2:54:28<24:57:07,  1.96it/s]global step 413200, epoch: 3, batch: 20498, loss: 0.55462, acc: 0.95653, time: Sun Jan 23 21:19:23 2022
epoch 3 training:  10%|█         | 20597/196351 [2:55:19<24:41:31,  1.98it/s]global step 413300, epoch: 3, batch: 20598, loss: 2.58408, acc: 0.95662, time: Sun Jan 23 21:20:14 2022
epoch 3 training:  11%|█         | 20697/196351 [2:56:10<25:11:46,  1.94it/s]global step 413400, epoch: 3, batch: 20698, loss: 1.40493, acc: 0.95661, time: Sun Jan 23 21:21:05 2022
epoch 3 training:  11%|█         | 20797/196351 [2:57:02<25:20:39,  1.92it/s]global step 413500, epoch: 3, batch: 20798, loss: 1.56342, acc: 0.95670, time: Sun Jan 23 21:21:57 2022
epoch 3 training:  11%|█         | 20897/196351 [2:57:53<25:08:51,  1.94it/s]global step 413600, epoch: 3, batch: 20898, loss: 0.54041, acc: 0.95662, time: Sun Jan 23 21:22:48 2022
epoch 3 training:  11%|█         | 20997/196351 [2:58:45<25:11:00,  1.93it/s]global step 413700, epoch: 3, batch: 20998, loss: 1.36776, acc: 0.95669, time: Sun Jan 23 21:23:40 2022
epoch 3 training:  11%|█         | 21097/196351 [2:59:36<24:23:39,  2.00it/s]global step 413800, epoch: 3, batch: 21098, loss: 0.73444, acc: 0.95677, time: Sun Jan 23 21:24:31 2022
epoch 3 training:  11%|█         | 21197/196351 [3:00:27<25:02:29,  1.94it/s]global step 413900, epoch: 3, batch: 21198, loss: 1.36214, acc: 0.95681, time: Sun Jan 23 21:25:22 2022
epoch 3 training:  11%|█         | 21297/196351 [3:01:19<25:01:22,  1.94it/s]global step 414000, epoch: 3, batch: 21298, loss: 0.14807, acc: 0.95683, time: Sun Jan 23 21:26:14 2022
epoch 3 training:  11%|█         | 21397/196351 [3:02:10<25:11:30,  1.93it/s]global step 414100, epoch: 3, batch: 21398, loss: 1.26710, acc: 0.95672, time: Sun Jan 23 21:27:05 2022
epoch 3 training:  11%|█         | 21497/196351 [3:03:02<25:05:08,  1.94it/s]global step 414200, epoch: 3, batch: 21498, loss: 1.98424, acc: 0.95674, time: Sun Jan 23 21:27:57 2022
epoch 3 training:  11%|█         | 21597/196351 [3:03:53<24:55:52,  1.95it/s]global step 414300, epoch: 3, batch: 21598, loss: 0.20534, acc: 0.95673, time: Sun Jan 23 21:28:48 2022
epoch 3 training:  11%|█         | 21697/196351 [3:04:44<24:35:31,  1.97it/s]global step 414400, epoch: 3, batch: 21698, loss: 0.96961, acc: 0.95672, time: Sun Jan 23 21:29:39 2022
epoch 3 training:  11%|█         | 21797/196351 [3:05:36<24:56:23,  1.94it/s]global step 414500, epoch: 3, batch: 21798, loss: 1.28453, acc: 0.95683, time: Sun Jan 23 21:30:31 2022
epoch 3 training:  11%|█         | 21897/196351 [3:06:28<25:06:43,  1.93it/s]global step 414600, epoch: 3, batch: 21898, loss: 0.25535, acc: 0.95696, time: Sun Jan 23 21:31:22 2022
epoch 3 training:  11%|█         | 21997/196351 [3:07:19<24:42:44,  1.96it/s]global step 414700, epoch: 3, batch: 21998, loss: 1.42819, acc: 0.95693, time: Sun Jan 23 21:32:14 2022
epoch 3 training:  11%|█▏        | 22097/196351 [3:08:10<24:31:18,  1.97it/s]global step 414800, epoch: 3, batch: 22098, loss: 2.50069, acc: 0.95699, time: Sun Jan 23 21:33:04 2022
epoch 3 training:  11%|█▏        | 22197/196351 [3:09:01<24:38:13,  1.96it/s]global step 414900, epoch: 3, batch: 22198, loss: 1.84760, acc: 0.95707, time: Sun Jan 23 21:33:56 2022
epoch 3 training:  11%|█▏        | 22297/196351 [3:09:52<24:58:57,  1.94it/s]global step 415000, epoch: 3, batch: 22298, loss: 0.40845, acc: 0.95717, time: Sun Jan 23 21:34:47 2022
epoch 3 training:  11%|█▏        | 22397/196351 [3:10:43<24:39:48,  1.96it/s]global step 415100, epoch: 3, batch: 22398, loss: 0.55366, acc: 0.95723, time: Sun Jan 23 21:35:38 2022
epoch 3 training:  11%|█▏        | 22497/196351 [3:11:35<24:50:24,  1.94it/s]global step 415200, epoch: 3, batch: 22498, loss: 1.68347, acc: 0.95729, time: Sun Jan 23 21:36:30 2022
epoch 3 training:  12%|█▏        | 22597/196351 [3:12:26<24:17:36,  1.99it/s]global step 415300, epoch: 3, batch: 22598, loss: 0.37582, acc: 0.95727, time: Sun Jan 23 21:37:21 2022
epoch 3 training:  12%|█▏        | 22697/196351 [3:13:17<24:44:46,  1.95it/s]global step 415400, epoch: 3, batch: 22698, loss: 0.94421, acc: 0.95724, time: Sun Jan 23 21:38:12 2022
epoch 3 training:  12%|█▏        | 22797/196351 [3:14:09<25:13:53,  1.91it/s]global step 415500, epoch: 3, batch: 22798, loss: 0.56820, acc: 0.95726, time: Sun Jan 23 21:39:04 2022
epoch 3 training:  12%|█▏        | 22897/196351 [3:15:01<25:10:59,  1.91it/s]global step 415600, epoch: 3, batch: 22898, loss: 0.18203, acc: 0.95727, time: Sun Jan 23 21:39:55 2022
epoch 3 training:  12%|█▏        | 22997/196351 [3:15:52<24:52:27,  1.94it/s]global step 415700, epoch: 3, batch: 22998, loss: 0.55293, acc: 0.95732, time: Sun Jan 23 21:40:47 2022
epoch 3 training:  12%|█▏        | 23097/196351 [3:16:44<24:49:18,  1.94it/s]global step 415800, epoch: 3, batch: 23098, loss: 2.71348, acc: 0.95736, time: Sun Jan 23 21:41:39 2022
epoch 3 training:  12%|█▏        | 23197/196351 [3:17:35<24:56:47,  1.93it/s]global step 415900, epoch: 3, batch: 23198, loss: 0.34250, acc: 0.95728, time: Sun Jan 23 21:42:30 2022
epoch 3 training:  12%|█▏        | 23297/196351 [3:18:27<24:13:54,  1.98it/s]global step 416000, epoch: 3, batch: 23298, loss: 1.66761, acc: 0.95727, time: Sun Jan 23 21:43:22 2022
epoch 3 training:  12%|█▏        | 23397/196351 [3:19:18<24:46:11,  1.94it/s]global step 416100, epoch: 3, batch: 23398, loss: 2.85420, acc: 0.95722, time: Sun Jan 23 21:44:13 2022
epoch 3 training:  12%|█▏        | 23497/196351 [3:20:10<24:35:21,  1.95it/s]global step 416200, epoch: 3, batch: 23498, loss: 2.47124, acc: 0.95729, time: Sun Jan 23 21:45:04 2022
epoch 3 training:  12%|█▏        | 23597/196351 [3:21:01<24:23:45,  1.97it/s]global step 416300, epoch: 3, batch: 23598, loss: 0.45032, acc: 0.95722, time: Sun Jan 23 21:45:56 2022
epoch 3 training:  12%|█▏        | 23697/196351 [3:21:52<24:46:40,  1.94it/s]global step 416400, epoch: 3, batch: 23698, loss: 1.06717, acc: 0.95721, time: Sun Jan 23 21:46:47 2022
epoch 3 training:  12%|█▏        | 23797/196351 [3:22:43<24:22:43,  1.97it/s]global step 416500, epoch: 3, batch: 23798, loss: 0.87535, acc: 0.95724, time: Sun Jan 23 21:47:38 2022
epoch 3 training:  12%|█▏        | 23897/196351 [3:23:34<24:35:27,  1.95it/s]global step 416600, epoch: 3, batch: 23898, loss: 0.23742, acc: 0.95732, time: Sun Jan 23 21:48:29 2022
epoch 3 training:  12%|█▏        | 23997/196351 [3:24:26<24:23:28,  1.96it/s]global step 416700, epoch: 3, batch: 23998, loss: 0.51440, acc: 0.95727, time: Sun Jan 23 21:49:21 2022
epoch 3 training:  12%|█▏        | 24097/196351 [3:25:17<24:48:57,  1.93it/s]global step 416800, epoch: 3, batch: 24098, loss: 0.58276, acc: 0.95724, time: Sun Jan 23 21:50:12 2022
epoch 3 training:  12%|█▏        | 24197/196351 [3:26:08<24:48:55,  1.93it/s]global step 416900, epoch: 3, batch: 24198, loss: 1.92694, acc: 0.95719, time: Sun Jan 23 21:51:03 2022
epoch 3 training:  12%|█▏        | 24297/196351 [3:27:00<24:45:06,  1.93it/s]global step 417000, epoch: 3, batch: 24298, loss: 0.57108, acc: 0.95718, time: Sun Jan 23 21:51:55 2022
epoch 3 training:  12%|█▏        | 24397/196351 [3:27:52<24:22:04,  1.96it/s]global step 417100, epoch: 3, batch: 24398, loss: 2.37587, acc: 0.95725, time: Sun Jan 23 21:52:46 2022
epoch 3 training:  12%|█▏        | 24497/196351 [3:28:43<24:19:55,  1.96it/s]global step 417200, epoch: 3, batch: 24498, loss: 0.12716, acc: 0.95730, time: Sun Jan 23 21:53:38 2022
epoch 3 training:  13%|█▎        | 24597/196351 [3:29:34<24:36:53,  1.94it/s]global step 417300, epoch: 3, batch: 24598, loss: 0.22575, acc: 0.95744, time: Sun Jan 23 21:54:29 2022
epoch 3 training:  13%|█▎        | 24697/196351 [3:30:27<24:29:00,  1.95it/s]global step 417400, epoch: 3, batch: 24698, loss: 1.44253, acc: 0.95745, time: Sun Jan 23 21:55:22 2022
epoch 3 training:  13%|█▎        | 24797/196351 [3:31:18<24:13:51,  1.97it/s]global step 417500, epoch: 3, batch: 24798, loss: 0.65448, acc: 0.95746, time: Sun Jan 23 21:56:13 2022
epoch 3 training:  13%|█▎        | 24897/196351 [3:32:09<24:12:10,  1.97it/s]global step 417600, epoch: 3, batch: 24898, loss: 1.08571, acc: 0.95741, time: Sun Jan 23 21:57:04 2022
epoch 3 training:  13%|█▎        | 24997/196351 [3:33:00<24:04:14,  1.98it/s]global step 417700, epoch: 3, batch: 24998, loss: 0.06962, acc: 0.95742, time: Sun Jan 23 21:57:55 2022
epoch 3 training:  13%|█▎        | 25097/196351 [3:33:52<24:32:37,  1.94it/s]global step 417800, epoch: 3, batch: 25098, loss: 0.10365, acc: 0.95745, time: Sun Jan 23 21:58:46 2022
epoch 3 training:  13%|█▎        | 25197/196351 [3:34:43<24:25:58,  1.95it/s]global step 417900, epoch: 3, batch: 25198, loss: 0.27221, acc: 0.95750, time: Sun Jan 23 21:59:38 2022
epoch 3 training:  13%|█▎        | 25297/196351 [3:35:34<24:35:31,  1.93it/s]global step 418000, epoch: 3, batch: 25298, loss: 1.24496, acc: 0.95743, time: Sun Jan 23 22:00:29 2022
epoch 3 training:  13%|█▎        | 25397/196351 [3:36:26<24:23:06,  1.95it/s]global step 418100, epoch: 3, batch: 25398, loss: 1.41722, acc: 0.95744, time: Sun Jan 23 22:01:21 2022
epoch 3 training:  13%|█▎        | 25497/196351 [3:37:18<24:24:33,  1.94it/s]global step 418200, epoch: 3, batch: 25498, loss: 0.53279, acc: 0.95749, time: Sun Jan 23 22:02:13 2022
epoch 3 training:  13%|█▎        | 25597/196351 [3:38:09<24:22:37,  1.95it/s]global step 418300, epoch: 3, batch: 25598, loss: 1.75408, acc: 0.95754, time: Sun Jan 23 22:03:04 2022
epoch 3 training:  13%|█▎        | 25697/196351 [3:39:01<24:32:11,  1.93it/s]global step 418400, epoch: 3, batch: 25698, loss: 1.04764, acc: 0.95756, time: Sun Jan 23 22:03:55 2022
epoch 3 training:  13%|█▎        | 25797/196351 [3:39:52<24:14:12,  1.95it/s]global step 418500, epoch: 3, batch: 25798, loss: 2.19487, acc: 0.95750, time: Sun Jan 23 22:04:47 2022
epoch 3 training:  13%|█▎        | 25897/196351 [3:40:44<23:50:35,  1.99it/s]global step 418600, epoch: 3, batch: 25898, loss: 4.34733, acc: 0.95745, time: Sun Jan 23 22:05:38 2022
epoch 3 training:  13%|█▎        | 25997/196351 [3:41:35<24:22:30,  1.94it/s]global step 418700, epoch: 3, batch: 25998, loss: 0.34089, acc: 0.95736, time: Sun Jan 23 22:06:30 2022
epoch 3 training:  13%|█▎        | 26097/196351 [3:42:26<24:28:35,  1.93it/s]global step 418800, epoch: 3, batch: 26098, loss: 2.15897, acc: 0.95735, time: Sun Jan 23 22:07:21 2022
epoch 3 training:  13%|█▎        | 26197/196351 [3:43:17<24:21:39,  1.94it/s]global step 418900, epoch: 3, batch: 26198, loss: 0.91021, acc: 0.95746, time: Sun Jan 23 22:08:12 2022
epoch 3 training:  13%|█▎        | 26297/196351 [3:44:09<23:51:50,  1.98it/s]global step 419000, epoch: 3, batch: 26298, loss: 0.86720, acc: 0.95749, time: Sun Jan 23 22:09:04 2022
epoch 3 training:  13%|█▎        | 26397/196351 [3:45:00<24:13:30,  1.95it/s]global step 419100, epoch: 3, batch: 26398, loss: 2.27887, acc: 0.95746, time: Sun Jan 23 22:09:55 2022
epoch 3 training:  13%|█▎        | 26497/196351 [3:45:51<24:25:22,  1.93it/s]global step 419200, epoch: 3, batch: 26498, loss: 1.47985, acc: 0.95745, time: Sun Jan 23 22:10:46 2022
epoch 3 training:  14%|█▎        | 26597/196351 [3:46:43<24:16:18,  1.94it/s]global step 419300, epoch: 3, batch: 26598, loss: 0.10488, acc: 0.95752, time: Sun Jan 23 22:11:38 2022
epoch 3 training:  14%|█▎        | 26697/196351 [3:47:34<24:19:26,  1.94it/s]global step 419400, epoch: 3, batch: 26698, loss: 2.67216, acc: 0.95756, time: Sun Jan 23 22:12:29 2022
epoch 3 training:  14%|█▎        | 26797/196351 [3:48:26<24:45:15,  1.90it/s]global step 419500, epoch: 3, batch: 26798, loss: 0.39207, acc: 0.95753, time: Sun Jan 23 22:13:21 2022
epoch 3 training:  14%|█▎        | 26897/196351 [3:49:18<24:45:59,  1.90it/s]global step 419600, epoch: 3, batch: 26898, loss: 0.94583, acc: 0.95758, time: Sun Jan 23 22:14:13 2022
epoch 3 training:  14%|█▎        | 26997/196351 [3:50:09<24:10:04,  1.95it/s]global step 419700, epoch: 3, batch: 26998, loss: 0.64183, acc: 0.95755, time: Sun Jan 23 22:15:04 2022
epoch 3 training:  14%|█▍        | 27097/196351 [3:51:01<23:48:20,  1.97it/s]global step 419800, epoch: 3, batch: 27098, loss: 0.91368, acc: 0.95762, time: Sun Jan 23 22:15:56 2022
epoch 3 training:  14%|█▍        | 27197/196351 [3:51:52<24:00:46,  1.96it/s]global step 419900, epoch: 3, batch: 27198, loss: 1.74880, acc: 0.95759, time: Sun Jan 23 22:16:47 2022
epoch 3 training:  14%|█▍        | 27297/196351 [3:52:44<24:26:27,  1.92it/s]global step 420000, epoch: 3, batch: 27298, loss: 2.40257, acc: 0.95758, time: Sun Jan 23 22:17:38 2022
epoch 3 training:  14%|█▍        | 27397/196351 [3:53:38<24:58:17,  1.88it/s]global step 420100, epoch: 3, batch: 27398, loss: 3.21838, acc: 0.95762, time: Sun Jan 23 22:18:33 2022
epoch 3 training:  14%|█▍        | 27497/196351 [3:54:29<24:05:28,  1.95it/s]global step 420200, epoch: 3, batch: 27498, loss: 0.54869, acc: 0.95769, time: Sun Jan 23 22:19:24 2022
epoch 3 training:  14%|█▍        | 27597/196351 [3:55:21<24:33:53,  1.91it/s]global step 420300, epoch: 3, batch: 27598, loss: 0.52053, acc: 0.95771, time: Sun Jan 23 22:20:16 2022
epoch 3 training:  14%|█▍        | 27697/196351 [3:56:13<24:12:22,  1.94it/s]global step 420400, epoch: 3, batch: 27698, loss: 0.30902, acc: 0.95783, time: Sun Jan 23 22:21:07 2022
epoch 3 training:  14%|█▍        | 27797/196351 [3:57:09<24:41:43,  1.90it/s]global step 420500, epoch: 3, batch: 27798, loss: 0.20804, acc: 0.95786, time: Sun Jan 23 22:22:04 2022
epoch 3 training:  14%|█▍        | 27897/196351 [3:58:04<25:15:48,  1.85it/s]global step 420600, epoch: 3, batch: 27898, loss: 0.78634, acc: 0.95792, time: Sun Jan 23 22:22:58 2022
epoch 3 training:  14%|█▍        | 27997/196351 [3:58:58<24:49:09,  1.88it/s]global step 420700, epoch: 3, batch: 27998, loss: 0.62604, acc: 0.95800, time: Sun Jan 23 22:23:53 2022
epoch 3 training:  14%|█▍        | 28097/196351 [3:59:50<23:55:50,  1.95it/s]global step 420800, epoch: 3, batch: 28098, loss: 0.68868, acc: 0.95793, time: Sun Jan 23 22:24:45 2022
epoch 3 training:  14%|█▍        | 28197/196351 [4:00:41<23:53:05,  1.96it/s]global step 420900, epoch: 3, batch: 28198, loss: 0.18193, acc: 0.95789, time: Sun Jan 23 22:25:36 2022
epoch 3 training:  14%|█▍        | 28297/196351 [4:01:33<23:48:31,  1.96it/s]global step 421000, epoch: 3, batch: 28298, loss: 0.47299, acc: 0.95786, time: Sun Jan 23 22:26:27 2022
epoch 3 training:  14%|█▍        | 28397/196351 [4:02:24<23:43:35,  1.97it/s]global step 421100, epoch: 3, batch: 28398, loss: 1.49044, acc: 0.95783, time: Sun Jan 23 22:27:18 2022
epoch 3 training:  15%|█▍        | 28497/196351 [4:03:15<24:15:48,  1.92it/s]global step 421200, epoch: 3, batch: 28498, loss: 1.23842, acc: 0.95782, time: Sun Jan 23 22:28:10 2022
epoch 3 training:  15%|█▍        | 28597/196351 [4:04:06<23:50:11,  1.95it/s]global step 421300, epoch: 3, batch: 28598, loss: 0.84847, acc: 0.95790, time: Sun Jan 23 22:29:01 2022
epoch 3 training:  15%|█▍        | 28697/196351 [4:04:57<23:53:27,  1.95it/s]global step 421400, epoch: 3, batch: 28698, loss: 0.31045, acc: 0.95780, time: Sun Jan 23 22:29:52 2022
epoch 3 training:  15%|█▍        | 28797/196351 [4:05:50<25:40:24,  1.81it/s]global step 421500, epoch: 3, batch: 28798, loss: 2.70362, acc: 0.95779, time: Sun Jan 23 22:30:45 2022
epoch 3 training:  15%|█▍        | 28897/196351 [4:06:43<23:53:47,  1.95it/s]global step 421600, epoch: 3, batch: 28898, loss: 1.96200, acc: 0.95780, time: Sun Jan 23 22:31:38 2022
epoch 3 training:  15%|█▍        | 28997/196351 [4:07:34<23:58:42,  1.94it/s]global step 421700, epoch: 3, batch: 28998, loss: 1.04547, acc: 0.95779, time: Sun Jan 23 22:32:29 2022
epoch 3 training:  15%|█▍        | 29097/196351 [4:08:25<23:51:44,  1.95it/s]global step 421800, epoch: 3, batch: 29098, loss: 0.48179, acc: 0.95775, time: Sun Jan 23 22:33:20 2022
epoch 3 training:  15%|█▍        | 29197/196351 [4:09:16<23:38:01,  1.96it/s]global step 421900, epoch: 3, batch: 29198, loss: 0.99535, acc: 0.95774, time: Sun Jan 23 22:34:11 2022
epoch 3 training:  15%|█▍        | 29297/196351 [4:10:08<24:05:52,  1.93it/s]global step 422000, epoch: 3, batch: 29298, loss: 0.26964, acc: 0.95773, time: Sun Jan 23 22:35:03 2022
epoch 3 training:  15%|█▍        | 29397/196351 [4:10:59<23:44:07,  1.95it/s]global step 422100, epoch: 3, batch: 29398, loss: 1.89654, acc: 0.95774, time: Sun Jan 23 22:35:54 2022
epoch 3 training:  15%|█▌        | 29497/196351 [4:11:50<23:39:49,  1.96it/s]global step 422200, epoch: 3, batch: 29498, loss: 0.78096, acc: 0.95781, time: Sun Jan 23 22:36:45 2022
epoch 3 training:  15%|█▌        | 29597/196351 [4:12:42<26:04:24,  1.78it/s]global step 422300, epoch: 3, batch: 29598, loss: 0.40841, acc: 0.95787, time: Sun Jan 23 22:37:37 2022
epoch 3 training:  15%|█▌        | 29697/196351 [4:13:33<23:54:50,  1.94it/s]global step 422400, epoch: 3, batch: 29698, loss: 3.87269, acc: 0.95783, time: Sun Jan 23 22:38:28 2022
epoch 3 training:  15%|█▌        | 29797/196351 [4:14:24<23:51:53,  1.94it/s]global step 422500, epoch: 3, batch: 29798, loss: 0.85956, acc: 0.95785, time: Sun Jan 23 22:39:19 2022
epoch 3 training:  15%|█▌        | 29897/196351 [4:15:16<23:45:50,  1.95it/s]global step 422600, epoch: 3, batch: 29898, loss: 0.60567, acc: 0.95782, time: Sun Jan 23 22:40:11 2022
epoch 3 training:  15%|█▌        | 29997/196351 [4:16:07<23:50:42,  1.94it/s]global step 422700, epoch: 3, batch: 29998, loss: 2.98788, acc: 0.95768, time: Sun Jan 23 22:41:02 2022
epoch 3 training:  15%|█▌        | 30097/196351 [4:16:59<23:45:08,  1.94it/s]global step 422800, epoch: 3, batch: 30098, loss: 1.76193, acc: 0.95767, time: Sun Jan 23 22:41:54 2022
epoch 3 training:  15%|█▌        | 30197/196351 [4:17:51<23:55:09,  1.93it/s]global step 422900, epoch: 3, batch: 30198, loss: 3.76910, acc: 0.95761, time: Sun Jan 23 22:42:45 2022
epoch 3 training:  15%|█▌        | 30297/196351 [4:18:43<23:42:10,  1.95it/s]global step 423000, epoch: 3, batch: 30298, loss: 0.30888, acc: 0.95762, time: Sun Jan 23 22:43:37 2022
epoch 3 training:  15%|█▌        | 30397/196351 [4:19:34<23:34:13,  1.96it/s]global step 423100, epoch: 3, batch: 30398, loss: 1.11636, acc: 0.95756, time: Sun Jan 23 22:44:29 2022
epoch 3 training:  16%|█▌        | 30497/196351 [4:20:25<23:34:29,  1.95it/s]global step 423200, epoch: 3, batch: 30498, loss: 0.40409, acc: 0.95757, time: Sun Jan 23 22:45:20 2022
epoch 3 training:  16%|█▌        | 30597/196351 [4:21:16<23:48:56,  1.93it/s]global step 423300, epoch: 3, batch: 30598, loss: 1.10764, acc: 0.95753, time: Sun Jan 23 22:46:11 2022
epoch 3 training:  16%|█▌        | 30697/196351 [4:22:08<23:25:12,  1.96it/s]global step 423400, epoch: 3, batch: 30698, loss: 0.48817, acc: 0.95757, time: Sun Jan 23 22:47:03 2022
epoch 3 training:  16%|█▌        | 30797/196351 [4:22:59<23:36:01,  1.95it/s]global step 423500, epoch: 3, batch: 30798, loss: 0.70810, acc: 0.95756, time: Sun Jan 23 22:47:54 2022
epoch 3 training:  16%|█▌        | 30897/196351 [4:23:50<22:55:53,  2.00it/s]global step 423600, epoch: 3, batch: 30898, loss: 0.28456, acc: 0.95762, time: Sun Jan 23 22:48:45 2022
epoch 3 training:  16%|█▌        | 30997/196351 [4:24:41<23:37:48,  1.94it/s]global step 423700, epoch: 3, batch: 30998, loss: 1.04194, acc: 0.95759, time: Sun Jan 23 22:49:36 2022
epoch 3 training:  16%|█▌        | 31097/196351 [4:25:32<23:27:23,  1.96it/s]global step 423800, epoch: 3, batch: 31098, loss: 0.32838, acc: 0.95760, time: Sun Jan 23 22:50:27 2022
epoch 3 training:  16%|█▌        | 31197/196351 [4:26:23<23:24:34,  1.96it/s]global step 423900, epoch: 3, batch: 31198, loss: 0.67807, acc: 0.95761, time: Sun Jan 23 22:51:18 2022
epoch 3 training:  16%|█▌        | 31297/196351 [4:27:14<23:17:28,  1.97it/s]global step 424000, epoch: 3, batch: 31298, loss: 0.48872, acc: 0.95767, time: Sun Jan 23 22:52:09 2022
epoch 3 training:  16%|█▌        | 31397/196351 [4:28:05<23:25:08,  1.96it/s]global step 424100, epoch: 3, batch: 31398, loss: 0.28831, acc: 0.95769, time: Sun Jan 23 22:53:00 2022
epoch 3 training:  16%|█▌        | 31497/196351 [4:28:56<23:29:18,  1.95it/s]global step 424200, epoch: 3, batch: 31498, loss: 2.64727, acc: 0.95774, time: Sun Jan 23 22:53:51 2022
epoch 3 training:  16%|█▌        | 31597/196351 [4:29:47<23:28:26,  1.95it/s]global step 424300, epoch: 3, batch: 31598, loss: 0.45734, acc: 0.95781, time: Sun Jan 23 22:54:42 2022
epoch 3 training:  16%|█▌        | 31697/196351 [4:30:39<22:59:01,  1.99it/s]global step 424400, epoch: 3, batch: 31698, loss: 1.81312, acc: 0.95787, time: Sun Jan 23 22:55:33 2022
epoch 3 training:  16%|█▌        | 31797/196351 [4:31:30<23:10:29,  1.97it/s]global step 424500, epoch: 3, batch: 31798, loss: 2.61218, acc: 0.95789, time: Sun Jan 23 22:56:25 2022
epoch 3 training:  16%|█▌        | 31897/196351 [4:32:21<22:59:16,  1.99it/s]global step 424600, epoch: 3, batch: 31898, loss: 0.80465, acc: 0.95791, time: Sun Jan 23 22:57:16 2022
epoch 3 training:  16%|█▋        | 31997/196351 [4:33:12<23:20:54,  1.96it/s]global step 424700, epoch: 3, batch: 31998, loss: 0.23846, acc: 0.95795, time: Sun Jan 23 22:58:07 2022
epoch 3 training:  16%|█▋        | 32097/196351 [4:34:03<23:23:23,  1.95it/s]global step 424800, epoch: 3, batch: 32098, loss: 1.99301, acc: 0.95786, time: Sun Jan 23 22:58:58 2022
epoch 3 training:  16%|█▋        | 32197/196351 [4:34:54<23:18:33,  1.96it/s]global step 424900, epoch: 3, batch: 32198, loss: 1.37455, acc: 0.95787, time: Sun Jan 23 22:59:49 2022
epoch 3 training:  16%|█▋        | 32297/196351 [4:35:44<23:04:16,  1.98it/s]global step 425000, epoch: 3, batch: 32298, loss: 1.54999, acc: 0.95786, time: Sun Jan 23 23:00:39 2022
epoch 3 training:  16%|█▋        | 32397/196351 [4:36:35<22:58:17,  1.98it/s]global step 425100, epoch: 3, batch: 32398, loss: 1.03874, acc: 0.95781, time: Sun Jan 23 23:01:30 2022
epoch 3 training:  17%|█▋        | 32497/196351 [4:37:26<22:56:50,  1.98it/s]global step 425200, epoch: 3, batch: 32498, loss: 0.56579, acc: 0.95769, time: Sun Jan 23 23:02:21 2022
epoch 3 training:  17%|█▋        | 32597/196351 [4:38:16<23:10:49,  1.96it/s]global step 425300, epoch: 3, batch: 32598, loss: 1.74374, acc: 0.95765, time: Sun Jan 23 23:03:11 2022
epoch 3 training:  17%|█▋        | 32697/196351 [4:39:07<23:01:08,  1.97it/s]global step 425400, epoch: 3, batch: 32698, loss: 1.05241, acc: 0.95763, time: Sun Jan 23 23:04:02 2022
epoch 3 training:  17%|█▋        | 32797/196351 [4:39:58<23:25:11,  1.94it/s]global step 425500, epoch: 3, batch: 32798, loss: 2.51632, acc: 0.95765, time: Sun Jan 23 23:04:53 2022
epoch 3 training:  17%|█▋        | 32897/196351 [4:40:49<23:07:48,  1.96it/s]global step 425600, epoch: 3, batch: 32898, loss: 0.33096, acc: 0.95764, time: Sun Jan 23 23:05:44 2022
epoch 3 training:  17%|█▋        | 32997/196351 [4:41:41<23:11:13,  1.96it/s]global step 425700, epoch: 3, batch: 32998, loss: 0.19423, acc: 0.95774, time: Sun Jan 23 23:06:36 2022
epoch 3 training:  17%|█▋        | 33097/196351 [4:42:32<23:20:12,  1.94it/s]global step 425800, epoch: 3, batch: 33098, loss: 2.05440, acc: 0.95767, time: Sun Jan 23 23:07:27 2022
epoch 3 training:  17%|█▋        | 33197/196351 [4:43:23<23:04:50,  1.96it/s]global step 425900, epoch: 3, batch: 33198, loss: 0.89549, acc: 0.95766, time: Sun Jan 23 23:08:18 2022
epoch 3 training:  17%|█▋        | 33297/196351 [4:44:14<22:39:56,  2.00it/s]global step 426000, epoch: 3, batch: 33298, loss: 0.31677, acc: 0.95767, time: Sun Jan 23 23:09:09 2022
epoch 3 training:  17%|█▋        | 33397/196351 [4:45:05<23:20:17,  1.94it/s]global step 426100, epoch: 3, batch: 33398, loss: 0.35220, acc: 0.95766, time: Sun Jan 23 23:10:00 2022
epoch 3 training:  17%|█▋        | 33497/196351 [4:45:56<22:52:54,  1.98it/s]global step 426200, epoch: 3, batch: 33498, loss: 0.61288, acc: 0.95761, time: Sun Jan 23 23:10:51 2022
epoch 3 training:  17%|█▋        | 33597/196351 [4:46:47<22:48:22,  1.98it/s]global step 426300, epoch: 3, batch: 33598, loss: 3.71754, acc: 0.95759, time: Sun Jan 23 23:11:42 2022
epoch 3 training:  17%|█▋        | 33697/196351 [4:47:38<23:05:25,  1.96it/s]global step 426400, epoch: 3, batch: 33698, loss: 0.44655, acc: 0.95761, time: Sun Jan 23 23:12:33 2022
epoch 3 training:  17%|█▋        | 33797/196351 [4:48:29<23:04:25,  1.96it/s]global step 426500, epoch: 3, batch: 33798, loss: 0.86488, acc: 0.95769, time: Sun Jan 23 23:13:24 2022
epoch 3 training:  17%|█▋        | 33897/196351 [4:49:20<23:01:53,  1.96it/s]global step 426600, epoch: 3, batch: 33898, loss: 2.34433, acc: 0.95768, time: Sun Jan 23 23:14:15 2022
epoch 3 training:  17%|█▋        | 33997/196351 [4:50:11<23:12:03,  1.94it/s]global step 426700, epoch: 3, batch: 33998, loss: 0.96006, acc: 0.95760, time: Sun Jan 23 23:15:06 2022
epoch 3 training:  17%|█▋        | 34097/196351 [4:51:02<23:28:01,  1.92it/s]global step 426800, epoch: 3, batch: 34098, loss: 0.29494, acc: 0.95762, time: Sun Jan 23 23:15:57 2022
epoch 3 training:  17%|█▋        | 34197/196351 [4:51:53<23:23:18,  1.93it/s]global step 426900, epoch: 3, batch: 34198, loss: 1.52451, acc: 0.95754, time: Sun Jan 23 23:16:48 2022
epoch 3 training:  17%|█▋        | 34297/196351 [4:52:44<22:44:12,  1.98it/s]global step 427000, epoch: 3, batch: 34298, loss: 0.45449, acc: 0.95758, time: Sun Jan 23 23:17:39 2022
epoch 3 training:  18%|█▊        | 34397/196351 [4:53:35<23:05:02,  1.95it/s]global step 427100, epoch: 3, batch: 34398, loss: 2.92850, acc: 0.95757, time: Sun Jan 23 23:18:30 2022
epoch 3 training:  18%|█▊        | 34497/196351 [4:54:26<22:56:48,  1.96it/s]global step 427200, epoch: 3, batch: 34498, loss: 0.65669, acc: 0.95753, time: Sun Jan 23 23:19:21 2022
epoch 3 training:  18%|█▊        | 34597/196351 [4:55:18<22:56:30,  1.96it/s]global step 427300, epoch: 3, batch: 34598, loss: 0.59762, acc: 0.95756, time: Sun Jan 23 23:20:12 2022
epoch 3 training:  18%|█▊        | 34697/196351 [4:56:09<23:08:43,  1.94it/s]global step 427400, epoch: 3, batch: 34698, loss: 0.10971, acc: 0.95758, time: Sun Jan 23 23:21:03 2022
epoch 3 training:  18%|█▊        | 34797/196351 [4:57:00<22:46:01,  1.97it/s]global step 427500, epoch: 3, batch: 34798, loss: 0.33712, acc: 0.95756, time: Sun Jan 23 23:21:54 2022
epoch 3 training:  18%|█▊        | 34897/196351 [4:57:51<22:49:47,  1.96it/s]global step 427600, epoch: 3, batch: 34898, loss: 0.44411, acc: 0.95763, time: Sun Jan 23 23:22:46 2022
epoch 3 training:  18%|█▊        | 34997/196351 [4:58:42<22:46:12,  1.97it/s]global step 427700, epoch: 3, batch: 34998, loss: 0.35957, acc: 0.95763, time: Sun Jan 23 23:23:37 2022
epoch 3 training:  18%|█▊        | 35097/196351 [4:59:33<23:02:53,  1.94it/s]global step 427800, epoch: 3, batch: 35098, loss: 0.54470, acc: 0.95766, time: Sun Jan 23 23:24:28 2022
epoch 3 training:  18%|█▊        | 35197/196351 [5:00:25<22:58:23,  1.95it/s]global step 427900, epoch: 3, batch: 35198, loss: 0.12062, acc: 0.95764, time: Sun Jan 23 23:25:20 2022
epoch 3 training:  18%|█▊        | 35297/196351 [5:01:16<23:05:56,  1.94it/s]global step 428000, epoch: 3, batch: 35298, loss: 0.67347, acc: 0.95762, time: Sun Jan 23 23:26:11 2022
epoch 3 training:  18%|█▊        | 35397/196351 [5:02:07<22:46:47,  1.96it/s]global step 428100, epoch: 3, batch: 35398, loss: 0.71785, acc: 0.95770, time: Sun Jan 23 23:27:02 2022
epoch 3 training:  18%|█▊        | 35497/196351 [5:02:58<23:12:25,  1.93it/s]global step 428200, epoch: 3, batch: 35498, loss: 0.15458, acc: 0.95769, time: Sun Jan 23 23:27:53 2022
epoch 3 training:  18%|█▊        | 35597/196351 [5:03:50<23:20:27,  1.91it/s]global step 428300, epoch: 3, batch: 35598, loss: 0.43219, acc: 0.95765, time: Sun Jan 23 23:28:45 2022
epoch 3 training:  18%|█▊        | 35697/196351 [5:04:42<22:56:03,  1.95it/s]global step 428400, epoch: 3, batch: 35698, loss: 2.08634, acc: 0.95763, time: Sun Jan 23 23:29:37 2022
epoch 3 training:  18%|█▊        | 35797/196351 [5:05:33<22:38:38,  1.97it/s]global step 428500, epoch: 3, batch: 35798, loss: 0.09197, acc: 0.95764, time: Sun Jan 23 23:30:28 2022
epoch 3 training:  18%|█▊        | 35897/196351 [5:06:24<22:43:32,  1.96it/s]global step 428600, epoch: 3, batch: 35898, loss: 2.66375, acc: 0.95759, time: Sun Jan 23 23:31:19 2022
epoch 3 training:  18%|█▊        | 35997/196351 [5:07:15<22:50:16,  1.95it/s]global step 428700, epoch: 3, batch: 35998, loss: 0.24043, acc: 0.95753, time: Sun Jan 23 23:32:10 2022
epoch 3 training:  18%|█▊        | 36097/196351 [5:08:06<22:31:51,  1.98it/s]global step 428800, epoch: 3, batch: 36098, loss: 0.39336, acc: 0.95756, time: Sun Jan 23 23:33:01 2022
epoch 3 training:  18%|█▊        | 36197/196351 [5:08:57<22:50:05,  1.95it/s]global step 428900, epoch: 3, batch: 36198, loss: 0.45947, acc: 0.95755, time: Sun Jan 23 23:33:52 2022
epoch 3 training:  18%|█▊        | 36297/196351 [5:09:48<22:29:35,  1.98it/s]global step 429000, epoch: 3, batch: 36298, loss: 3.36195, acc: 0.95756, time: Sun Jan 23 23:34:43 2022
epoch 3 training:  19%|█▊        | 36397/196351 [5:10:40<22:47:25,  1.95it/s]global step 429100, epoch: 3, batch: 36398, loss: 2.26659, acc: 0.95755, time: Sun Jan 23 23:35:34 2022
epoch 3 training:  19%|█▊        | 36497/196351 [5:11:31<22:49:47,  1.95it/s]global step 429200, epoch: 3, batch: 36498, loss: 0.46135, acc: 0.95755, time: Sun Jan 23 23:36:26 2022
epoch 3 training:  19%|█▊        | 36597/196351 [5:12:22<23:05:23,  1.92it/s]global step 429300, epoch: 3, batch: 36598, loss: 0.89358, acc: 0.95758, time: Sun Jan 23 23:37:17 2022
epoch 3 training:  19%|█▊        | 36697/196351 [5:13:13<22:18:10,  1.99it/s]global step 429400, epoch: 3, batch: 36698, loss: 1.26189, acc: 0.95760, time: Sun Jan 23 23:38:08 2022
epoch 3 training:  19%|█▊        | 36797/196351 [5:14:04<22:53:57,  1.94it/s]global step 429500, epoch: 3, batch: 36798, loss: 0.67290, acc: 0.95761, time: Sun Jan 23 23:38:59 2022
epoch 3 training:  19%|█▉        | 36897/196351 [5:14:55<22:42:02,  1.95it/s]global step 429600, epoch: 3, batch: 36898, loss: 0.48730, acc: 0.95760, time: Sun Jan 23 23:39:50 2022
epoch 3 training:  19%|█▉        | 36997/196351 [5:15:46<22:31:10,  1.97it/s]global step 429700, epoch: 3, batch: 36998, loss: 0.58274, acc: 0.95759, time: Sun Jan 23 23:40:41 2022
epoch 3 training:  19%|█▉        | 37097/196351 [5:16:37<22:05:24,  2.00it/s]global step 429800, epoch: 3, batch: 37098, loss: 0.69574, acc: 0.95759, time: Sun Jan 23 23:41:32 2022
epoch 3 training:  19%|█▉        | 37197/196351 [5:17:28<22:48:55,  1.94it/s]global step 429900, epoch: 3, batch: 37198, loss: 0.15455, acc: 0.95761, time: Sun Jan 23 23:42:22 2022
epoch 3 training:  19%|█▉        | 37297/196351 [5:18:19<22:14:45,  1.99it/s]global step 430000, epoch: 3, batch: 37298, loss: 0.22015, acc: 0.95763, time: Sun Jan 23 23:43:13 2022
epoch 3 training:  19%|█▉        | 37397/196351 [5:19:10<22:40:52,  1.95it/s]global step 430100, epoch: 3, batch: 37398, loss: 0.24017, acc: 0.95768, time: Sun Jan 23 23:44:05 2022
epoch 3 training:  19%|█▉        | 37497/196351 [5:20:02<22:53:54,  1.93it/s]global step 430200, epoch: 3, batch: 37498, loss: 0.67847, acc: 0.95776, time: Sun Jan 23 23:44:57 2022
epoch 3 training:  19%|█▉        | 37597/196351 [5:20:53<22:52:34,  1.93it/s]global step 430300, epoch: 3, batch: 37598, loss: 0.46401, acc: 0.95778, time: Sun Jan 23 23:45:48 2022
epoch 3 training:  19%|█▉        | 37697/196351 [5:21:45<22:35:40,  1.95it/s]global step 430400, epoch: 3, batch: 37698, loss: 0.31764, acc: 0.95781, time: Sun Jan 23 23:46:39 2022
epoch 3 training:  19%|█▉        | 37797/196351 [5:22:36<22:43:16,  1.94it/s]global step 430500, epoch: 3, batch: 37798, loss: 2.11197, acc: 0.95785, time: Sun Jan 23 23:47:31 2022
epoch 3 training:  19%|█▉        | 37897/196351 [5:23:28<22:40:50,  1.94it/s]global step 430600, epoch: 3, batch: 37898, loss: 0.64499, acc: 0.95778, time: Sun Jan 23 23:48:22 2022
epoch 3 training:  19%|█▉        | 37997/196351 [5:24:19<22:32:53,  1.95it/s]global step 430700, epoch: 3, batch: 37998, loss: 0.75862, acc: 0.95776, time: Sun Jan 23 23:49:14 2022
epoch 3 training:  19%|█▉        | 38097/196351 [5:25:11<22:13:15,  1.98it/s]global step 430800, epoch: 3, batch: 38098, loss: 1.21918, acc: 0.95781, time: Sun Jan 23 23:50:05 2022
epoch 3 training:  19%|█▉        | 38197/196351 [5:26:01<22:21:07,  1.97it/s]global step 430900, epoch: 3, batch: 38198, loss: 0.67342, acc: 0.95777, time: Sun Jan 23 23:50:56 2022
epoch 3 training:  20%|█▉        | 38297/196351 [5:26:53<22:26:36,  1.96it/s]global step 431000, epoch: 3, batch: 38298, loss: 0.26746, acc: 0.95775, time: Sun Jan 23 23:51:48 2022
epoch 3 training:  20%|█▉        | 38397/196351 [5:27:44<22:06:30,  1.98it/s]global step 431100, epoch: 3, batch: 38398, loss: 1.49260, acc: 0.95775, time: Sun Jan 23 23:52:39 2022
epoch 3 training:  20%|█▉        | 38497/196351 [5:28:36<22:35:20,  1.94it/s]global step 431200, epoch: 3, batch: 38498, loss: 1.25071, acc: 0.95775, time: Sun Jan 23 23:53:30 2022
epoch 3 training:  20%|█▉        | 38597/196351 [5:29:27<22:31:02,  1.95it/s]global step 431300, epoch: 3, batch: 38598, loss: 0.19910, acc: 0.95773, time: Sun Jan 23 23:54:22 2022
epoch 3 training:  20%|█▉        | 38697/196351 [5:30:18<22:27:24,  1.95it/s]global step 431400, epoch: 3, batch: 38698, loss: 0.50556, acc: 0.95779, time: Sun Jan 23 23:55:13 2022
epoch 3 training:  20%|█▉        | 38797/196351 [5:31:09<22:06:09,  1.98it/s]global step 431500, epoch: 3, batch: 38798, loss: 1.03914, acc: 0.95779, time: Sun Jan 23 23:56:04 2022
epoch 3 training:  20%|█▉        | 38897/196351 [5:32:01<22:39:06,  1.93it/s]global step 431600, epoch: 3, batch: 38898, loss: 4.34571, acc: 0.95772, time: Sun Jan 23 23:56:55 2022
epoch 3 training:  20%|█▉        | 38997/196351 [5:32:52<22:03:17,  1.98it/s]global step 431700, epoch: 3, batch: 38998, loss: 3.98203, acc: 0.95770, time: Sun Jan 23 23:57:46 2022
epoch 3 training:  20%|█▉        | 39097/196351 [5:33:43<22:07:42,  1.97it/s]global step 431800, epoch: 3, batch: 39098, loss: 0.30935, acc: 0.95764, time: Sun Jan 23 23:58:38 2022
epoch 3 training:  20%|█▉        | 39197/196351 [5:34:34<22:03:44,  1.98it/s]global step 431900, epoch: 3, batch: 39198, loss: 2.47283, acc: 0.95761, time: Sun Jan 23 23:59:29 2022
epoch 3 training:  20%|██        | 39297/196351 [5:35:24<22:22:18,  1.95it/s]global step 432000, epoch: 3, batch: 39298, loss: 2.00349, acc: 0.95768, time: Mon Jan 24 00:00:19 2022
epoch 3 training:  20%|██        | 39397/196351 [5:36:16<22:12:15,  1.96it/s]global step 432100, epoch: 3, batch: 39398, loss: 0.33653, acc: 0.95765, time: Mon Jan 24 00:01:11 2022
epoch 3 training:  20%|██        | 39497/196351 [5:37:06<22:24:18,  1.94it/s]global step 432200, epoch: 3, batch: 39498, loss: 2.68367, acc: 0.95768, time: Mon Jan 24 00:02:01 2022
epoch 3 training:  20%|██        | 39597/196351 [5:37:57<22:14:37,  1.96it/s]global step 432300, epoch: 3, batch: 39598, loss: 1.29882, acc: 0.95764, time: Mon Jan 24 00:02:52 2022
epoch 3 training:  20%|██        | 39697/196351 [5:38:48<22:08:03,  1.97it/s]global step 432400, epoch: 3, batch: 39698, loss: 0.63153, acc: 0.95759, time: Mon Jan 24 00:03:43 2022
epoch 3 training:  20%|██        | 39797/196351 [5:39:39<22:48:41,  1.91it/s]global step 432500, epoch: 3, batch: 39798, loss: 0.51524, acc: 0.95755, time: Mon Jan 24 00:04:34 2022
epoch 3 training:  20%|██        | 39897/196351 [5:40:30<22:16:42,  1.95it/s]global step 432600, epoch: 3, batch: 39898, loss: 0.04771, acc: 0.95754, time: Mon Jan 24 00:05:25 2022
epoch 3 training:  20%|██        | 39997/196351 [5:41:22<22:23:45,  1.94it/s]global step 432700, epoch: 3, batch: 39998, loss: 2.14152, acc: 0.95756, time: Mon Jan 24 00:06:17 2022
epoch 3 training:  20%|██        | 40097/196351 [5:42:13<22:24:22,  1.94it/s]global step 432800, epoch: 3, batch: 40098, loss: 1.07295, acc: 0.95750, time: Mon Jan 24 00:07:07 2022
epoch 3 training:  20%|██        | 40197/196351 [5:43:04<21:55:11,  1.98it/s]global step 432900, epoch: 3, batch: 40198, loss: 0.56333, acc: 0.95751, time: Mon Jan 24 00:07:59 2022
epoch 3 training:  21%|██        | 40297/196351 [5:43:55<22:18:18,  1.94it/s]global step 433000, epoch: 3, batch: 40298, loss: 1.08021, acc: 0.95755, time: Mon Jan 24 00:08:50 2022
epoch 3 training:  21%|██        | 40397/196351 [5:44:46<22:12:25,  1.95it/s]global step 433100, epoch: 3, batch: 40398, loss: 0.67784, acc: 0.95749, time: Mon Jan 24 00:09:41 2022
epoch 3 training:  21%|██        | 40497/196351 [5:45:37<22:04:32,  1.96it/s]global step 433200, epoch: 3, batch: 40498, loss: 2.21727, acc: 0.95752, time: Mon Jan 24 00:10:32 2022
epoch 3 training:  21%|██        | 40597/196351 [5:46:29<22:36:04,  1.91it/s]global step 433300, epoch: 3, batch: 40598, loss: 1.51427, acc: 0.95756, time: Mon Jan 24 00:11:24 2022
epoch 3 training:  21%|██        | 40697/196351 [5:47:20<21:44:12,  1.99it/s]global step 433400, epoch: 3, batch: 40698, loss: 2.59184, acc: 0.95754, time: Mon Jan 24 00:12:15 2022
epoch 3 training:  21%|██        | 40797/196351 [5:48:11<22:24:42,  1.93it/s]global step 433500, epoch: 3, batch: 40798, loss: 1.96651, acc: 0.95757, time: Mon Jan 24 00:13:06 2022
epoch 3 training:  21%|██        | 40897/196351 [5:49:03<22:01:30,  1.96it/s]global step 433600, epoch: 3, batch: 40898, loss: 0.23400, acc: 0.95758, time: Mon Jan 24 00:13:58 2022
epoch 3 training:  21%|██        | 40997/196351 [5:49:54<21:51:33,  1.97it/s]global step 433700, epoch: 3, batch: 40998, loss: 0.49195, acc: 0.95760, time: Mon Jan 24 00:14:49 2022
epoch 3 training:  21%|██        | 41097/196351 [5:50:45<21:50:18,  1.97it/s]global step 433800, epoch: 3, batch: 41098, loss: 0.66827, acc: 0.95760, time: Mon Jan 24 00:15:40 2022
epoch 3 training:  21%|██        | 41197/196351 [5:51:37<21:56:36,  1.96it/s]global step 433900, epoch: 3, batch: 41198, loss: 0.39440, acc: 0.95762, time: Mon Jan 24 00:16:32 2022
epoch 3 training:  21%|██        | 41297/196351 [5:52:28<21:38:16,  1.99it/s]global step 434000, epoch: 3, batch: 41298, loss: 1.02910, acc: 0.95765, time: Mon Jan 24 00:17:22 2022
epoch 3 training:  21%|██        | 41397/196351 [5:53:21<22:33:04,  1.91it/s]global step 434100, epoch: 3, batch: 41398, loss: 0.73705, acc: 0.95762, time: Mon Jan 24 00:18:16 2022
epoch 3 training:  21%|██        | 41497/196351 [5:54:13<22:09:18,  1.94it/s]global step 434200, epoch: 3, batch: 41498, loss: 1.22010, acc: 0.95759, time: Mon Jan 24 00:19:08 2022
epoch 3 training:  21%|██        | 41597/196351 [5:55:05<22:40:54,  1.90it/s]global step 434300, epoch: 3, batch: 41598, loss: 0.22177, acc: 0.95751, time: Mon Jan 24 00:20:00 2022
epoch 3 training:  21%|██        | 41697/196351 [5:55:57<22:42:49,  1.89it/s]global step 434400, epoch: 3, batch: 41698, loss: 0.67142, acc: 0.95754, time: Mon Jan 24 00:20:52 2022
epoch 3 training:  21%|██▏       | 41797/196351 [5:56:49<22:34:05,  1.90it/s]global step 434500, epoch: 3, batch: 41798, loss: 0.54840, acc: 0.95749, time: Mon Jan 24 00:21:44 2022
epoch 3 training:  21%|██▏       | 41897/196351 [5:57:40<21:46:06,  1.97it/s]global step 434600, epoch: 3, batch: 41898, loss: 0.31449, acc: 0.95748, time: Mon Jan 24 00:22:35 2022
epoch 3 training:  21%|██▏       | 41997/196351 [5:58:32<21:52:12,  1.96it/s]global step 434700, epoch: 3, batch: 41998, loss: 0.27069, acc: 0.95753, time: Mon Jan 24 00:23:26 2022
epoch 3 training:  21%|██▏       | 42097/196351 [5:59:22<21:59:20,  1.95it/s]global step 434800, epoch: 3, batch: 42098, loss: 0.36379, acc: 0.95753, time: Mon Jan 24 00:24:17 2022
epoch 3 training:  21%|██▏       | 42197/196351 [6:00:13<21:56:10,  1.95it/s]global step 434900, epoch: 3, batch: 42198, loss: 3.58809, acc: 0.95756, time: Mon Jan 24 00:25:08 2022
epoch 3 training:  22%|██▏       | 42297/196351 [6:01:05<22:13:38,  1.93it/s]global step 435000, epoch: 3, batch: 42298, loss: 0.95379, acc: 0.95760, time: Mon Jan 24 00:25:59 2022
epoch 3 training:  22%|██▏       | 42397/196351 [6:01:56<22:03:31,  1.94it/s]global step 435100, epoch: 3, batch: 42398, loss: 0.32382, acc: 0.95759, time: Mon Jan 24 00:26:51 2022
epoch 3 training:  22%|██▏       | 42497/196351 [6:02:49<22:55:41,  1.86it/s]global step 435200, epoch: 3, batch: 42498, loss: 0.17177, acc: 0.95760, time: Mon Jan 24 00:27:43 2022
epoch 3 training:  22%|██▏       | 42597/196351 [6:03:41<22:30:11,  1.90it/s]global step 435300, epoch: 3, batch: 42598, loss: 3.19904, acc: 0.95751, time: Mon Jan 24 00:28:36 2022
epoch 3 training:  22%|██▏       | 42697/196351 [6:04:33<22:10:53,  1.92it/s]global step 435400, epoch: 3, batch: 42698, loss: 0.50586, acc: 0.95752, time: Mon Jan 24 00:29:28 2022
epoch 3 training:  22%|██▏       | 42797/196351 [6:05:25<21:59:04,  1.94it/s]global step 435500, epoch: 3, batch: 42798, loss: 0.18739, acc: 0.95752, time: Mon Jan 24 00:30:19 2022
epoch 3 training:  22%|██▏       | 42897/196351 [6:06:16<22:01:39,  1.94it/s]global step 435600, epoch: 3, batch: 42898, loss: 1.29179, acc: 0.95747, time: Mon Jan 24 00:31:11 2022
epoch 3 training:  22%|██▏       | 42997/196351 [6:07:08<22:01:42,  1.93it/s]global step 435700, epoch: 3, batch: 42998, loss: 0.91830, acc: 0.95751, time: Mon Jan 24 00:32:03 2022
epoch 3 training:  22%|██▏       | 43097/196351 [6:08:00<22:23:51,  1.90it/s]global step 435800, epoch: 3, batch: 43098, loss: 0.64943, acc: 0.95756, time: Mon Jan 24 00:32:55 2022
epoch 3 training:  22%|██▏       | 43197/196351 [6:08:53<22:32:20,  1.89it/s]global step 435900, epoch: 3, batch: 43198, loss: 1.73565, acc: 0.95759, time: Mon Jan 24 00:33:47 2022
epoch 3 training:  22%|██▏       | 43297/196351 [6:09:45<22:05:13,  1.92it/s]global step 436000, epoch: 3, batch: 43298, loss: 1.76257, acc: 0.95761, time: Mon Jan 24 00:34:40 2022
epoch 3 training:  22%|██▏       | 43397/196351 [6:10:37<22:08:30,  1.92it/s]global step 436100, epoch: 3, batch: 43398, loss: 2.01062, acc: 0.95758, time: Mon Jan 24 00:35:32 2022
epoch 3 training:  22%|██▏       | 43497/196351 [6:11:29<22:16:29,  1.91it/s]global step 436200, epoch: 3, batch: 43498, loss: 1.09092, acc: 0.95760, time: Mon Jan 24 00:36:24 2022
epoch 3 training:  22%|██▏       | 43597/196351 [6:12:21<21:44:55,  1.95it/s]global step 436300, epoch: 3, batch: 43598, loss: 0.50776, acc: 0.95760, time: Mon Jan 24 00:37:15 2022
epoch 3 training:  22%|██▏       | 43697/196351 [6:13:13<22:43:41,  1.87it/s]global step 436400, epoch: 3, batch: 43698, loss: 1.36078, acc: 0.95761, time: Mon Jan 24 00:38:08 2022
epoch 3 training:  22%|██▏       | 43797/196351 [6:14:05<22:18:45,  1.90it/s]global step 436500, epoch: 3, batch: 43798, loss: 1.48943, acc: 0.95753, time: Mon Jan 24 00:39:00 2022
epoch 3 training:  22%|██▏       | 43897/196351 [6:14:57<22:12:25,  1.91it/s]global step 436600, epoch: 3, batch: 43898, loss: 0.26678, acc: 0.95757, time: Mon Jan 24 00:39:52 2022
epoch 3 training:  22%|██▏       | 43997/196351 [6:15:49<21:52:43,  1.93it/s]global step 436700, epoch: 3, batch: 43998, loss: 0.47329, acc: 0.95754, time: Mon Jan 24 00:40:44 2022
epoch 3 training:  22%|██▏       | 44097/196351 [6:16:41<21:58:08,  1.93it/s]global step 436800, epoch: 3, batch: 44098, loss: 3.27402, acc: 0.95757, time: Mon Jan 24 00:41:36 2022
epoch 3 training:  23%|██▎       | 44197/196351 [6:17:33<21:50:14,  1.94it/s]global step 436900, epoch: 3, batch: 44198, loss: 1.31217, acc: 0.95752, time: Mon Jan 24 00:42:28 2022
epoch 3 training:  23%|██▎       | 44297/196351 [6:18:25<22:01:00,  1.92it/s]global step 437000, epoch: 3, batch: 44298, loss: 1.51957, acc: 0.95754, time: Mon Jan 24 00:43:20 2022
epoch 3 training:  23%|██▎       | 44397/196351 [6:19:17<21:53:42,  1.93it/s]global step 437100, epoch: 3, batch: 44398, loss: 1.70214, acc: 0.95749, time: Mon Jan 24 00:44:12 2022
epoch 3 training:  23%|██▎       | 44497/196351 [6:20:10<22:09:52,  1.90it/s]global step 437200, epoch: 3, batch: 44498, loss: 0.20218, acc: 0.95745, time: Mon Jan 24 00:45:04 2022
epoch 3 training:  23%|██▎       | 44597/196351 [6:21:02<21:27:20,  1.96it/s]global step 437300, epoch: 3, batch: 44598, loss: 1.54370, acc: 0.95743, time: Mon Jan 24 00:45:56 2022
epoch 3 training:  23%|██▎       | 44697/196351 [6:21:54<22:10:37,  1.90it/s]global step 437400, epoch: 3, batch: 44698, loss: 0.54064, acc: 0.95740, time: Mon Jan 24 00:46:49 2022
epoch 3 training:  23%|██▎       | 44797/196351 [6:22:46<21:55:12,  1.92it/s]global step 437500, epoch: 3, batch: 44798, loss: 0.83823, acc: 0.95741, time: Mon Jan 24 00:47:41 2022
epoch 3 training:  23%|██▎       | 44897/196351 [6:23:38<21:56:22,  1.92it/s]global step 437600, epoch: 3, batch: 44898, loss: 0.91021, acc: 0.95740, time: Mon Jan 24 00:48:33 2022
epoch 3 training:  23%|██▎       | 44997/196351 [6:24:30<21:32:48,  1.95it/s]global step 437700, epoch: 3, batch: 44998, loss: 2.55302, acc: 0.95742, time: Mon Jan 24 00:49:25 2022
epoch 3 training:  23%|██▎       | 45097/196351 [6:25:23<22:30:45,  1.87it/s]global step 437800, epoch: 3, batch: 45098, loss: 0.60403, acc: 0.95747, time: Mon Jan 24 00:50:17 2022
epoch 3 training:  23%|██▎       | 45197/196351 [6:26:15<21:59:33,  1.91it/s]global step 437900, epoch: 3, batch: 45198, loss: 1.77470, acc: 0.95746, time: Mon Jan 24 00:51:10 2022
epoch 3 training:  23%|██▎       | 45297/196351 [6:27:07<21:47:46,  1.93it/s]global step 438000, epoch: 3, batch: 45298, loss: 0.57672, acc: 0.95750, time: Mon Jan 24 00:52:02 2022
epoch 3 training:  23%|██▎       | 45397/196351 [6:27:59<21:26:31,  1.96it/s]global step 438100, epoch: 3, batch: 45398, loss: 1.39442, acc: 0.95749, time: Mon Jan 24 00:52:54 2022
epoch 3 training:  23%|██▎       | 45497/196351 [6:28:51<21:50:32,  1.92it/s]global step 438200, epoch: 3, batch: 45498, loss: 0.30752, acc: 0.95753, time: Mon Jan 24 00:53:46 2022
epoch 3 training:  23%|██▎       | 45597/196351 [6:29:44<21:38:13,  1.94it/s]global step 438300, epoch: 3, batch: 45598, loss: 0.07917, acc: 0.95752, time: Mon Jan 24 00:54:39 2022
epoch 3 training:  23%|██▎       | 45697/196351 [6:30:36<21:36:05,  1.94it/s]global step 438400, epoch: 3, batch: 45698, loss: 1.01686, acc: 0.95749, time: Mon Jan 24 00:55:31 2022
epoch 3 training:  23%|██▎       | 45797/196351 [6:31:28<21:46:14,  1.92it/s]global step 438500, epoch: 3, batch: 45798, loss: 2.95260, acc: 0.95751, time: Mon Jan 24 00:56:23 2022
epoch 3 training:  23%|██▎       | 45897/196351 [6:32:20<21:54:36,  1.91it/s]global step 438600, epoch: 3, batch: 45898, loss: 0.43411, acc: 0.95749, time: Mon Jan 24 00:57:14 2022
epoch 3 training:  23%|██▎       | 45997/196351 [6:33:12<21:55:23,  1.91it/s]global step 438700, epoch: 3, batch: 45998, loss: 2.66118, acc: 0.95750, time: Mon Jan 24 00:58:07 2022
epoch 3 training:  23%|██▎       | 46097/196351 [6:34:04<21:40:01,  1.93it/s]global step 438800, epoch: 3, batch: 46098, loss: 0.86252, acc: 0.95751, time: Mon Jan 24 00:58:59 2022
epoch 3 training:  24%|██▎       | 46197/196351 [6:34:56<21:19:01,  1.96it/s]global step 438900, epoch: 3, batch: 46198, loss: 0.05361, acc: 0.95749, time: Mon Jan 24 00:59:51 2022
epoch 3 training:  24%|██▎       | 46297/196351 [6:35:48<21:44:17,  1.92it/s]global step 439000, epoch: 3, batch: 46298, loss: 1.07973, acc: 0.95750, time: Mon Jan 24 01:00:43 2022
epoch 3 training:  24%|██▎       | 46397/196351 [6:36:40<22:07:03,  1.88it/s]global step 439100, epoch: 3, batch: 46398, loss: 1.29291, acc: 0.95750, time: Mon Jan 24 01:01:35 2022
epoch 3 training:  24%|██▎       | 46497/196351 [6:37:32<21:45:19,  1.91it/s]global step 439200, epoch: 3, batch: 46498, loss: 0.19969, acc: 0.95749, time: Mon Jan 24 01:02:27 2022
epoch 3 training:  24%|██▎       | 46597/196351 [6:38:23<21:05:22,  1.97it/s]global step 439300, epoch: 3, batch: 46598, loss: 0.43411, acc: 0.95751, time: Mon Jan 24 01:03:18 2022
epoch 3 training:  24%|██▍       | 46697/196351 [6:39:14<21:22:13,  1.95it/s]global step 439400, epoch: 3, batch: 46698, loss: 0.17444, acc: 0.95752, time: Mon Jan 24 01:04:09 2022
epoch 3 training:  24%|██▍       | 46797/196351 [6:40:05<20:50:47,  1.99it/s]global step 439500, epoch: 3, batch: 46798, loss: 0.56749, acc: 0.95753, time: Mon Jan 24 01:05:00 2022
epoch 3 training:  24%|██▍       | 46897/196351 [6:40:55<20:50:03,  1.99it/s]global step 439600, epoch: 3, batch: 46898, loss: 1.56488, acc: 0.95756, time: Mon Jan 24 01:05:50 2022
epoch 3 training:  24%|██▍       | 46997/196351 [6:41:46<20:57:13,  1.98it/s]global step 439700, epoch: 3, batch: 46998, loss: 0.82239, acc: 0.95760, time: Mon Jan 24 01:06:41 2022
epoch 3 training:  24%|██▍       | 47097/196351 [6:42:37<20:56:16,  1.98it/s]global step 439800, epoch: 3, batch: 47098, loss: 0.37110, acc: 0.95759, time: Mon Jan 24 01:07:31 2022
epoch 3 training:  24%|██▍       | 47197/196351 [6:43:28<21:18:35,  1.94it/s]global step 439900, epoch: 3, batch: 47198, loss: 1.10729, acc: 0.95760, time: Mon Jan 24 01:08:23 2022
epoch 3 training:  24%|██▍       | 47297/196351 [6:44:19<21:06:47,  1.96it/s]global step 440000, epoch: 3, batch: 47298, loss: 0.62064, acc: 0.95762, time: Mon Jan 24 01:09:13 2022
epoch 3 training:  24%|██▍       | 47397/196351 [6:45:09<21:13:48,  1.95it/s]global step 440100, epoch: 3, batch: 47398, loss: 1.31825, acc: 0.95762, time: Mon Jan 24 01:10:04 2022
epoch 3 training:  24%|██▍       | 47497/196351 [6:46:00<20:44:43,  1.99it/s]global step 440200, epoch: 3, batch: 47498, loss: 1.95710, acc: 0.95761, time: Mon Jan 24 01:10:55 2022
epoch 3 training:  24%|██▍       | 47597/196351 [6:46:51<20:55:22,  1.97it/s]global step 440300, epoch: 3, batch: 47598, loss: 0.32574, acc: 0.95757, time: Mon Jan 24 01:11:46 2022
epoch 3 training:  24%|██▍       | 47697/196351 [6:47:41<21:09:02,  1.95it/s]global step 440400, epoch: 3, batch: 47698, loss: 0.52758, acc: 0.95761, time: Mon Jan 24 01:12:36 2022
epoch 3 training:  24%|██▍       | 47797/196351 [6:48:32<20:42:05,  1.99it/s]global step 440500, epoch: 3, batch: 47798, loss: 1.45461, acc: 0.95762, time: Mon Jan 24 01:13:27 2022
epoch 3 training:  24%|██▍       | 47897/196351 [6:49:23<20:53:39,  1.97it/s]global step 440600, epoch: 3, batch: 47898, loss: 2.47209, acc: 0.95760, time: Mon Jan 24 01:14:18 2022
epoch 3 training:  24%|██▍       | 47997/196351 [6:50:14<21:12:59,  1.94it/s]global step 440700, epoch: 3, batch: 47998, loss: 0.47964, acc: 0.95760, time: Mon Jan 24 01:15:09 2022
epoch 3 training:  24%|██▍       | 48097/196351 [6:51:05<20:46:16,  1.98it/s]global step 440800, epoch: 3, batch: 48098, loss: 0.96279, acc: 0.95761, time: Mon Jan 24 01:16:00 2022
epoch 3 training:  25%|██▍       | 48197/196351 [6:51:56<21:20:36,  1.93it/s]global step 440900, epoch: 3, batch: 48198, loss: 0.72715, acc: 0.95758, time: Mon Jan 24 01:16:50 2022
epoch 3 training:  25%|██▍       | 48297/196351 [6:52:47<21:16:22,  1.93it/s]global step 441000, epoch: 3, batch: 48298, loss: 1.32171, acc: 0.95762, time: Mon Jan 24 01:17:42 2022
epoch 3 training:  25%|██▍       | 48397/196351 [6:53:38<21:00:57,  1.96it/s]global step 441100, epoch: 3, batch: 48398, loss: 0.59807, acc: 0.95757, time: Mon Jan 24 01:18:33 2022
epoch 3 training:  25%|██▍       | 48497/196351 [6:54:29<20:45:30,  1.98it/s]global step 441200, epoch: 3, batch: 48498, loss: 0.37513, acc: 0.95761, time: Mon Jan 24 01:19:24 2022
epoch 3 training:  25%|██▍       | 48597/196351 [6:55:20<20:31:34,  2.00it/s]global step 441300, epoch: 3, batch: 48598, loss: 0.65125, acc: 0.95755, time: Mon Jan 24 01:20:15 2022
epoch 3 training:  25%|██▍       | 48697/196351 [6:56:11<20:39:11,  1.99it/s]global step 441400, epoch: 3, batch: 48698, loss: 1.14905, acc: 0.95759, time: Mon Jan 24 01:21:06 2022
epoch 3 training:  25%|██▍       | 48797/196351 [6:57:02<21:05:45,  1.94it/s]global step 441500, epoch: 3, batch: 48798, loss: 2.18051, acc: 0.95761, time: Mon Jan 24 01:21:57 2022
epoch 3 training:  25%|██▍       | 48897/196351 [6:57:53<20:53:32,  1.96it/s]global step 441600, epoch: 3, batch: 48898, loss: 2.11474, acc: 0.95759, time: Mon Jan 24 01:22:48 2022
epoch 3 training:  25%|██▍       | 48997/196351 [6:58:44<20:53:10,  1.96it/s]global step 441700, epoch: 3, batch: 48998, loss: 1.70453, acc: 0.95756, time: Mon Jan 24 01:23:39 2022
epoch 3 training:  25%|██▌       | 49097/196351 [6:59:35<20:56:17,  1.95it/s]global step 441800, epoch: 3, batch: 49098, loss: 2.99350, acc: 0.95755, time: Mon Jan 24 01:24:30 2022
epoch 3 training:  25%|██▌       | 49197/196351 [7:00:26<21:03:08,  1.94it/s]global step 441900, epoch: 3, batch: 49198, loss: 1.70529, acc: 0.95757, time: Mon Jan 24 01:25:21 2022
epoch 3 training:  25%|██▌       | 49297/196351 [7:01:17<20:48:40,  1.96it/s]global step 442000, epoch: 3, batch: 49298, loss: 0.26784, acc: 0.95754, time: Mon Jan 24 01:26:12 2022
epoch 3 training:  25%|██▌       | 49397/196351 [7:02:08<20:40:18,  1.97it/s]global step 442100, epoch: 3, batch: 49398, loss: 1.10690, acc: 0.95759, time: Mon Jan 24 01:27:03 2022
epoch 3 training:  25%|██▌       | 49497/196351 [7:02:59<20:40:40,  1.97it/s]global step 442200, epoch: 3, batch: 49498, loss: 0.76951, acc: 0.95762, time: Mon Jan 24 01:27:54 2022
epoch 3 training:  25%|██▌       | 49597/196351 [7:03:50<20:33:26,  1.98it/s]global step 442300, epoch: 3, batch: 49598, loss: 0.67348, acc: 0.95765, time: Mon Jan 24 01:28:45 2022
epoch 3 training:  25%|██▌       | 49697/196351 [7:04:41<20:50:34,  1.95it/s]global step 442400, epoch: 3, batch: 49698, loss: 0.56054, acc: 0.95760, time: Mon Jan 24 01:29:35 2022
epoch 3 training:  25%|██▌       | 49797/196351 [7:05:32<20:41:52,  1.97it/s]global step 442500, epoch: 3, batch: 49798, loss: 0.32618, acc: 0.95761, time: Mon Jan 24 01:30:27 2022
epoch 3 training:  25%|██▌       | 49897/196351 [7:06:23<20:55:20,  1.94it/s]global step 442600, epoch: 3, batch: 49898, loss: 1.86667, acc: 0.95760, time: Mon Jan 24 01:31:17 2022
epoch 3 training:  25%|██▌       | 49997/196351 [7:07:14<20:48:24,  1.95it/s]global step 442700, epoch: 3, batch: 49998, loss: 1.66257, acc: 0.95755, time: Mon Jan 24 01:32:09 2022
epoch 3 training:  26%|██▌       | 50097/196351 [7:08:05<20:34:58,  1.97it/s]global step 442800, epoch: 3, batch: 50098, loss: 0.66488, acc: 0.95756, time: Mon Jan 24 01:33:00 2022
epoch 3 training:  26%|██▌       | 50197/196351 [7:08:56<20:25:00,  1.99it/s]global step 442900, epoch: 3, batch: 50198, loss: 1.29385, acc: 0.95752, time: Mon Jan 24 01:33:51 2022
epoch 3 training:  26%|██▌       | 50297/196351 [7:09:46<20:54:15,  1.94it/s]global step 443000, epoch: 3, batch: 50298, loss: 3.03194, acc: 0.95755, time: Mon Jan 24 01:34:41 2022
epoch 3 training:  26%|██▌       | 50397/196351 [7:10:37<20:37:23,  1.97it/s]global step 443100, epoch: 3, batch: 50398, loss: 0.31121, acc: 0.95749, time: Mon Jan 24 01:35:32 2022
epoch 3 training:  26%|██▌       | 50497/196351 [7:11:28<20:51:20,  1.94it/s]global step 443200, epoch: 3, batch: 50498, loss: 0.37404, acc: 0.95749, time: Mon Jan 24 01:36:23 2022
epoch 3 training:  26%|██▌       | 50597/196351 [7:12:20<20:30:20,  1.97it/s]global step 443300, epoch: 3, batch: 50598, loss: 1.62922, acc: 0.95753, time: Mon Jan 24 01:37:14 2022
epoch 3 training:  26%|██▌       | 50697/196351 [7:13:11<20:46:05,  1.95it/s]global step 443400, epoch: 3, batch: 50698, loss: 0.67424, acc: 0.95755, time: Mon Jan 24 01:38:06 2022
epoch 3 training:  26%|██▌       | 50797/196351 [7:14:02<20:25:42,  1.98it/s]global step 443500, epoch: 3, batch: 50798, loss: 0.14469, acc: 0.95754, time: Mon Jan 24 01:38:57 2022
epoch 3 training:  26%|██▌       | 50897/196351 [7:14:53<20:38:37,  1.96it/s]global step 443600, epoch: 3, batch: 50898, loss: 0.58456, acc: 0.95757, time: Mon Jan 24 01:39:48 2022
epoch 3 training:  26%|██▌       | 50997/196351 [7:15:44<20:43:53,  1.95it/s]global step 443700, epoch: 3, batch: 50998, loss: 0.45381, acc: 0.95757, time: Mon Jan 24 01:40:39 2022
epoch 3 training:  26%|██▌       | 51097/196351 [7:16:35<20:53:48,  1.93it/s]global step 443800, epoch: 3, batch: 51098, loss: 3.53525, acc: 0.95757, time: Mon Jan 24 01:41:30 2022
epoch 3 training:  26%|██▌       | 51197/196351 [7:17:26<20:29:26,  1.97it/s]global step 443900, epoch: 3, batch: 51198, loss: 0.90458, acc: 0.95757, time: Mon Jan 24 01:42:21 2022
epoch 3 training:  26%|██▌       | 51297/196351 [7:18:17<20:26:30,  1.97it/s]global step 444000, epoch: 3, batch: 51298, loss: 0.20624, acc: 0.95757, time: Mon Jan 24 01:43:12 2022
epoch 3 training:  26%|██▌       | 51397/196351 [7:19:08<20:26:00,  1.97it/s]global step 444100, epoch: 3, batch: 51398, loss: 0.05407, acc: 0.95757, time: Mon Jan 24 01:44:03 2022
epoch 3 training:  26%|██▌       | 51497/196351 [7:19:59<20:37:00,  1.95it/s]global step 444200, epoch: 3, batch: 51498, loss: 1.39908, acc: 0.95760, time: Mon Jan 24 01:44:54 2022
epoch 3 training:  26%|██▋       | 51597/196351 [7:20:50<20:11:51,  1.99it/s]global step 444300, epoch: 3, batch: 51598, loss: 1.18715, acc: 0.95760, time: Mon Jan 24 01:45:45 2022
epoch 3 training:  26%|██▋       | 51697/196351 [7:21:41<20:42:30,  1.94it/s]global step 444400, epoch: 3, batch: 51698, loss: 0.28812, acc: 0.95762, time: Mon Jan 24 01:46:36 2022
epoch 3 training:  26%|██▋       | 51797/196351 [7:22:32<20:24:33,  1.97it/s]global step 444500, epoch: 3, batch: 51798, loss: 0.46884, acc: 0.95762, time: Mon Jan 24 01:47:27 2022
epoch 3 training:  26%|██▋       | 51897/196351 [7:23:23<20:23:24,  1.97it/s]global step 444600, epoch: 3, batch: 51898, loss: 0.69272, acc: 0.95756, time: Mon Jan 24 01:48:18 2022
epoch 3 training:  26%|██▋       | 51997/196351 [7:24:14<20:20:09,  1.97it/s]global step 444700, epoch: 3, batch: 51998, loss: 1.06611, acc: 0.95756, time: Mon Jan 24 01:49:09 2022
epoch 3 training:  27%|██▋       | 52097/196351 [7:25:05<20:25:12,  1.96it/s]global step 444800, epoch: 3, batch: 52098, loss: 1.55628, acc: 0.95752, time: Mon Jan 24 01:49:59 2022
epoch 3 training:  27%|██▋       | 52197/196351 [7:25:55<20:05:22,  1.99it/s]global step 444900, epoch: 3, batch: 52198, loss: 1.35103, acc: 0.95755, time: Mon Jan 24 01:50:50 2022
epoch 3 training:  27%|██▋       | 52297/196351 [7:26:46<20:08:27,  1.99it/s]global step 445000, epoch: 3, batch: 52298, loss: 2.08282, acc: 0.95752, time: Mon Jan 24 01:51:41 2022
epoch 3 training:  27%|██▋       | 52397/196351 [7:27:37<20:06:08,  1.99it/s]global step 445100, epoch: 3, batch: 52398, loss: 1.06568, acc: 0.95755, time: Mon Jan 24 01:52:32 2022
epoch 3 training:  27%|██▋       | 52497/196351 [7:28:28<20:18:15,  1.97it/s]global step 445200, epoch: 3, batch: 52498, loss: 0.55624, acc: 0.95755, time: Mon Jan 24 01:53:23 2022
epoch 3 training:  27%|██▋       | 52597/196351 [7:29:18<20:19:47,  1.96it/s]global step 445300, epoch: 3, batch: 52598, loss: 0.59700, acc: 0.95757, time: Mon Jan 24 01:54:13 2022
epoch 3 training:  27%|██▋       | 52697/196351 [7:30:09<20:11:52,  1.98it/s]global step 445400, epoch: 3, batch: 52698, loss: 0.23868, acc: 0.95763, time: Mon Jan 24 01:55:04 2022
epoch 3 training:  27%|██▋       | 52797/196351 [7:31:00<20:21:58,  1.96it/s]global step 445500, epoch: 3, batch: 52798, loss: 0.31858, acc: 0.95761, time: Mon Jan 24 01:55:55 2022
epoch 3 training:  27%|██▋       | 52897/196351 [7:31:51<20:03:09,  1.99it/s]global step 445600, epoch: 3, batch: 52898, loss: 0.18735, acc: 0.95763, time: Mon Jan 24 01:56:46 2022
epoch 3 training:  27%|██▋       | 52997/196351 [7:32:42<20:24:28,  1.95it/s]global step 445700, epoch: 3, batch: 52998, loss: 0.83287, acc: 0.95760, time: Mon Jan 24 01:57:37 2022
epoch 3 training:  27%|██▋       | 53097/196351 [7:33:33<20:00:51,  1.99it/s]global step 445800, epoch: 3, batch: 53098, loss: 0.93757, acc: 0.95762, time: Mon Jan 24 01:58:28 2022
epoch 3 training:  27%|██▋       | 53197/196351 [7:34:23<19:56:08,  1.99it/s]global step 445900, epoch: 3, batch: 53198, loss: 2.10650, acc: 0.95755, time: Mon Jan 24 01:59:18 2022
epoch 3 training:  27%|██▋       | 53297/196351 [7:35:14<20:15:35,  1.96it/s]global step 446000, epoch: 3, batch: 53298, loss: 0.84616, acc: 0.95752, time: Mon Jan 24 02:00:09 2022
epoch 3 training:  27%|██▋       | 53397/196351 [7:36:05<20:17:02,  1.96it/s]global step 446100, epoch: 3, batch: 53398, loss: 0.68344, acc: 0.95755, time: Mon Jan 24 02:01:00 2022
epoch 3 training:  27%|██▋       | 53497/196351 [7:36:55<20:08:48,  1.97it/s]global step 446200, epoch: 3, batch: 53498, loss: 0.53074, acc: 0.95757, time: Mon Jan 24 02:01:50 2022
epoch 3 training:  27%|██▋       | 53597/196351 [7:37:46<19:55:57,  1.99it/s]global step 446300, epoch: 3, batch: 53598, loss: 1.76606, acc: 0.95755, time: Mon Jan 24 02:02:41 2022
epoch 3 training:  27%|██▋       | 53697/196351 [7:38:37<19:57:01,  1.99it/s]global step 446400, epoch: 3, batch: 53698, loss: 1.80471, acc: 0.95758, time: Mon Jan 24 02:03:32 2022
epoch 3 training:  27%|██▋       | 53797/196351 [7:39:28<19:50:18,  2.00it/s]global step 446500, epoch: 3, batch: 53798, loss: 0.78636, acc: 0.95759, time: Mon Jan 24 02:04:23 2022
epoch 3 training:  27%|██▋       | 53897/196351 [7:40:19<20:08:38,  1.96it/s]global step 446600, epoch: 3, batch: 53898, loss: 1.78590, acc: 0.95761, time: Mon Jan 24 02:05:14 2022
epoch 3 training:  28%|██▊       | 53997/196351 [7:41:10<20:32:09,  1.93it/s]global step 446700, epoch: 3, batch: 53998, loss: 0.40875, acc: 0.95761, time: Mon Jan 24 02:06:05 2022
epoch 3 training:  28%|██▊       | 54097/196351 [7:42:01<20:14:15,  1.95it/s]global step 446800, epoch: 3, batch: 54098, loss: 0.31151, acc: 0.95759, time: Mon Jan 24 02:06:56 2022
epoch 3 training:  28%|██▊       | 54197/196351 [7:42:52<19:57:02,  1.98it/s]global step 446900, epoch: 3, batch: 54198, loss: 0.89394, acc: 0.95759, time: Mon Jan 24 02:07:47 2022
epoch 3 training:  28%|██▊       | 54297/196351 [7:43:44<20:10:47,  1.96it/s]global step 447000, epoch: 3, batch: 54298, loss: 2.44414, acc: 0.95757, time: Mon Jan 24 02:08:38 2022
epoch 3 training:  28%|██▊       | 54397/196351 [7:44:34<20:06:44,  1.96it/s]global step 447100, epoch: 3, batch: 54398, loss: 1.63638, acc: 0.95754, time: Mon Jan 24 02:09:29 2022
epoch 3 training:  28%|██▊       | 54497/196351 [7:45:25<20:10:27,  1.95it/s]global step 447200, epoch: 3, batch: 54498, loss: 0.23709, acc: 0.95758, time: Mon Jan 24 02:10:20 2022
epoch 3 training:  28%|██▊       | 54597/196351 [7:46:15<19:58:19,  1.97it/s]global step 447300, epoch: 3, batch: 54598, loss: 0.68451, acc: 0.95755, time: Mon Jan 24 02:11:10 2022
epoch 3 training:  28%|██▊       | 54697/196351 [7:47:06<20:07:35,  1.96it/s]global step 447400, epoch: 3, batch: 54698, loss: 0.21959, acc: 0.95748, time: Mon Jan 24 02:12:01 2022
epoch 3 training:  28%|██▊       | 54797/196351 [7:47:57<20:04:16,  1.96it/s]global step 447500, epoch: 3, batch: 54798, loss: 1.49473, acc: 0.95751, time: Mon Jan 24 02:12:52 2022
epoch 3 training:  28%|██▊       | 54897/196351 [7:48:48<19:49:22,  1.98it/s]global step 447600, epoch: 3, batch: 54898, loss: 0.63032, acc: 0.95745, time: Mon Jan 24 02:13:43 2022
epoch 3 training:  28%|██▊       | 54997/196351 [7:49:39<20:26:52,  1.92it/s]global step 447700, epoch: 3, batch: 54998, loss: 2.33084, acc: 0.95747, time: Mon Jan 24 02:14:34 2022
epoch 3 training:  28%|██▊       | 55097/196351 [7:50:30<20:05:23,  1.95it/s]global step 447800, epoch: 3, batch: 55098, loss: 2.30333, acc: 0.95743, time: Mon Jan 24 02:15:25 2022
epoch 3 training:  28%|██▊       | 55197/196351 [7:51:21<20:04:18,  1.95it/s]global step 447900, epoch: 3, batch: 55198, loss: 0.30032, acc: 0.95743, time: Mon Jan 24 02:16:15 2022
epoch 3 training:  28%|██▊       | 55297/196351 [7:52:11<19:51:20,  1.97it/s]global step 448000, epoch: 3, batch: 55298, loss: 1.05669, acc: 0.95743, time: Mon Jan 24 02:17:06 2022
epoch 3 training:  28%|██▊       | 55397/196351 [7:53:02<20:08:59,  1.94it/s]global step 448100, epoch: 3, batch: 55398, loss: 3.00584, acc: 0.95734, time: Mon Jan 24 02:17:57 2022
epoch 3 training:  28%|██▊       | 55497/196351 [7:53:53<19:57:41,  1.96it/s]global step 448200, epoch: 3, batch: 55498, loss: 0.11300, acc: 0.95733, time: Mon Jan 24 02:18:48 2022
epoch 3 training:  28%|██▊       | 55597/196351 [7:54:44<19:54:21,  1.96it/s]global step 448300, epoch: 3, batch: 55598, loss: 0.38529, acc: 0.95733, time: Mon Jan 24 02:19:39 2022
epoch 3 training:  28%|██▊       | 55697/196351 [7:55:35<19:58:14,  1.96it/s]global step 448400, epoch: 3, batch: 55698, loss: 2.20844, acc: 0.95736, time: Mon Jan 24 02:20:30 2022
epoch 3 training:  28%|██▊       | 55797/196351 [7:56:26<19:40:03,  1.99it/s]global step 448500, epoch: 3, batch: 55798, loss: 1.48656, acc: 0.95739, time: Mon Jan 24 02:21:21 2022
epoch 3 training:  28%|██▊       | 55897/196351 [7:57:16<19:38:05,  1.99it/s]global step 448600, epoch: 3, batch: 55898, loss: 0.78500, acc: 0.95744, time: Mon Jan 24 02:22:11 2022
epoch 3 training:  29%|██▊       | 55997/196351 [7:58:08<20:11:31,  1.93it/s]global step 448700, epoch: 3, batch: 55998, loss: 0.50137, acc: 0.95745, time: Mon Jan 24 02:23:02 2022
epoch 3 training:  29%|██▊       | 56097/196351 [7:58:58<20:01:15,  1.95it/s]global step 448800, epoch: 3, batch: 56098, loss: 1.58576, acc: 0.95741, time: Mon Jan 24 02:23:53 2022
epoch 3 training:  29%|██▊       | 56197/196351 [7:59:49<19:45:59,  1.97it/s]global step 448900, epoch: 3, batch: 56198, loss: 0.43991, acc: 0.95745, time: Mon Jan 24 02:24:44 2022
epoch 3 training:  29%|██▊       | 56297/196351 [8:00:40<19:41:40,  1.98it/s]global step 449000, epoch: 3, batch: 56298, loss: 0.72124, acc: 0.95747, time: Mon Jan 24 02:25:35 2022
epoch 3 training:  29%|██▊       | 56397/196351 [8:01:31<19:39:11,  1.98it/s]global step 449100, epoch: 3, batch: 56398, loss: 1.42154, acc: 0.95744, time: Mon Jan 24 02:26:26 2022
epoch 3 training:  29%|██▉       | 56497/196351 [8:02:22<19:43:01,  1.97it/s]global step 449200, epoch: 3, batch: 56498, loss: 0.68245, acc: 0.95746, time: Mon Jan 24 02:27:17 2022
epoch 3 training:  29%|██▉       | 56597/196351 [8:03:13<19:55:04,  1.95it/s]global step 449300, epoch: 3, batch: 56598, loss: 0.78748, acc: 0.95749, time: Mon Jan 24 02:28:08 2022
epoch 3 training:  29%|██▉       | 56697/196351 [8:04:04<19:50:41,  1.95it/s]global step 449400, epoch: 3, batch: 56698, loss: 0.26283, acc: 0.95751, time: Mon Jan 24 02:28:59 2022
epoch 3 training:  29%|██▉       | 56797/196351 [8:04:55<19:40:33,  1.97it/s]global step 449500, epoch: 3, batch: 56798, loss: 0.61170, acc: 0.95757, time: Mon Jan 24 02:29:50 2022
epoch 3 training:  29%|██▉       | 56897/196351 [8:05:46<19:31:20,  1.98it/s]global step 449600, epoch: 3, batch: 56898, loss: 0.40461, acc: 0.95757, time: Mon Jan 24 02:30:41 2022
epoch 3 training:  29%|██▉       | 56997/196351 [8:06:37<19:47:50,  1.96it/s]global step 449700, epoch: 3, batch: 56998, loss: 0.08576, acc: 0.95757, time: Mon Jan 24 02:31:31 2022
epoch 3 training:  29%|██▉       | 57097/196351 [8:07:27<19:41:01,  1.97it/s]global step 449800, epoch: 3, batch: 57098, loss: 0.24360, acc: 0.95755, time: Mon Jan 24 02:32:22 2022
epoch 3 training:  29%|██▉       | 57197/196351 [8:08:18<19:44:53,  1.96it/s]global step 449900, epoch: 3, batch: 57198, loss: 0.60145, acc: 0.95758, time: Mon Jan 24 02:33:13 2022
epoch 3 training:  29%|██▉       | 57297/196351 [8:09:09<19:44:15,  1.96it/s]global step 450000, epoch: 3, batch: 57298, loss: 1.30361, acc: 0.95754, time: Mon Jan 24 02:34:04 2022
epoch 3 training:  29%|██▉       | 57397/196351 [8:10:00<19:23:38,  1.99it/s]global step 450100, epoch: 3, batch: 57398, loss: 1.39157, acc: 0.95752, time: Mon Jan 24 02:34:55 2022
epoch 3 training:  29%|██▉       | 57497/196351 [8:10:51<19:32:28,  1.97it/s]global step 450200, epoch: 3, batch: 57498, loss: 0.98352, acc: 0.95756, time: Mon Jan 24 02:35:46 2022
epoch 3 training:  29%|██▉       | 57597/196351 [8:11:42<19:35:31,  1.97it/s]global step 450300, epoch: 3, batch: 57598, loss: 2.21580, acc: 0.95759, time: Mon Jan 24 02:36:37 2022
epoch 3 training:  29%|██▉       | 57697/196351 [8:12:33<19:30:31,  1.97it/s]global step 450400, epoch: 3, batch: 57698, loss: 0.35522, acc: 0.95759, time: Mon Jan 24 02:37:28 2022
epoch 3 training:  29%|██▉       | 57797/196351 [8:13:24<19:31:29,  1.97it/s]global step 450500, epoch: 3, batch: 57798, loss: 0.17217, acc: 0.95762, time: Mon Jan 24 02:38:19 2022
epoch 3 training:  29%|██▉       | 57897/196351 [8:14:15<19:20:38,  1.99it/s]global step 450600, epoch: 3, batch: 57898, loss: 0.58731, acc: 0.95758, time: Mon Jan 24 02:39:10 2022
epoch 3 training:  30%|██▉       | 57997/196351 [8:15:06<19:36:17,  1.96it/s]global step 450700, epoch: 3, batch: 57998, loss: 0.45889, acc: 0.95758, time: Mon Jan 24 02:40:01 2022
epoch 3 training:  30%|██▉       | 58097/196351 [8:15:57<19:48:40,  1.94it/s]global step 450800, epoch: 3, batch: 58098, loss: 2.51491, acc: 0.95761, time: Mon Jan 24 02:40:52 2022
epoch 3 training:  30%|██▉       | 58197/196351 [8:16:48<19:28:24,  1.97it/s]global step 450900, epoch: 3, batch: 58198, loss: 0.80339, acc: 0.95761, time: Mon Jan 24 02:41:43 2022
epoch 3 training:  30%|██▉       | 58297/196351 [8:17:39<19:17:12,  1.99it/s]global step 451000, epoch: 3, batch: 58298, loss: 0.69554, acc: 0.95761, time: Mon Jan 24 02:42:33 2022
epoch 3 training:  30%|██▉       | 58397/196351 [8:18:30<19:40:23,  1.95it/s]global step 451100, epoch: 3, batch: 58398, loss: 1.79017, acc: 0.95762, time: Mon Jan 24 02:43:24 2022
epoch 3 training:  30%|██▉       | 58497/196351 [8:19:21<19:26:22,  1.97it/s]global step 451200, epoch: 3, batch: 58498, loss: 0.84683, acc: 0.95761, time: Mon Jan 24 02:44:15 2022
epoch 3 training:  30%|██▉       | 58597/196351 [8:20:11<19:36:40,  1.95it/s]global step 451300, epoch: 3, batch: 58598, loss: 0.84693, acc: 0.95758, time: Mon Jan 24 02:45:06 2022
epoch 3 training:  30%|██▉       | 58697/196351 [8:21:02<19:07:19,  2.00it/s]global step 451400, epoch: 3, batch: 58698, loss: 0.81036, acc: 0.95761, time: Mon Jan 24 02:45:57 2022
epoch 3 training:  30%|██▉       | 58797/196351 [8:21:53<19:09:25,  1.99it/s]global step 451500, epoch: 3, batch: 58798, loss: 0.96231, acc: 0.95758, time: Mon Jan 24 02:46:48 2022
epoch 3 training:  30%|██▉       | 58897/196351 [8:22:44<19:19:21,  1.98it/s]global step 451600, epoch: 3, batch: 58898, loss: 1.44668, acc: 0.95758, time: Mon Jan 24 02:47:39 2022
epoch 3 training:  30%|███       | 58997/196351 [8:23:35<19:31:41,  1.95it/s]global step 451700, epoch: 3, batch: 58998, loss: 0.80567, acc: 0.95756, time: Mon Jan 24 02:48:30 2022
epoch 3 training:  30%|███       | 59097/196351 [8:24:26<19:20:05,  1.97it/s]global step 451800, epoch: 3, batch: 59098, loss: 2.45540, acc: 0.95755, time: Mon Jan 24 02:49:21 2022
epoch 3 training:  30%|███       | 59197/196351 [8:25:17<19:39:53,  1.94it/s]global step 451900, epoch: 3, batch: 59198, loss: 0.82614, acc: 0.95758, time: Mon Jan 24 02:50:11 2022
epoch 3 training:  30%|███       | 59297/196351 [8:26:07<19:01:17,  2.00it/s]global step 452000, epoch: 3, batch: 59298, loss: 1.84756, acc: 0.95760, time: Mon Jan 24 02:51:02 2022
epoch 3 training:  30%|███       | 59397/196351 [8:26:58<19:05:58,  1.99it/s]global step 452100, epoch: 3, batch: 59398, loss: 1.01621, acc: 0.95759, time: Mon Jan 24 02:51:53 2022
epoch 3 training:  30%|███       | 59497/196351 [8:27:48<19:16:36,  1.97it/s]global step 452200, epoch: 3, batch: 59498, loss: 0.62282, acc: 0.95761, time: Mon Jan 24 02:52:43 2022
epoch 3 training:  30%|███       | 59597/196351 [8:28:39<19:14:39,  1.97it/s]global step 452300, epoch: 3, batch: 59598, loss: 0.14037, acc: 0.95762, time: Mon Jan 24 02:53:34 2022
epoch 3 training:  30%|███       | 59697/196351 [8:29:30<19:13:51,  1.97it/s]global step 452400, epoch: 3, batch: 59698, loss: 0.15218, acc: 0.95765, time: Mon Jan 24 02:54:25 2022
epoch 3 training:  30%|███       | 59797/196351 [8:30:20<19:14:52,  1.97it/s]global step 452500, epoch: 3, batch: 59798, loss: 0.90366, acc: 0.95762, time: Mon Jan 24 02:55:15 2022
epoch 3 training:  31%|███       | 59897/196351 [8:31:11<19:16:58,  1.97it/s]global step 452600, epoch: 3, batch: 59898, loss: 0.70618, acc: 0.95762, time: Mon Jan 24 02:56:06 2022
epoch 3 training:  31%|███       | 59997/196351 [8:32:02<18:58:57,  2.00it/s]global step 452700, epoch: 3, batch: 59998, loss: 0.57170, acc: 0.95762, time: Mon Jan 24 02:56:57 2022
epoch 3 training:  31%|███       | 60097/196351 [8:32:53<19:24:49,  1.95it/s]global step 452800, epoch: 3, batch: 60098, loss: 1.17667, acc: 0.95765, time: Mon Jan 24 02:57:48 2022
epoch 3 training:  31%|███       | 60197/196351 [8:33:44<19:24:26,  1.95it/s]global step 452900, epoch: 3, batch: 60198, loss: 1.88059, acc: 0.95766, time: Mon Jan 24 02:58:39 2022
epoch 3 training:  31%|███       | 60297/196351 [8:34:35<18:58:24,  1.99it/s]global step 453000, epoch: 3, batch: 60298, loss: 1.20664, acc: 0.95762, time: Mon Jan 24 02:59:30 2022
epoch 3 training:  31%|███       | 60397/196351 [8:35:25<19:05:45,  1.98it/s]global step 453100, epoch: 3, batch: 60398, loss: 0.86453, acc: 0.95764, time: Mon Jan 24 03:00:20 2022
epoch 3 training:  31%|███       | 60497/196351 [8:36:16<19:15:50,  1.96it/s]global step 453200, epoch: 3, batch: 60498, loss: 0.70569, acc: 0.95764, time: Mon Jan 24 03:01:11 2022
epoch 3 training:  31%|███       | 60597/196351 [8:37:07<19:19:17,  1.95it/s]global step 453300, epoch: 3, batch: 60598, loss: 0.17707, acc: 0.95764, time: Mon Jan 24 03:02:02 2022
epoch 3 training:  31%|███       | 60697/196351 [8:37:58<19:25:37,  1.94it/s]global step 453400, epoch: 3, batch: 60698, loss: 1.08785, acc: 0.95762, time: Mon Jan 24 03:02:53 2022
epoch 3 training:  31%|███       | 60797/196351 [8:38:48<19:05:54,  1.97it/s]global step 453500, epoch: 3, batch: 60798, loss: 0.67749, acc: 0.95757, time: Mon Jan 24 03:03:43 2022
epoch 3 training:  31%|███       | 60897/196351 [8:39:39<19:03:47,  1.97it/s]global step 453600, epoch: 3, batch: 60898, loss: 0.84618, acc: 0.95761, time: Mon Jan 24 03:04:34 2022
epoch 3 training:  31%|███       | 60997/196351 [8:40:30<19:09:33,  1.96it/s]global step 453700, epoch: 3, batch: 60998, loss: 1.03007, acc: 0.95762, time: Mon Jan 24 03:05:25 2022
epoch 3 training:  31%|███       | 61097/196351 [8:41:21<19:09:46,  1.96it/s]global step 453800, epoch: 3, batch: 61098, loss: 0.21263, acc: 0.95758, time: Mon Jan 24 03:06:16 2022
epoch 3 training:  31%|███       | 61197/196351 [8:42:12<19:12:42,  1.95it/s]global step 453900, epoch: 3, batch: 61198, loss: 2.91270, acc: 0.95758, time: Mon Jan 24 03:07:07 2022
epoch 3 training:  31%|███       | 61297/196351 [8:43:03<19:16:19,  1.95it/s]global step 454000, epoch: 3, batch: 61298, loss: 1.92548, acc: 0.95758, time: Mon Jan 24 03:07:58 2022
epoch 3 training:  31%|███▏      | 61397/196351 [8:43:54<19:13:00,  1.95it/s]global step 454100, epoch: 3, batch: 61398, loss: 0.62513, acc: 0.95758, time: Mon Jan 24 03:08:49 2022
epoch 3 training:  31%|███▏      | 61497/196351 [8:44:45<19:05:43,  1.96it/s]global step 454200, epoch: 3, batch: 61498, loss: 1.49811, acc: 0.95759, time: Mon Jan 24 03:09:40 2022
epoch 3 training:  31%|███▏      | 61597/196351 [8:45:36<19:27:41,  1.92it/s]global step 454300, epoch: 3, batch: 61598, loss: 0.26958, acc: 0.95760, time: Mon Jan 24 03:10:31 2022
epoch 3 training:  31%|███▏      | 61697/196351 [8:46:27<19:04:53,  1.96it/s]global step 454400, epoch: 3, batch: 61698, loss: 0.91314, acc: 0.95759, time: Mon Jan 24 03:11:22 2022
epoch 3 training:  31%|███▏      | 61797/196351 [8:47:18<19:11:04,  1.95it/s]global step 454500, epoch: 3, batch: 61798, loss: 3.38887, acc: 0.95756, time: Mon Jan 24 03:12:13 2022
epoch 3 training:  32%|███▏      | 61897/196351 [8:48:09<19:02:52,  1.96it/s]global step 454600, epoch: 3, batch: 61898, loss: 0.68050, acc: 0.95759, time: Mon Jan 24 03:13:04 2022
epoch 3 training:  32%|███▏      | 61997/196351 [8:49:00<18:43:08,  1.99it/s]global step 454700, epoch: 3, batch: 61998, loss: 2.19681, acc: 0.95760, time: Mon Jan 24 03:13:55 2022
epoch 3 training:  32%|███▏      | 62097/196351 [8:49:51<18:44:37,  1.99it/s]global step 454800, epoch: 3, batch: 62098, loss: 0.40535, acc: 0.95763, time: Mon Jan 24 03:14:46 2022
epoch 3 training:  32%|███▏      | 62197/196351 [8:50:41<18:50:07,  1.98it/s]global step 454900, epoch: 3, batch: 62198, loss: 0.87175, acc: 0.95760, time: Mon Jan 24 03:15:36 2022
epoch 3 training:  32%|███▏      | 62297/196351 [8:51:32<19:00:19,  1.96it/s]global step 455000, epoch: 3, batch: 62298, loss: 1.01141, acc: 0.95761, time: Mon Jan 24 03:16:27 2022
epoch 3 training:  32%|███▏      | 62397/196351 [8:52:23<18:35:49,  2.00it/s]global step 455100, epoch: 3, batch: 62398, loss: 1.52259, acc: 0.95759, time: Mon Jan 24 03:17:18 2022
epoch 3 training:  32%|███▏      | 62497/196351 [8:53:14<19:15:34,  1.93it/s]global step 455200, epoch: 3, batch: 62498, loss: 0.58928, acc: 0.95761, time: Mon Jan 24 03:18:09 2022
epoch 3 training:  32%|███▏      | 62597/196351 [8:54:05<19:01:38,  1.95it/s]global step 455300, epoch: 3, batch: 62598, loss: 0.18563, acc: 0.95763, time: Mon Jan 24 03:19:00 2022
epoch 3 training:  32%|███▏      | 62697/196351 [8:54:56<18:50:30,  1.97it/s]global step 455400, epoch: 3, batch: 62698, loss: 0.97637, acc: 0.95765, time: Mon Jan 24 03:19:51 2022
epoch 3 training:  32%|███▏      | 62797/196351 [8:55:47<18:55:05,  1.96it/s]global step 455500, epoch: 3, batch: 62798, loss: 0.62100, acc: 0.95767, time: Mon Jan 24 03:20:42 2022
epoch 3 training:  32%|███▏      | 62897/196351 [8:56:38<18:52:54,  1.96it/s]global step 455600, epoch: 3, batch: 62898, loss: 0.11957, acc: 0.95771, time: Mon Jan 24 03:21:33 2022
epoch 3 training:  32%|███▏      | 62997/196351 [8:57:29<18:47:24,  1.97it/s]global step 455700, epoch: 3, batch: 62998, loss: 2.24498, acc: 0.95772, time: Mon Jan 24 03:22:24 2022
epoch 3 training:  32%|███▏      | 63097/196351 [8:58:19<18:45:55,  1.97it/s]global step 455800, epoch: 3, batch: 63098, loss: 0.47711, acc: 0.95774, time: Mon Jan 24 03:23:14 2022
epoch 3 training:  32%|███▏      | 63197/196351 [8:59:10<18:45:30,  1.97it/s]global step 455900, epoch: 3, batch: 63198, loss: 0.33364, acc: 0.95772, time: Mon Jan 24 03:24:05 2022
epoch 3 training:  32%|███▏      | 63297/196351 [9:00:01<19:02:38,  1.94it/s]global step 456000, epoch: 3, batch: 63298, loss: 1.20811, acc: 0.95772, time: Mon Jan 24 03:24:56 2022
epoch 3 training:  32%|███▏      | 63397/196351 [9:00:52<19:22:53,  1.91it/s]global step 456100, epoch: 3, batch: 63398, loss: 2.22327, acc: 0.95771, time: Mon Jan 24 03:25:47 2022
epoch 3 training:  32%|███▏      | 63497/196351 [9:01:44<18:44:09,  1.97it/s]global step 456200, epoch: 3, batch: 63498, loss: 1.20029, acc: 0.95775, time: Mon Jan 24 03:26:39 2022
epoch 3 training:  32%|███▏      | 63597/196351 [9:02:35<18:44:38,  1.97it/s]global step 456300, epoch: 3, batch: 63598, loss: 0.97017, acc: 0.95774, time: Mon Jan 24 03:27:29 2022
epoch 3 training:  32%|███▏      | 63697/196351 [9:03:26<18:42:14,  1.97it/s]global step 456400, epoch: 3, batch: 63698, loss: 0.23754, acc: 0.95777, time: Mon Jan 24 03:28:21 2022
epoch 3 training:  32%|███▏      | 63797/196351 [9:04:17<18:45:35,  1.96it/s]global step 456500, epoch: 3, batch: 63798, loss: 0.17195, acc: 0.95778, time: Mon Jan 24 03:29:11 2022
epoch 3 training:  33%|███▎      | 63897/196351 [9:05:08<19:11:59,  1.92it/s]global step 456600, epoch: 3, batch: 63898, loss: 0.96828, acc: 0.95777, time: Mon Jan 24 03:30:03 2022
epoch 3 training:  33%|███▎      | 63997/196351 [9:05:59<18:50:28,  1.95it/s]global step 456700, epoch: 3, batch: 63998, loss: 2.78041, acc: 0.95770, time: Mon Jan 24 03:30:54 2022
epoch 3 training:  33%|███▎      | 64097/196351 [9:06:50<18:43:50,  1.96it/s]global step 456800, epoch: 3, batch: 64098, loss: 0.33352, acc: 0.95769, time: Mon Jan 24 03:31:45 2022
epoch 3 training:  33%|███▎      | 64197/196351 [9:07:41<18:22:43,  2.00it/s]global step 456900, epoch: 3, batch: 64198, loss: 4.08611, acc: 0.95768, time: Mon Jan 24 03:32:36 2022
epoch 3 training:  33%|███▎      | 64297/196351 [9:08:32<18:43:45,  1.96it/s]global step 457000, epoch: 3, batch: 64298, loss: 2.69315, acc: 0.95770, time: Mon Jan 24 03:33:26 2022
epoch 3 training:  33%|███▎      | 64397/196351 [9:09:23<18:44:39,  1.96it/s]global step 457100, epoch: 3, batch: 64398, loss: 0.64834, acc: 0.95772, time: Mon Jan 24 03:34:18 2022
epoch 3 training:  33%|███▎      | 64497/196351 [9:10:14<18:25:58,  1.99it/s]global step 457200, epoch: 3, batch: 64498, loss: 4.22215, acc: 0.95773, time: Mon Jan 24 03:35:09 2022
epoch 3 training:  33%|███▎      | 64597/196351 [9:11:05<19:01:39,  1.92it/s]global step 457300, epoch: 3, batch: 64598, loss: 1.15186, acc: 0.95772, time: Mon Jan 24 03:35:59 2022
epoch 3 training:  33%|███▎      | 64697/196351 [9:11:55<18:29:07,  1.98it/s]global step 457400, epoch: 3, batch: 64698, loss: 0.24962, acc: 0.95775, time: Mon Jan 24 03:36:50 2022
epoch 3 training:  33%|███▎      | 64797/196351 [9:12:47<18:25:41,  1.98it/s]global step 457500, epoch: 3, batch: 64798, loss: 0.88363, acc: 0.95770, time: Mon Jan 24 03:37:41 2022
epoch 3 training:  33%|███▎      | 64897/196351 [9:13:37<18:37:25,  1.96it/s]global step 457600, epoch: 3, batch: 64898, loss: 0.75528, acc: 0.95768, time: Mon Jan 24 03:38:32 2022
epoch 3 training:  33%|███▎      | 64997/196351 [9:14:28<18:34:06,  1.96it/s]global step 457700, epoch: 3, batch: 64998, loss: 0.15031, acc: 0.95769, time: Mon Jan 24 03:39:23 2022
epoch 3 training:  33%|███▎      | 65097/196351 [9:15:19<18:28:05,  1.97it/s]global step 457800, epoch: 3, batch: 65098, loss: 1.34166, acc: 0.95769, time: Mon Jan 24 03:40:14 2022
epoch 3 training:  33%|███▎      | 65197/196351 [9:16:10<18:36:17,  1.96it/s]global step 457900, epoch: 3, batch: 65198, loss: 0.15916, acc: 0.95771, time: Mon Jan 24 03:41:05 2022
epoch 3 training:  33%|███▎      | 65297/196351 [9:17:00<18:19:47,  1.99it/s]global step 458000, epoch: 3, batch: 65298, loss: 1.20222, acc: 0.95772, time: Mon Jan 24 03:41:55 2022
epoch 3 training:  33%|███▎      | 65397/196351 [9:17:51<18:37:25,  1.95it/s]global step 458100, epoch: 3, batch: 65398, loss: 2.06148, acc: 0.95773, time: Mon Jan 24 03:42:46 2022
epoch 3 training:  33%|███▎      | 65497/196351 [9:18:43<18:20:20,  1.98it/s]global step 458200, epoch: 3, batch: 65498, loss: 1.99483, acc: 0.95772, time: Mon Jan 24 03:43:37 2022
epoch 3 training:  33%|███▎      | 65597/196351 [9:19:33<18:39:29,  1.95it/s]global step 458300, epoch: 3, batch: 65598, loss: 0.21383, acc: 0.95773, time: Mon Jan 24 03:44:28 2022
epoch 3 training:  33%|███▎      | 65697/196351 [9:20:24<18:38:54,  1.95it/s]global step 458400, epoch: 3, batch: 65698, loss: 0.39347, acc: 0.95772, time: Mon Jan 24 03:45:19 2022
epoch 3 training:  34%|███▎      | 65797/196351 [9:21:15<18:28:53,  1.96it/s]global step 458500, epoch: 3, batch: 65798, loss: 2.03205, acc: 0.95770, time: Mon Jan 24 03:46:10 2022
epoch 3 training:  34%|███▎      | 65897/196351 [9:22:06<18:33:44,  1.95it/s]global step 458600, epoch: 3, batch: 65898, loss: 0.85847, acc: 0.95772, time: Mon Jan 24 03:47:01 2022
epoch 3 training:  34%|███▎      | 65997/196351 [9:22:57<18:24:50,  1.97it/s]global step 458700, epoch: 3, batch: 65998, loss: 2.11941, acc: 0.95773, time: Mon Jan 24 03:47:52 2022
epoch 3 training:  34%|███▎      | 66097/196351 [9:23:48<18:32:15,  1.95it/s]global step 458800, epoch: 3, batch: 66098, loss: 0.35604, acc: 0.95771, time: Mon Jan 24 03:48:43 2022
epoch 3 training:  34%|███▎      | 66197/196351 [9:24:39<18:29:49,  1.95it/s]global step 458900, epoch: 3, batch: 66198, loss: 0.87090, acc: 0.95767, time: Mon Jan 24 03:49:34 2022
epoch 3 training:  34%|███▍      | 66297/196351 [9:25:30<18:10:47,  1.99it/s]global step 459000, epoch: 3, batch: 66298, loss: 0.36516, acc: 0.95764, time: Mon Jan 24 03:50:25 2022
epoch 3 training:  34%|███▍      | 66397/196351 [9:26:21<18:22:24,  1.96it/s]global step 459100, epoch: 3, batch: 66398, loss: 0.75634, acc: 0.95761, time: Mon Jan 24 03:51:16 2022
epoch 3 training:  34%|███▍      | 66497/196351 [9:27:12<18:15:29,  1.98it/s]global step 459200, epoch: 3, batch: 66498, loss: 0.50819, acc: 0.95761, time: Mon Jan 24 03:52:07 2022
epoch 3 training:  34%|███▍      | 66597/196351 [9:28:03<18:28:43,  1.95it/s]global step 459300, epoch: 3, batch: 66598, loss: 0.39497, acc: 0.95758, time: Mon Jan 24 03:52:58 2022
epoch 3 training:  34%|███▍      | 66697/196351 [9:28:54<18:39:52,  1.93it/s]global step 459400, epoch: 3, batch: 66698, loss: 2.02039, acc: 0.95758, time: Mon Jan 24 03:53:49 2022
epoch 3 training:  34%|███▍      | 66797/196351 [9:29:46<18:27:27,  1.95it/s]global step 459500, epoch: 3, batch: 66798, loss: 2.86714, acc: 0.95761, time: Mon Jan 24 03:54:40 2022
epoch 3 training:  34%|███▍      | 66897/196351 [9:30:37<18:16:49,  1.97it/s]global step 459600, epoch: 3, batch: 66898, loss: 1.80940, acc: 0.95759, time: Mon Jan 24 03:55:32 2022
epoch 3 training:  34%|███▍      | 66997/196351 [9:31:28<18:22:57,  1.95it/s]global step 459700, epoch: 3, batch: 66998, loss: 0.47698, acc: 0.95759, time: Mon Jan 24 03:56:23 2022
epoch 3 training:  34%|███▍      | 67097/196351 [9:32:19<18:04:28,  1.99it/s]global step 459800, epoch: 3, batch: 67098, loss: 1.09476, acc: 0.95757, time: Mon Jan 24 03:57:14 2022
epoch 3 training:  34%|███▍      | 67197/196351 [9:33:10<18:11:45,  1.97it/s]global step 459900, epoch: 3, batch: 67198, loss: 1.17408, acc: 0.95755, time: Mon Jan 24 03:58:05 2022
epoch 3 training:  34%|███▍      | 67297/196351 [9:34:01<18:08:27,  1.98it/s]global step 460000, epoch: 3, batch: 67298, loss: 0.54024, acc: 0.95757, time: Mon Jan 24 03:58:56 2022
epoch 3 training:  34%|███▍      | 67397/196351 [9:34:52<18:23:16,  1.95it/s]global step 460100, epoch: 3, batch: 67398, loss: 2.82714, acc: 0.95756, time: Mon Jan 24 03:59:47 2022
epoch 3 training:  34%|███▍      | 67497/196351 [9:35:42<18:01:19,  1.99it/s]global step 460200, epoch: 3, batch: 67498, loss: 0.32089, acc: 0.95755, time: Mon Jan 24 04:00:37 2022
epoch 3 training:  34%|███▍      | 67597/196351 [9:36:33<18:19:12,  1.95it/s]global step 460300, epoch: 3, batch: 67598, loss: 1.88563, acc: 0.95753, time: Mon Jan 24 04:01:28 2022
epoch 3 training:  34%|███▍      | 67697/196351 [9:37:24<18:07:21,  1.97it/s]global step 460400, epoch: 3, batch: 67698, loss: 0.91486, acc: 0.95755, time: Mon Jan 24 04:02:19 2022
epoch 3 training:  35%|███▍      | 67797/196351 [9:38:15<18:20:36,  1.95it/s]global step 460500, epoch: 3, batch: 67798, loss: 0.68667, acc: 0.95756, time: Mon Jan 24 04:03:09 2022
epoch 3 training:  35%|███▍      | 67897/196351 [9:39:05<17:59:40,  1.98it/s]global step 460600, epoch: 3, batch: 67898, loss: 2.08093, acc: 0.95754, time: Mon Jan 24 04:04:00 2022
epoch 3 training:  35%|███▍      | 67997/196351 [9:39:56<17:50:38,  2.00it/s]global step 460700, epoch: 3, batch: 67998, loss: 0.71314, acc: 0.95755, time: Mon Jan 24 04:04:51 2022
epoch 3 training:  35%|███▍      | 68097/196351 [9:40:47<18:11:45,  1.96it/s]global step 460800, epoch: 3, batch: 68098, loss: 1.88949, acc: 0.95758, time: Mon Jan 24 04:05:42 2022
epoch 3 training:  35%|███▍      | 68197/196351 [9:41:38<18:12:24,  1.96it/s]global step 460900, epoch: 3, batch: 68198, loss: 1.26652, acc: 0.95758, time: Mon Jan 24 04:06:33 2022
epoch 3 training:  35%|███▍      | 68297/196351 [9:42:29<18:07:10,  1.96it/s]global step 461000, epoch: 3, batch: 68298, loss: 0.40975, acc: 0.95760, time: Mon Jan 24 04:07:24 2022
epoch 3 training:  35%|███▍      | 68397/196351 [9:43:21<18:00:20,  1.97it/s]global step 461100, epoch: 3, batch: 68398, loss: 1.61170, acc: 0.95759, time: Mon Jan 24 04:08:15 2022
epoch 3 training:  35%|███▍      | 68497/196351 [9:44:12<17:53:33,  1.98it/s]global step 461200, epoch: 3, batch: 68498, loss: 2.46611, acc: 0.95757, time: Mon Jan 24 04:09:07 2022
epoch 3 training:  35%|███▍      | 68597/196351 [9:45:03<18:23:32,  1.93it/s]global step 461300, epoch: 3, batch: 68598, loss: 0.75864, acc: 0.95755, time: Mon Jan 24 04:09:58 2022
epoch 3 training:  35%|███▍      | 68697/196351 [9:45:54<18:00:47,  1.97it/s]global step 461400, epoch: 3, batch: 68698, loss: 0.14976, acc: 0.95753, time: Mon Jan 24 04:10:49 2022
epoch 3 training:  35%|███▌      | 68797/196351 [9:46:45<18:29:36,  1.92it/s]global step 461500, epoch: 3, batch: 68798, loss: 0.96257, acc: 0.95751, time: Mon Jan 24 04:11:40 2022
epoch 3 training:  35%|███▌      | 68897/196351 [9:47:37<18:14:58,  1.94it/s]global step 461600, epoch: 3, batch: 68898, loss: 2.59043, acc: 0.95752, time: Mon Jan 24 04:12:31 2022
epoch 3 training:  35%|███▌      | 68997/196351 [9:48:28<17:57:42,  1.97it/s]global step 461700, epoch: 3, batch: 68998, loss: 0.17383, acc: 0.95755, time: Mon Jan 24 04:13:22 2022
epoch 3 training:  35%|███▌      | 69097/196351 [9:49:18<18:05:06,  1.95it/s]global step 461800, epoch: 3, batch: 69098, loss: 1.00059, acc: 0.95752, time: Mon Jan 24 04:14:13 2022
epoch 3 training:  35%|███▌      | 69197/196351 [9:50:09<18:10:04,  1.94it/s]global step 461900, epoch: 3, batch: 69198, loss: 0.27820, acc: 0.95751, time: Mon Jan 24 04:15:04 2022
epoch 3 training:  35%|███▌      | 69297/196351 [9:51:00<17:47:22,  1.98it/s]global step 462000, epoch: 3, batch: 69298, loss: 0.82248, acc: 0.95754, time: Mon Jan 24 04:15:55 2022
epoch 3 training:  35%|███▌      | 69397/196351 [9:51:51<18:10:07,  1.94it/s]global step 462100, epoch: 3, batch: 69398, loss: 2.61549, acc: 0.95754, time: Mon Jan 24 04:16:46 2022
epoch 3 training:  35%|███▌      | 69497/196351 [9:52:42<17:45:28,  1.98it/s]global step 462200, epoch: 3, batch: 69498, loss: 2.12335, acc: 0.95755, time: Mon Jan 24 04:17:37 2022
epoch 3 training:  35%|███▌      | 69597/196351 [9:53:32<17:32:13,  2.01it/s]global step 462300, epoch: 3, batch: 69598, loss: 1.81678, acc: 0.95756, time: Mon Jan 24 04:18:27 2022
epoch 3 training:  35%|███▌      | 69697/196351 [9:54:23<17:46:11,  1.98it/s]global step 462400, epoch: 3, batch: 69698, loss: 0.80926, acc: 0.95757, time: Mon Jan 24 04:19:18 2022
epoch 3 training:  36%|███▌      | 69797/196351 [9:55:14<17:57:12,  1.96it/s]global step 462500, epoch: 3, batch: 69798, loss: 1.07843, acc: 0.95756, time: Mon Jan 24 04:20:09 2022
epoch 3 training:  36%|███▌      | 69897/196351 [9:56:05<17:55:03,  1.96it/s]global step 462600, epoch: 3, batch: 69898, loss: 0.26899, acc: 0.95756, time: Mon Jan 24 04:21:00 2022
epoch 3 training:  36%|███▌      | 69997/196351 [9:56:56<17:46:47,  1.97it/s]global step 462700, epoch: 3, batch: 69998, loss: 1.45928, acc: 0.95753, time: Mon Jan 24 04:21:51 2022
epoch 3 training:  36%|███▌      | 70097/196351 [9:57:47<17:55:44,  1.96it/s]global step 462800, epoch: 3, batch: 70098, loss: 1.21240, acc: 0.95751, time: Mon Jan 24 04:22:42 2022
epoch 3 training:  36%|███▌      | 70197/196351 [9:58:38<17:54:46,  1.96it/s]global step 462900, epoch: 3, batch: 70198, loss: 0.45273, acc: 0.95751, time: Mon Jan 24 04:23:33 2022
epoch 3 training:  36%|███▌      | 70297/196351 [9:59:29<19:14:02,  1.82it/s]global step 463000, epoch: 3, batch: 70298, loss: 0.62482, acc: 0.95749, time: Mon Jan 24 04:24:24 2022
epoch 3 training:  36%|███▌      | 70397/196351 [10:00:20<17:37:27,  1.99it/s]global step 463100, epoch: 3, batch: 70398, loss: 0.36602, acc: 0.95751, time: Mon Jan 24 04:25:15 2022
epoch 3 training:  36%|███▌      | 70497/196351 [10:01:11<17:31:36,  1.99it/s]global step 463200, epoch: 3, batch: 70498, loss: 1.37981, acc: 0.95750, time: Mon Jan 24 04:26:06 2022
epoch 3 training:  36%|███▌      | 70597/196351 [10:02:02<17:51:27,  1.96it/s]global step 463300, epoch: 3, batch: 70598, loss: 3.06345, acc: 0.95752, time: Mon Jan 24 04:26:57 2022
epoch 3 training:  36%|███▌      | 70697/196351 [10:02:53<17:34:23,  1.99it/s]global step 463400, epoch: 3, batch: 70698, loss: 1.98067, acc: 0.95754, time: Mon Jan 24 04:27:48 2022
epoch 3 training:  36%|███▌      | 70797/196351 [10:03:44<18:02:28,  1.93it/s]global step 463500, epoch: 3, batch: 70798, loss: 1.07041, acc: 0.95753, time: Mon Jan 24 04:28:39 2022
epoch 3 training:  36%|███▌      | 70897/196351 [10:04:35<17:56:31,  1.94it/s]global step 463600, epoch: 3, batch: 70898, loss: 2.79482, acc: 0.95755, time: Mon Jan 24 04:29:30 2022
epoch 3 training:  36%|███▌      | 70997/196351 [10:05:26<17:58:17,  1.94it/s]global step 463700, epoch: 3, batch: 70998, loss: 0.70749, acc: 0.95752, time: Mon Jan 24 04:30:21 2022
epoch 3 training:  36%|███▌      | 71097/196351 [10:06:17<17:47:16,  1.96it/s]global step 463800, epoch: 3, batch: 71098, loss: 0.46683, acc: 0.95750, time: Mon Jan 24 04:31:12 2022
epoch 3 training:  36%|███▋      | 71197/196351 [10:07:08<17:32:48,  1.98it/s]global step 463900, epoch: 3, batch: 71198, loss: 0.57821, acc: 0.95748, time: Mon Jan 24 04:32:03 2022
epoch 3 training:  36%|███▋      | 71297/196351 [10:07:59<17:27:44,  1.99it/s]global step 464000, epoch: 3, batch: 71298, loss: 0.67331, acc: 0.95747, time: Mon Jan 24 04:32:54 2022
epoch 3 training:  36%|███▋      | 71397/196351 [10:08:50<17:45:20,  1.95it/s]global step 464100, epoch: 3, batch: 71398, loss: 0.77653, acc: 0.95747, time: Mon Jan 24 04:33:45 2022
epoch 3 training:  36%|███▋      | 71497/196351 [10:09:41<17:30:05,  1.98it/s]global step 464200, epoch: 3, batch: 71498, loss: 1.98956, acc: 0.95745, time: Mon Jan 24 04:34:36 2022
epoch 3 training:  36%|███▋      | 71597/196351 [10:10:32<17:42:13,  1.96it/s]global step 464300, epoch: 3, batch: 71598, loss: 1.56219, acc: 0.95742, time: Mon Jan 24 04:35:27 2022
epoch 3 training:  37%|███▋      | 71697/196351 [10:11:23<17:32:22,  1.97it/s]global step 464400, epoch: 3, batch: 71698, loss: 1.19727, acc: 0.95744, time: Mon Jan 24 04:36:18 2022
epoch 3 training:  37%|███▋      | 71797/196351 [10:12:14<17:44:12,  1.95it/s]global step 464500, epoch: 3, batch: 71798, loss: 2.49544, acc: 0.95739, time: Mon Jan 24 04:37:09 2022
epoch 3 training:  37%|███▋      | 71897/196351 [10:13:04<17:34:41,  1.97it/s]global step 464600, epoch: 3, batch: 71898, loss: 1.74201, acc: 0.95736, time: Mon Jan 24 04:37:59 2022
epoch 3 training:  37%|███▋      | 71997/196351 [10:13:55<17:43:09,  1.95it/s]global step 464700, epoch: 3, batch: 71998, loss: 0.99414, acc: 0.95739, time: Mon Jan 24 04:38:50 2022
epoch 3 training:  37%|███▋      | 72097/196351 [10:14:46<17:18:29,  1.99it/s]global step 464800, epoch: 3, batch: 72098, loss: 0.85479, acc: 0.95742, time: Mon Jan 24 04:39:41 2022
epoch 3 training:  37%|███▋      | 72197/196351 [10:15:37<17:23:21,  1.98it/s]global step 464900, epoch: 3, batch: 72198, loss: 2.29698, acc: 0.95745, time: Mon Jan 24 04:40:32 2022
epoch 3 training:  37%|███▋      | 72297/196351 [10:16:28<17:27:00,  1.97it/s]global step 465000, epoch: 3, batch: 72298, loss: 0.71981, acc: 0.95744, time: Mon Jan 24 04:41:23 2022
epoch 3 training:  37%|███▋      | 72397/196351 [10:17:19<17:23:40,  1.98it/s]global step 465100, epoch: 3, batch: 72398, loss: 0.67601, acc: 0.95737, time: Mon Jan 24 04:42:13 2022
epoch 3 training:  37%|███▋      | 72497/196351 [10:18:10<17:43:10,  1.94it/s]global step 465200, epoch: 3, batch: 72498, loss: 0.82722, acc: 0.95739, time: Mon Jan 24 04:43:04 2022
epoch 3 training:  37%|███▋      | 72597/196351 [10:19:00<17:33:42,  1.96it/s]global step 465300, epoch: 3, batch: 72598, loss: 1.19176, acc: 0.95743, time: Mon Jan 24 04:43:55 2022
epoch 3 training:  37%|███▋      | 72697/196351 [10:19:52<17:35:27,  1.95it/s]global step 465400, epoch: 3, batch: 72698, loss: 0.40498, acc: 0.95747, time: Mon Jan 24 04:44:46 2022
epoch 3 training:  37%|███▋      | 72797/196351 [10:20:43<17:14:13,  1.99it/s]global step 465500, epoch: 3, batch: 72798, loss: 1.98559, acc: 0.95745, time: Mon Jan 24 04:45:38 2022
epoch 3 training:  37%|███▋      | 72897/196351 [10:21:34<17:33:59,  1.95it/s]global step 465600, epoch: 3, batch: 72898, loss: 3.04180, acc: 0.95743, time: Mon Jan 24 04:46:29 2022
epoch 3 training:  37%|███▋      | 72997/196351 [10:22:25<17:32:54,  1.95it/s]global step 465700, epoch: 3, batch: 72998, loss: 0.45432, acc: 0.95742, time: Mon Jan 24 04:47:20 2022
epoch 3 training:  37%|███▋      | 73097/196351 [10:23:16<17:34:11,  1.95it/s]global step 465800, epoch: 3, batch: 73098, loss: 1.28279, acc: 0.95746, time: Mon Jan 24 04:48:11 2022
epoch 3 training:  37%|███▋      | 73197/196351 [10:24:08<17:28:14,  1.96it/s]global step 465900, epoch: 3, batch: 73198, loss: 0.57522, acc: 0.95748, time: Mon Jan 24 04:49:03 2022
epoch 3 training:  37%|███▋      | 73297/196351 [10:24:59<17:08:22,  1.99it/s]global step 466000, epoch: 3, batch: 73298, loss: 0.14889, acc: 0.95747, time: Mon Jan 24 04:49:53 2022
epoch 3 training:  37%|███▋      | 73397/196351 [10:25:49<17:09:22,  1.99it/s]global step 466100, epoch: 3, batch: 73398, loss: 0.22188, acc: 0.95746, time: Mon Jan 24 04:50:44 2022
epoch 3 training:  37%|███▋      | 73497/196351 [10:26:40<17:15:53,  1.98it/s]global step 466200, epoch: 3, batch: 73498, loss: 0.45437, acc: 0.95746, time: Mon Jan 24 04:51:35 2022
epoch 3 training:  37%|███▋      | 73597/196351 [10:27:31<17:39:26,  1.93it/s]global step 466300, epoch: 3, batch: 73598, loss: 1.19085, acc: 0.95747, time: Mon Jan 24 04:52:26 2022
epoch 3 training:  38%|███▊      | 73697/196351 [10:28:22<17:36:56,  1.93it/s]global step 466400, epoch: 3, batch: 73698, loss: 2.03520, acc: 0.95748, time: Mon Jan 24 04:53:17 2022
epoch 3 training:  38%|███▊      | 73797/196351 [10:29:13<17:22:11,  1.96it/s]global step 466500, epoch: 3, batch: 73798, loss: 2.43903, acc: 0.95746, time: Mon Jan 24 04:54:08 2022
epoch 3 training:  38%|███▊      | 73897/196351 [10:30:04<17:19:36,  1.96it/s]global step 466600, epoch: 3, batch: 73898, loss: 1.98570, acc: 0.95750, time: Mon Jan 24 04:54:59 2022
epoch 3 training:  38%|███▊      | 73997/196351 [10:30:55<17:14:01,  1.97it/s]global step 466700, epoch: 3, batch: 73998, loss: 0.22373, acc: 0.95752, time: Mon Jan 24 04:55:49 2022
epoch 3 training:  38%|███▊      | 74097/196351 [10:31:46<17:15:46,  1.97it/s]global step 466800, epoch: 3, batch: 74098, loss: 1.26901, acc: 0.95748, time: Mon Jan 24 04:56:40 2022
epoch 3 training:  38%|███▊      | 74197/196351 [10:32:37<17:22:38,  1.95it/s]global step 466900, epoch: 3, batch: 74198, loss: 0.26793, acc: 0.95747, time: Mon Jan 24 04:57:32 2022
epoch 3 training:  38%|███▊      | 74297/196351 [10:33:28<17:15:03,  1.97it/s]global step 467000, epoch: 3, batch: 74298, loss: 0.52469, acc: 0.95745, time: Mon Jan 24 04:58:23 2022
epoch 3 training:  38%|███▊      | 74397/196351 [10:34:19<17:18:57,  1.96it/s]global step 467100, epoch: 3, batch: 74398, loss: 1.24821, acc: 0.95747, time: Mon Jan 24 04:59:14 2022
epoch 3 training:  38%|███▊      | 74497/196351 [10:35:10<17:15:37,  1.96it/s]global step 467200, epoch: 3, batch: 74498, loss: 0.44227, acc: 0.95746, time: Mon Jan 24 05:00:05 2022
epoch 3 training:  38%|███▊      | 74597/196351 [10:36:01<16:57:29,  1.99it/s]global step 467300, epoch: 3, batch: 74598, loss: 1.70868, acc: 0.95744, time: Mon Jan 24 05:00:56 2022
epoch 3 training:  38%|███▊      | 74697/196351 [10:36:52<17:18:32,  1.95it/s]global step 467400, epoch: 3, batch: 74698, loss: 5.59947, acc: 0.95742, time: Mon Jan 24 05:01:47 2022
epoch 3 training:  38%|███▊      | 74797/196351 [10:37:43<16:57:50,  1.99it/s]global step 467500, epoch: 3, batch: 74798, loss: 2.00236, acc: 0.95739, time: Mon Jan 24 05:02:38 2022
epoch 3 training:  38%|███▊      | 74897/196351 [10:38:34<17:07:55,  1.97it/s]global step 467600, epoch: 3, batch: 74898, loss: 0.42008, acc: 0.95739, time: Mon Jan 24 05:03:28 2022
epoch 3 training:  38%|███▊      | 74997/196351 [10:39:24<17:12:10,  1.96it/s]global step 467700, epoch: 3, batch: 74998, loss: 0.45281, acc: 0.95739, time: Mon Jan 24 05:04:19 2022
epoch 3 training:  38%|███▊      | 75097/196351 [10:40:15<16:54:50,  1.99it/s]global step 467800, epoch: 3, batch: 75098, loss: 2.44326, acc: 0.95742, time: Mon Jan 24 05:05:10 2022
epoch 3 training:  38%|███▊      | 75197/196351 [10:41:06<17:06:03,  1.97it/s]global step 467900, epoch: 3, batch: 75198, loss: 0.23320, acc: 0.95740, time: Mon Jan 24 05:06:01 2022
epoch 3 training:  38%|███▊      | 75297/196351 [10:41:56<17:11:15,  1.96it/s]global step 468000, epoch: 3, batch: 75298, loss: 1.17213, acc: 0.95739, time: Mon Jan 24 05:06:51 2022
epoch 3 training:  38%|███▊      | 75397/196351 [10:42:47<17:01:24,  1.97it/s]global step 468100, epoch: 3, batch: 75398, loss: 1.10751, acc: 0.95741, time: Mon Jan 24 05:07:42 2022
epoch 3 training:  38%|███▊      | 75497/196351 [10:43:38<17:00:46,  1.97it/s]global step 468200, epoch: 3, batch: 75498, loss: 0.05699, acc: 0.95740, time: Mon Jan 24 05:08:32 2022
epoch 3 training:  39%|███▊      | 75597/196351 [10:44:29<16:57:11,  1.98it/s]global step 468300, epoch: 3, batch: 75598, loss: 0.25449, acc: 0.95742, time: Mon Jan 24 05:09:24 2022
epoch 3 training:  39%|███▊      | 75697/196351 [10:45:20<17:13:20,  1.95it/s]global step 468400, epoch: 3, batch: 75698, loss: 2.06708, acc: 0.95743, time: Mon Jan 24 05:10:15 2022
epoch 3 training:  39%|███▊      | 75797/196351 [10:46:11<16:57:24,  1.97it/s]global step 468500, epoch: 3, batch: 75798, loss: 0.90554, acc: 0.95743, time: Mon Jan 24 05:11:06 2022
epoch 3 training:  39%|███▊      | 75897/196351 [10:47:01<16:55:21,  1.98it/s]global step 468600, epoch: 3, batch: 75898, loss: 0.32262, acc: 0.95741, time: Mon Jan 24 05:11:56 2022
epoch 3 training:  39%|███▊      | 75997/196351 [10:47:53<16:52:22,  1.98it/s]global step 468700, epoch: 3, batch: 75998, loss: 0.26896, acc: 0.95742, time: Mon Jan 24 05:12:47 2022
epoch 3 training:  39%|███▉      | 76097/196351 [10:48:44<17:02:16,  1.96it/s]global step 468800, epoch: 3, batch: 76098, loss: 2.11400, acc: 0.95744, time: Mon Jan 24 05:13:39 2022
epoch 3 training:  39%|███▉      | 76197/196351 [10:49:35<17:02:48,  1.96it/s]global step 468900, epoch: 3, batch: 76198, loss: 0.36504, acc: 0.95745, time: Mon Jan 24 05:14:30 2022
epoch 3 training:  39%|███▉      | 76297/196351 [10:50:26<16:54:45,  1.97it/s]global step 469000, epoch: 3, batch: 76298, loss: 0.91893, acc: 0.95745, time: Mon Jan 24 05:15:21 2022
epoch 3 training:  39%|███▉      | 76397/196351 [10:51:16<16:46:41,  1.99it/s]global step 469100, epoch: 3, batch: 76398, loss: 0.21107, acc: 0.95743, time: Mon Jan 24 05:16:11 2022
epoch 3 training:  39%|███▉      | 76497/196351 [10:52:07<16:47:44,  1.98it/s]global step 469200, epoch: 3, batch: 76498, loss: 1.34440, acc: 0.95744, time: Mon Jan 24 05:17:02 2022
epoch 3 training:  39%|███▉      | 76597/196351 [10:52:58<17:12:09,  1.93it/s]global step 469300, epoch: 3, batch: 76598, loss: 0.78970, acc: 0.95741, time: Mon Jan 24 05:17:53 2022
epoch 3 training:  39%|███▉      | 76697/196351 [10:53:50<17:02:35,  1.95it/s]global step 469400, epoch: 3, batch: 76698, loss: 1.03940, acc: 0.95740, time: Mon Jan 24 05:18:44 2022
epoch 3 training:  39%|███▉      | 76797/196351 [10:54:41<17:00:06,  1.95it/s]global step 469500, epoch: 3, batch: 76798, loss: 0.66262, acc: 0.95740, time: Mon Jan 24 05:19:36 2022
epoch 3 training:  39%|███▉      | 76897/196351 [10:55:32<16:45:45,  1.98it/s]global step 469600, epoch: 3, batch: 76898, loss: 0.94626, acc: 0.95742, time: Mon Jan 24 05:20:27 2022
epoch 3 training:  39%|███▉      | 76997/196351 [10:56:23<16:48:08,  1.97it/s]global step 469700, epoch: 3, batch: 76998, loss: 0.92028, acc: 0.95738, time: Mon Jan 24 05:21:18 2022
epoch 3 training:  39%|███▉      | 77097/196351 [10:57:14<16:39:56,  1.99it/s]global step 469800, epoch: 3, batch: 77098, loss: 0.43543, acc: 0.95739, time: Mon Jan 24 05:22:09 2022
epoch 3 training:  39%|███▉      | 77197/196351 [10:58:05<16:40:16,  1.99it/s]global step 469900, epoch: 3, batch: 77198, loss: 1.39496, acc: 0.95738, time: Mon Jan 24 05:23:00 2022
epoch 3 training:  39%|███▉      | 77297/196351 [10:58:55<16:52:05,  1.96it/s]global step 470000, epoch: 3, batch: 77298, loss: 1.18476, acc: 0.95737, time: Mon Jan 24 05:23:50 2022
epoch 3 training:  39%|███▉      | 77397/196351 [10:59:47<16:49:30,  1.96it/s]global step 470100, epoch: 3, batch: 77398, loss: 0.94963, acc: 0.95738, time: Mon Jan 24 05:24:41 2022
epoch 3 training:  39%|███▉      | 77497/196351 [11:00:38<17:00:06,  1.94it/s]global step 470200, epoch: 3, batch: 77498, loss: 0.86907, acc: 0.95738, time: Mon Jan 24 05:25:33 2022
epoch 3 training:  40%|███▉      | 77597/196351 [11:01:29<16:46:53,  1.97it/s]global step 470300, epoch: 3, batch: 77598, loss: 1.53787, acc: 0.95736, time: Mon Jan 24 05:26:24 2022
epoch 3 training:  40%|███▉      | 77697/196351 [11:02:20<17:01:15,  1.94it/s]global step 470400, epoch: 3, batch: 77698, loss: 1.31568, acc: 0.95737, time: Mon Jan 24 05:27:15 2022
epoch 3 training:  40%|███▉      | 77797/196351 [11:03:11<16:38:21,  1.98it/s]global step 470500, epoch: 3, batch: 77798, loss: 0.60060, acc: 0.95739, time: Mon Jan 24 05:28:06 2022
epoch 3 training:  40%|███▉      | 77897/196351 [11:04:02<16:47:21,  1.96it/s]global step 470600, epoch: 3, batch: 77898, loss: 3.79045, acc: 0.95739, time: Mon Jan 24 05:28:57 2022
epoch 3 training:  40%|███▉      | 77997/196351 [11:04:53<16:33:37,  1.99it/s]global step 470700, epoch: 3, batch: 77998, loss: 2.57305, acc: 0.95740, time: Mon Jan 24 05:29:48 2022
epoch 3 training:  40%|███▉      | 78097/196351 [11:05:44<16:31:45,  1.99it/s]global step 470800, epoch: 3, batch: 78098, loss: 1.68912, acc: 0.95740, time: Mon Jan 24 05:30:39 2022
epoch 3 training:  40%|███▉      | 78197/196351 [11:06:35<16:54:08,  1.94it/s]global step 470900, epoch: 3, batch: 78198, loss: 1.76343, acc: 0.95742, time: Mon Jan 24 05:31:30 2022
epoch 3 training:  40%|███▉      | 78297/196351 [11:07:26<16:25:40,  2.00it/s]global step 471000, epoch: 3, batch: 78298, loss: 0.52741, acc: 0.95743, time: Mon Jan 24 05:32:21 2022
epoch 3 training:  40%|███▉      | 78397/196351 [11:08:17<16:17:46,  2.01it/s]global step 471100, epoch: 3, batch: 78398, loss: 1.05896, acc: 0.95741, time: Mon Jan 24 05:33:11 2022
epoch 3 training:  40%|███▉      | 78497/196351 [11:09:07<16:31:09,  1.98it/s]global step 471200, epoch: 3, batch: 78498, loss: 0.53888, acc: 0.95738, time: Mon Jan 24 05:34:02 2022
epoch 3 training:  40%|████      | 78597/196351 [11:09:58<16:41:52,  1.96it/s]global step 471300, epoch: 3, batch: 78598, loss: 0.40489, acc: 0.95742, time: Mon Jan 24 05:34:53 2022
epoch 3 training:  40%|████      | 78697/196351 [11:10:49<16:28:16,  1.98it/s]global step 471400, epoch: 3, batch: 78698, loss: 0.35982, acc: 0.95743, time: Mon Jan 24 05:35:44 2022
epoch 3 training:  40%|████      | 78797/196351 [11:11:40<16:27:15,  1.98it/s]global step 471500, epoch: 3, batch: 78798, loss: 0.69273, acc: 0.95738, time: Mon Jan 24 05:36:35 2022
epoch 3 training:  40%|████      | 78897/196351 [11:12:31<16:22:45,  1.99it/s]global step 471600, epoch: 3, batch: 78898, loss: 1.67589, acc: 0.95739, time: Mon Jan 24 05:37:26 2022
epoch 3 training:  40%|████      | 78997/196351 [11:13:22<16:31:46,  1.97it/s]global step 471700, epoch: 3, batch: 78998, loss: 0.64257, acc: 0.95741, time: Mon Jan 24 05:38:17 2022
epoch 3 training:  40%|████      | 79097/196351 [11:14:13<16:45:45,  1.94it/s]global step 471800, epoch: 3, batch: 79098, loss: 0.51454, acc: 0.95739, time: Mon Jan 24 05:39:07 2022
epoch 3 training:  40%|████      | 79197/196351 [11:15:04<16:27:39,  1.98it/s]global step 471900, epoch: 3, batch: 79198, loss: 0.36133, acc: 0.95740, time: Mon Jan 24 05:39:59 2022
epoch 3 training:  40%|████      | 79297/196351 [11:15:55<16:30:17,  1.97it/s]global step 472000, epoch: 3, batch: 79298, loss: 1.32539, acc: 0.95741, time: Mon Jan 24 05:40:50 2022
epoch 3 training:  40%|████      | 79397/196351 [11:16:46<16:23:55,  1.98it/s]global step 472100, epoch: 3, batch: 79398, loss: 4.03900, acc: 0.95741, time: Mon Jan 24 05:41:41 2022
epoch 3 training:  40%|████      | 79497/196351 [11:17:37<16:34:52,  1.96it/s]global step 472200, epoch: 3, batch: 79498, loss: 0.37968, acc: 0.95741, time: Mon Jan 24 05:42:32 2022
epoch 3 training:  41%|████      | 79597/196351 [11:18:28<16:22:24,  1.98it/s]global step 472300, epoch: 3, batch: 79598, loss: 1.73346, acc: 0.95742, time: Mon Jan 24 05:43:23 2022
epoch 3 training:  41%|████      | 79697/196351 [11:19:19<16:31:27,  1.96it/s]global step 472400, epoch: 3, batch: 79698, loss: 0.62089, acc: 0.95745, time: Mon Jan 24 05:44:14 2022
epoch 3 training:  41%|████      | 79797/196351 [11:20:10<16:35:43,  1.95it/s]global step 472500, epoch: 3, batch: 79798, loss: 0.34560, acc: 0.95744, time: Mon Jan 24 05:45:04 2022
epoch 3 training:  41%|████      | 79897/196351 [11:21:01<16:46:40,  1.93it/s]global step 472600, epoch: 3, batch: 79898, loss: 0.10709, acc: 0.95741, time: Mon Jan 24 05:45:55 2022
epoch 3 training:  41%|████      | 79997/196351 [11:21:52<16:21:48,  1.98it/s]global step 472700, epoch: 3, batch: 79998, loss: 1.32309, acc: 0.95741, time: Mon Jan 24 05:46:46 2022
epoch 3 training:  41%|████      | 80097/196351 [11:22:42<16:10:55,  2.00it/s]global step 472800, epoch: 3, batch: 80098, loss: 2.45563, acc: 0.95738, time: Mon Jan 24 05:47:37 2022
epoch 3 training:  41%|████      | 80197/196351 [11:23:33<16:31:14,  1.95it/s]global step 472900, epoch: 3, batch: 80198, loss: 0.67392, acc: 0.95736, time: Mon Jan 24 05:48:28 2022
epoch 3 training:  41%|████      | 80297/196351 [11:24:24<16:21:06,  1.97it/s]global step 473000, epoch: 3, batch: 80298, loss: 3.94223, acc: 0.95737, time: Mon Jan 24 05:49:19 2022
epoch 3 training:  41%|████      | 80397/196351 [11:25:14<16:07:18,  2.00it/s]global step 473100, epoch: 3, batch: 80398, loss: 0.70521, acc: 0.95736, time: Mon Jan 24 05:50:09 2022
epoch 3 training:  41%|████      | 80497/196351 [11:26:05<16:16:53,  1.98it/s]global step 473200, epoch: 3, batch: 80498, loss: 0.61065, acc: 0.95737, time: Mon Jan 24 05:51:00 2022
epoch 3 training:  41%|████      | 80597/196351 [11:26:56<16:07:45,  1.99it/s]global step 473300, epoch: 3, batch: 80598, loss: 1.10921, acc: 0.95738, time: Mon Jan 24 05:51:51 2022
epoch 3 training:  41%|████      | 80697/196351 [11:27:47<16:34:03,  1.94it/s]global step 473400, epoch: 3, batch: 80698, loss: 1.93611, acc: 0.95740, time: Mon Jan 24 05:52:41 2022
epoch 3 training:  41%|████      | 80797/196351 [11:28:38<16:19:16,  1.97it/s]global step 473500, epoch: 3, batch: 80798, loss: 0.59172, acc: 0.95741, time: Mon Jan 24 05:53:32 2022
epoch 3 training:  41%|████      | 80897/196351 [11:29:28<16:41:27,  1.92it/s]global step 473600, epoch: 3, batch: 80898, loss: 0.97640, acc: 0.95743, time: Mon Jan 24 05:54:23 2022
epoch 3 training:  41%|████▏     | 80997/196351 [11:30:20<16:07:58,  1.99it/s]global step 473700, epoch: 3, batch: 80998, loss: 1.53643, acc: 0.95744, time: Mon Jan 24 05:55:14 2022
epoch 3 training:  41%|████▏     | 81097/196351 [11:31:10<16:24:00,  1.95it/s]global step 473800, epoch: 3, batch: 81098, loss: 0.16726, acc: 0.95745, time: Mon Jan 24 05:56:05 2022
epoch 3 training:  41%|████▏     | 81197/196351 [11:32:01<16:09:58,  1.98it/s]global step 473900, epoch: 3, batch: 81198, loss: 0.66800, acc: 0.95746, time: Mon Jan 24 05:56:56 2022
epoch 3 training:  41%|████▏     | 81297/196351 [11:32:52<16:26:38,  1.94it/s]global step 474000, epoch: 3, batch: 81298, loss: 2.53981, acc: 0.95747, time: Mon Jan 24 05:57:47 2022
epoch 3 training:  41%|████▏     | 81397/196351 [11:33:43<16:02:20,  1.99it/s]global step 474100, epoch: 3, batch: 81398, loss: 0.24143, acc: 0.95749, time: Mon Jan 24 05:58:38 2022
epoch 3 training:  42%|████▏     | 81497/196351 [11:34:34<16:23:47,  1.95it/s]global step 474200, epoch: 3, batch: 81498, loss: 1.96789, acc: 0.95743, time: Mon Jan 24 05:59:28 2022
epoch 3 training:  42%|████▏     | 81597/196351 [11:35:24<15:57:45,  2.00it/s]global step 474300, epoch: 3, batch: 81598, loss: 2.43613, acc: 0.95744, time: Mon Jan 24 06:00:19 2022
epoch 3 training:  42%|████▏     | 81697/196351 [11:36:15<16:09:47,  1.97it/s]global step 474400, epoch: 3, batch: 81698, loss: 1.41935, acc: 0.95744, time: Mon Jan 24 06:01:10 2022
epoch 3 training:  42%|████▏     | 81797/196351 [11:37:06<16:10:19,  1.97it/s]global step 474500, epoch: 3, batch: 81798, loss: 1.61751, acc: 0.95743, time: Mon Jan 24 06:02:01 2022
epoch 3 training:  42%|████▏     | 81897/196351 [11:37:57<16:23:29,  1.94it/s]global step 474600, epoch: 3, batch: 81898, loss: 1.83652, acc: 0.95743, time: Mon Jan 24 06:02:52 2022
epoch 3 training:  42%|████▏     | 81997/196351 [11:38:48<16:15:13,  1.95it/s]global step 474700, epoch: 3, batch: 81998, loss: 0.23360, acc: 0.95741, time: Mon Jan 24 06:03:42 2022
epoch 3 training:  42%|████▏     | 82097/196351 [11:39:38<16:20:33,  1.94it/s]global step 474800, epoch: 3, batch: 82098, loss: 1.97494, acc: 0.95743, time: Mon Jan 24 06:04:33 2022
epoch 3 training:  42%|████▏     | 82197/196351 [11:40:29<16:03:49,  1.97it/s]global step 474900, epoch: 3, batch: 82198, loss: 2.03873, acc: 0.95738, time: Mon Jan 24 06:05:24 2022
epoch 3 training:  42%|████▏     | 82297/196351 [11:41:20<16:07:24,  1.96it/s]global step 475000, epoch: 3, batch: 82298, loss: 0.18781, acc: 0.95739, time: Mon Jan 24 06:06:15 2022
epoch 3 training:  42%|████▏     | 82397/196351 [11:42:11<16:01:15,  1.98it/s]global step 475100, epoch: 3, batch: 82398, loss: 2.73732, acc: 0.95741, time: Mon Jan 24 06:07:06 2022
epoch 3 training:  42%|████▏     | 82497/196351 [11:43:01<16:04:34,  1.97it/s]global step 475200, epoch: 3, batch: 82498, loss: 0.29607, acc: 0.95737, time: Mon Jan 24 06:07:56 2022
epoch 3 training:  42%|████▏     | 82597/196351 [11:43:52<16:03:25,  1.97it/s]global step 475300, epoch: 3, batch: 82598, loss: 2.72052, acc: 0.95731, time: Mon Jan 24 06:08:47 2022
epoch 3 training:  42%|████▏     | 82697/196351 [11:44:43<15:51:31,  1.99it/s]global step 475400, epoch: 3, batch: 82698, loss: 2.71994, acc: 0.95732, time: Mon Jan 24 06:09:38 2022
epoch 3 training:  42%|████▏     | 82797/196351 [11:45:34<16:13:09,  1.94it/s]global step 475500, epoch: 3, batch: 82798, loss: 0.59694, acc: 0.95730, time: Mon Jan 24 06:10:29 2022
epoch 3 training:  42%|████▏     | 82897/196351 [11:46:26<16:02:10,  1.97it/s]global step 475600, epoch: 3, batch: 82898, loss: 0.87807, acc: 0.95730, time: Mon Jan 24 06:11:21 2022
epoch 3 training:  42%|████▏     | 82997/196351 [11:47:17<16:01:40,  1.96it/s]global step 475700, epoch: 3, batch: 82998, loss: 0.69297, acc: 0.95733, time: Mon Jan 24 06:12:11 2022
epoch 3 training:  42%|████▏     | 83097/196351 [11:48:08<16:01:59,  1.96it/s]global step 475800, epoch: 3, batch: 83098, loss: 1.32583, acc: 0.95731, time: Mon Jan 24 06:13:03 2022
epoch 3 training:  42%|████▏     | 83197/196351 [11:48:59<16:15:11,  1.93it/s]global step 475900, epoch: 3, batch: 83198, loss: 0.17034, acc: 0.95732, time: Mon Jan 24 06:13:54 2022
epoch 3 training:  42%|████▏     | 83297/196351 [11:49:50<16:10:20,  1.94it/s]global step 476000, epoch: 3, batch: 83298, loss: 1.29902, acc: 0.95733, time: Mon Jan 24 06:14:45 2022
epoch 3 training:  42%|████▏     | 83397/196351 [11:50:41<16:14:52,  1.93it/s]global step 476100, epoch: 3, batch: 83398, loss: 2.24555, acc: 0.95732, time: Mon Jan 24 06:15:36 2022
epoch 3 training:  43%|████▎     | 83497/196351 [11:51:33<16:08:00,  1.94it/s]global step 476200, epoch: 3, batch: 83498, loss: 0.10648, acc: 0.95730, time: Mon Jan 24 06:16:28 2022
epoch 3 training:  43%|████▎     | 83597/196351 [11:52:24<16:22:46,  1.91it/s]global step 476300, epoch: 3, batch: 83598, loss: 1.76241, acc: 0.95729, time: Mon Jan 24 06:17:19 2022
epoch 3 training:  43%|████▎     | 83697/196351 [11:53:15<15:57:12,  1.96it/s]global step 476400, epoch: 3, batch: 83698, loss: 1.06613, acc: 0.95729, time: Mon Jan 24 06:18:10 2022
epoch 3 training:  43%|████▎     | 83797/196351 [11:54:07<15:53:26,  1.97it/s]global step 476500, epoch: 3, batch: 83798, loss: 0.23525, acc: 0.95730, time: Mon Jan 24 06:19:02 2022
epoch 3 training:  43%|████▎     | 83897/196351 [11:54:58<16:14:45,  1.92it/s]global step 476600, epoch: 3, batch: 83898, loss: 0.79945, acc: 0.95732, time: Mon Jan 24 06:19:53 2022
epoch 3 training:  43%|████▎     | 83997/196351 [11:55:49<15:50:36,  1.97it/s]global step 476700, epoch: 3, batch: 83998, loss: 0.22104, acc: 0.95730, time: Mon Jan 24 06:20:44 2022
epoch 3 training:  43%|████▎     | 84097/196351 [11:56:40<15:48:55,  1.97it/s]global step 476800, epoch: 3, batch: 84098, loss: 0.88714, acc: 0.95732, time: Mon Jan 24 06:21:35 2022
epoch 3 training:  43%|████▎     | 84197/196351 [11:57:31<15:36:30,  2.00it/s]global step 476900, epoch: 3, batch: 84198, loss: 1.31174, acc: 0.95735, time: Mon Jan 24 06:22:25 2022
epoch 3 training:  43%|████▎     | 84297/196351 [11:58:21<15:56:29,  1.95it/s]global step 477000, epoch: 3, batch: 84298, loss: 1.27884, acc: 0.95732, time: Mon Jan 24 06:23:16 2022
epoch 3 training:  43%|████▎     | 84397/196351 [11:59:12<15:39:35,  1.99it/s]global step 477100, epoch: 3, batch: 84398, loss: 1.96229, acc: 0.95735, time: Mon Jan 24 06:24:07 2022
epoch 3 training:  43%|████▎     | 84497/196351 [12:00:02<15:32:24,  2.00it/s]global step 477200, epoch: 3, batch: 84498, loss: 0.22817, acc: 0.95732, time: Mon Jan 24 06:24:57 2022
epoch 3 training:  43%|████▎     | 84597/196351 [12:00:53<16:06:11,  1.93it/s]global step 477300, epoch: 3, batch: 84598, loss: 1.00499, acc: 0.95730, time: Mon Jan 24 06:25:48 2022
epoch 3 training:  43%|████▎     | 84697/196351 [12:01:44<15:43:15,  1.97it/s]global step 477400, epoch: 3, batch: 84698, loss: 0.72357, acc: 0.95731, time: Mon Jan 24 06:26:39 2022
epoch 3 training:  43%|████▎     | 84797/196351 [12:02:36<15:43:05,  1.97it/s]global step 477500, epoch: 3, batch: 84798, loss: 1.68259, acc: 0.95730, time: Mon Jan 24 06:27:30 2022
epoch 3 training:  43%|████▎     | 84897/196351 [12:03:27<15:53:21,  1.95it/s]global step 477600, epoch: 3, batch: 84898, loss: 0.94383, acc: 0.95732, time: Mon Jan 24 06:28:21 2022
epoch 3 training:  43%|████▎     | 84997/196351 [12:04:18<15:49:17,  1.96it/s]global step 477700, epoch: 3, batch: 84998, loss: 2.60750, acc: 0.95732, time: Mon Jan 24 06:29:12 2022
epoch 3 training:  43%|████▎     | 85097/196351 [12:05:09<15:39:06,  1.97it/s]global step 477800, epoch: 3, batch: 85098, loss: 0.38986, acc: 0.95733, time: Mon Jan 24 06:30:03 2022
epoch 3 training:  43%|████▎     | 85197/196351 [12:06:00<15:45:01,  1.96it/s]global step 477900, epoch: 3, batch: 85198, loss: 1.56811, acc: 0.95734, time: Mon Jan 24 06:30:54 2022
epoch 3 training:  43%|████▎     | 85297/196351 [12:06:50<15:45:18,  1.96it/s]global step 478000, epoch: 3, batch: 85298, loss: 1.22340, acc: 0.95734, time: Mon Jan 24 06:31:45 2022
epoch 3 training:  43%|████▎     | 85397/196351 [12:07:41<15:41:01,  1.97it/s]global step 478100, epoch: 3, batch: 85398, loss: 0.82239, acc: 0.95732, time: Mon Jan 24 06:32:36 2022
epoch 3 training:  44%|████▎     | 85497/196351 [12:08:32<15:44:39,  1.96it/s]global step 478200, epoch: 3, batch: 85498, loss: 0.87512, acc: 0.95734, time: Mon Jan 24 06:33:27 2022
epoch 3 training:  44%|████▎     | 85597/196351 [12:09:24<15:35:52,  1.97it/s]global step 478300, epoch: 3, batch: 85598, loss: 1.38299, acc: 0.95735, time: Mon Jan 24 06:34:19 2022
epoch 3 training:  44%|████▎     | 85697/196351 [12:10:14<15:32:18,  1.98it/s]global step 478400, epoch: 3, batch: 85698, loss: 0.40554, acc: 0.95735, time: Mon Jan 24 06:35:09 2022
epoch 3 training:  44%|████▎     | 85797/196351 [12:11:05<15:32:31,  1.98it/s]global step 478500, epoch: 3, batch: 85798, loss: 0.26191, acc: 0.95736, time: Mon Jan 24 06:36:00 2022
epoch 3 training:  44%|████▎     | 85897/196351 [12:11:56<15:22:49,  1.99it/s]global step 478600, epoch: 3, batch: 85898, loss: 1.29767, acc: 0.95736, time: Mon Jan 24 06:36:51 2022
epoch 3 training:  44%|████▍     | 85997/196351 [12:12:47<15:27:02,  1.98it/s]global step 478700, epoch: 3, batch: 85998, loss: 1.96565, acc: 0.95736, time: Mon Jan 24 06:37:42 2022
epoch 3 training:  44%|████▍     | 86097/196351 [12:13:38<15:29:20,  1.98it/s]global step 478800, epoch: 3, batch: 86098, loss: 0.25440, acc: 0.95737, time: Mon Jan 24 06:38:32 2022
epoch 3 training:  44%|████▍     | 86197/196351 [12:14:28<15:29:32,  1.98it/s]global step 478900, epoch: 3, batch: 86198, loss: 0.51172, acc: 0.95739, time: Mon Jan 24 06:39:23 2022
epoch 3 training:  44%|████▍     | 86297/196351 [12:15:19<15:29:37,  1.97it/s]global step 479000, epoch: 3, batch: 86298, loss: 1.36994, acc: 0.95739, time: Mon Jan 24 06:40:14 2022
epoch 3 training:  44%|████▍     | 86397/196351 [12:16:10<15:28:07,  1.97it/s]global step 479100, epoch: 3, batch: 86398, loss: 1.21916, acc: 0.95739, time: Mon Jan 24 06:41:05 2022
epoch 3 training:  44%|████▍     | 86497/196351 [12:17:01<15:33:14,  1.96it/s]global step 479200, epoch: 3, batch: 86498, loss: 0.53906, acc: 0.95737, time: Mon Jan 24 06:41:55 2022
epoch 3 training:  44%|████▍     | 86597/196351 [12:17:51<15:34:22,  1.96it/s]global step 479300, epoch: 3, batch: 86598, loss: 0.55862, acc: 0.95735, time: Mon Jan 24 06:42:46 2022
epoch 3 training:  44%|████▍     | 86697/196351 [12:18:42<15:26:16,  1.97it/s]global step 479400, epoch: 3, batch: 86698, loss: 0.64962, acc: 0.95735, time: Mon Jan 24 06:43:37 2022
epoch 3 training:  44%|████▍     | 86797/196351 [12:19:33<15:27:56,  1.97it/s]global step 479500, epoch: 3, batch: 86798, loss: 2.05041, acc: 0.95734, time: Mon Jan 24 06:44:27 2022
epoch 3 training:  44%|████▍     | 86897/196351 [12:20:23<15:36:04,  1.95it/s]global step 479600, epoch: 3, batch: 86898, loss: 2.48238, acc: 0.95737, time: Mon Jan 24 06:45:18 2022
epoch 3 training:  44%|████▍     | 86997/196351 [12:21:14<15:36:04,  1.95it/s]global step 479700, epoch: 3, batch: 86998, loss: 2.03847, acc: 0.95736, time: Mon Jan 24 06:46:09 2022
epoch 3 training:  44%|████▍     | 87097/196351 [12:22:05<15:42:28,  1.93it/s]global step 479800, epoch: 3, batch: 87098, loss: 0.69418, acc: 0.95736, time: Mon Jan 24 06:47:00 2022
epoch 3 training:  44%|████▍     | 87197/196351 [12:22:56<15:59:23,  1.90it/s]global step 479900, epoch: 3, batch: 87198, loss: 0.52837, acc: 0.95737, time: Mon Jan 24 06:47:51 2022
epoch 3 training:  44%|████▍     | 87297/196351 [12:23:47<16:16:26,  1.86it/s]global step 480000, epoch: 3, batch: 87298, loss: 1.14166, acc: 0.95737, time: Mon Jan 24 06:48:42 2022
epoch 3 training:  45%|████▍     | 87397/196351 [12:24:38<15:12:01,  1.99it/s]global step 480100, epoch: 3, batch: 87398, loss: 0.46323, acc: 0.95736, time: Mon Jan 24 06:49:33 2022
epoch 3 training:  45%|████▍     | 87497/196351 [12:25:29<15:15:27,  1.98it/s]global step 480200, epoch: 3, batch: 87498, loss: 0.75401, acc: 0.95736, time: Mon Jan 24 06:50:24 2022
epoch 3 training:  45%|████▍     | 87597/196351 [12:26:20<15:24:26,  1.96it/s]global step 480300, epoch: 3, batch: 87598, loss: 0.42912, acc: 0.95737, time: Mon Jan 24 06:51:15 2022
epoch 3 training:  45%|████▍     | 87697/196351 [12:27:11<15:19:32,  1.97it/s]global step 480400, epoch: 3, batch: 87698, loss: 2.41915, acc: 0.95736, time: Mon Jan 24 06:52:06 2022
epoch 3 training:  45%|████▍     | 87797/196351 [12:28:02<15:28:43,  1.95it/s]global step 480500, epoch: 3, batch: 87798, loss: 1.97102, acc: 0.95735, time: Mon Jan 24 06:52:56 2022
epoch 3 training:  45%|████▍     | 87897/196351 [12:28:52<15:16:13,  1.97it/s]global step 480600, epoch: 3, batch: 87898, loss: 1.57478, acc: 0.95739, time: Mon Jan 24 06:53:47 2022
epoch 3 training:  45%|████▍     | 87997/196351 [12:29:43<15:21:22,  1.96it/s]global step 480700, epoch: 3, batch: 87998, loss: 0.48445, acc: 0.95741, time: Mon Jan 24 06:54:38 2022
epoch 3 training:  45%|████▍     | 88097/196351 [12:30:34<15:29:59,  1.94it/s]global step 480800, epoch: 3, batch: 88098, loss: 0.58043, acc: 0.95741, time: Mon Jan 24 06:55:29 2022
epoch 3 training:  45%|████▍     | 88197/196351 [12:31:25<15:13:37,  1.97it/s]global step 480900, epoch: 3, batch: 88198, loss: 0.81023, acc: 0.95741, time: Mon Jan 24 06:56:20 2022
epoch 3 training:  45%|████▍     | 88297/196351 [12:32:16<15:04:25,  1.99it/s]global step 481000, epoch: 3, batch: 88298, loss: 2.50163, acc: 0.95741, time: Mon Jan 24 06:57:11 2022
epoch 3 training:  45%|████▌     | 88397/196351 [12:33:07<15:23:10,  1.95it/s]global step 481100, epoch: 3, batch: 88398, loss: 2.78762, acc: 0.95745, time: Mon Jan 24 06:58:02 2022
epoch 3 training:  45%|████▌     | 88497/196351 [12:33:57<15:16:08,  1.96it/s]global step 481200, epoch: 3, batch: 88498, loss: 2.22146, acc: 0.95746, time: Mon Jan 24 06:58:52 2022
epoch 3 training:  45%|████▌     | 88597/196351 [12:34:48<15:06:40,  1.98it/s]global step 481300, epoch: 3, batch: 88598, loss: 1.15238, acc: 0.95746, time: Mon Jan 24 06:59:43 2022
epoch 3 training:  45%|████▌     | 88697/196351 [12:35:39<15:16:05,  1.96it/s]global step 481400, epoch: 3, batch: 88698, loss: 1.32675, acc: 0.95743, time: Mon Jan 24 07:00:34 2022
epoch 3 training:  45%|████▌     | 88797/196351 [12:36:30<14:58:06,  2.00it/s]global step 481500, epoch: 3, batch: 88798, loss: 0.28734, acc: 0.95740, time: Mon Jan 24 07:01:25 2022
epoch 3 training:  45%|████▌     | 88897/196351 [12:37:21<15:21:19,  1.94it/s]global step 481600, epoch: 3, batch: 88898, loss: 2.61022, acc: 0.95737, time: Mon Jan 24 07:02:16 2022
epoch 3 training:  45%|████▌     | 88997/196351 [12:38:12<14:58:49,  1.99it/s]global step 481700, epoch: 3, batch: 88998, loss: 0.74174, acc: 0.95736, time: Mon Jan 24 07:03:06 2022
epoch 3 training:  45%|████▌     | 89097/196351 [12:39:02<15:14:47,  1.95it/s]global step 481800, epoch: 3, batch: 89098, loss: 1.92425, acc: 0.95737, time: Mon Jan 24 07:03:57 2022
epoch 3 training:  45%|████▌     | 89197/196351 [12:39:53<15:12:01,  1.96it/s]global step 481900, epoch: 3, batch: 89198, loss: 2.28142, acc: 0.95736, time: Mon Jan 24 07:04:48 2022
epoch 3 training:  45%|████▌     | 89297/196351 [12:40:44<15:08:42,  1.96it/s]global step 482000, epoch: 3, batch: 89298, loss: 0.55565, acc: 0.95738, time: Mon Jan 24 07:05:39 2022
epoch 3 training:  46%|████▌     | 89397/196351 [12:41:35<14:52:30,  2.00it/s]global step 482100, epoch: 3, batch: 89398, loss: 0.53497, acc: 0.95740, time: Mon Jan 24 07:06:30 2022
epoch 3 training:  46%|████▌     | 89497/196351 [12:42:26<15:07:30,  1.96it/s]global step 482200, epoch: 3, batch: 89498, loss: 0.82233, acc: 0.95740, time: Mon Jan 24 07:07:21 2022
epoch 3 training:  46%|████▌     | 89597/196351 [12:43:17<15:08:55,  1.96it/s]global step 482300, epoch: 3, batch: 89598, loss: 1.55876, acc: 0.95740, time: Mon Jan 24 07:08:12 2022
epoch 3 training:  46%|████▌     | 89697/196351 [12:44:08<15:13:00,  1.95it/s]global step 482400, epoch: 3, batch: 89698, loss: 0.89991, acc: 0.95738, time: Mon Jan 24 07:09:03 2022
epoch 3 training:  46%|████▌     | 89797/196351 [12:44:59<14:57:31,  1.98it/s]global step 482500, epoch: 3, batch: 89798, loss: 1.03941, acc: 0.95737, time: Mon Jan 24 07:09:54 2022
epoch 3 training:  46%|████▌     | 89897/196351 [12:45:51<15:06:49,  1.96it/s]global step 482600, epoch: 3, batch: 89898, loss: 3.01295, acc: 0.95740, time: Mon Jan 24 07:10:45 2022
epoch 3 training:  46%|████▌     | 89997/196351 [12:46:41<15:05:03,  1.96it/s]global step 482700, epoch: 3, batch: 89998, loss: 2.83464, acc: 0.95741, time: Mon Jan 24 07:11:36 2022
epoch 3 training:  46%|████▌     | 90097/196351 [12:47:32<14:55:48,  1.98it/s]global step 482800, epoch: 3, batch: 90098, loss: 1.85241, acc: 0.95741, time: Mon Jan 24 07:12:27 2022
epoch 3 training:  46%|████▌     | 90197/196351 [12:48:24<14:58:41,  1.97it/s]global step 482900, epoch: 3, batch: 90198, loss: 0.28622, acc: 0.95740, time: Mon Jan 24 07:13:18 2022
epoch 3 training:  46%|████▌     | 90297/196351 [12:49:15<15:05:28,  1.95it/s]global step 483000, epoch: 3, batch: 90298, loss: 0.39904, acc: 0.95740, time: Mon Jan 24 07:14:09 2022
epoch 3 training:  46%|████▌     | 90397/196351 [12:50:05<15:00:49,  1.96it/s]global step 483100, epoch: 3, batch: 90398, loss: 0.38907, acc: 0.95740, time: Mon Jan 24 07:15:00 2022
epoch 3 training:  46%|████▌     | 90497/196351 [12:50:56<14:46:55,  1.99it/s]global step 483200, epoch: 3, batch: 90498, loss: 0.79456, acc: 0.95742, time: Mon Jan 24 07:15:51 2022
epoch 3 training:  46%|████▌     | 90597/196351 [12:51:47<14:44:31,  1.99it/s]global step 483300, epoch: 3, batch: 90598, loss: 0.26294, acc: 0.95744, time: Mon Jan 24 07:16:42 2022
epoch 3 training:  46%|████▌     | 90697/196351 [12:52:38<14:54:34,  1.97it/s]global step 483400, epoch: 3, batch: 90698, loss: 2.41726, acc: 0.95744, time: Mon Jan 24 07:17:33 2022
epoch 3 training:  46%|████▌     | 90797/196351 [12:53:29<14:35:06,  2.01it/s]global step 483500, epoch: 3, batch: 90798, loss: 2.20543, acc: 0.95745, time: Mon Jan 24 07:18:24 2022
epoch 3 training:  46%|████▋     | 90897/196351 [12:54:19<15:02:52,  1.95it/s]global step 483600, epoch: 3, batch: 90898, loss: 2.65771, acc: 0.95742, time: Mon Jan 24 07:19:14 2022
epoch 3 training:  46%|████▋     | 90997/196351 [12:55:10<14:43:46,  1.99it/s]global step 483700, epoch: 3, batch: 90998, loss: 1.59706, acc: 0.95743, time: Mon Jan 24 07:20:05 2022
epoch 3 training:  46%|████▋     | 91097/196351 [12:56:01<14:40:40,  1.99it/s]global step 483800, epoch: 3, batch: 91098, loss: 1.95234, acc: 0.95741, time: Mon Jan 24 07:20:56 2022
epoch 3 training:  46%|████▋     | 91197/196351 [12:56:52<14:53:02,  1.96it/s]global step 483900, epoch: 3, batch: 91198, loss: 1.19799, acc: 0.95738, time: Mon Jan 24 07:21:46 2022
epoch 3 training:  46%|████▋     | 91297/196351 [12:57:42<14:41:55,  1.99it/s]global step 484000, epoch: 3, batch: 91298, loss: 0.71967, acc: 0.95738, time: Mon Jan 24 07:22:37 2022
epoch 3 training:  47%|████▋     | 91397/196351 [12:58:33<14:48:17,  1.97it/s]global step 484100, epoch: 3, batch: 91398, loss: 3.02274, acc: 0.95737, time: Mon Jan 24 07:23:28 2022
epoch 3 training:  47%|████▋     | 91497/196351 [12:59:25<14:39:25,  1.99it/s]global step 484200, epoch: 3, batch: 91498, loss: 0.46830, acc: 0.95735, time: Mon Jan 24 07:24:19 2022
epoch 3 training:  47%|████▋     | 91597/196351 [13:00:15<14:41:55,  1.98it/s]global step 484300, epoch: 3, batch: 91598, loss: 1.56120, acc: 0.95737, time: Mon Jan 24 07:25:10 2022
epoch 3 training:  47%|████▋     | 91697/196351 [13:01:06<14:32:16,  2.00it/s]global step 484400, epoch: 3, batch: 91698, loss: 0.48795, acc: 0.95736, time: Mon Jan 24 07:26:01 2022
epoch 3 training:  47%|████▋     | 91797/196351 [13:01:57<14:33:04,  2.00it/s]global step 484500, epoch: 3, batch: 91798, loss: 1.38410, acc: 0.95733, time: Mon Jan 24 07:26:52 2022
epoch 3 training:  47%|████▋     | 91897/196351 [13:02:48<14:58:58,  1.94it/s]global step 484600, epoch: 3, batch: 91898, loss: 1.61301, acc: 0.95735, time: Mon Jan 24 07:27:43 2022
epoch 3 training:  47%|████▋     | 91997/196351 [13:03:38<14:47:44,  1.96it/s]global step 484700, epoch: 3, batch: 91998, loss: 0.59527, acc: 0.95734, time: Mon Jan 24 07:28:33 2022
epoch 3 training:  47%|████▋     | 92097/196351 [13:04:29<14:40:46,  1.97it/s]global step 484800, epoch: 3, batch: 92098, loss: 0.66418, acc: 0.95736, time: Mon Jan 24 07:29:24 2022
epoch 3 training:  47%|████▋     | 92197/196351 [13:05:20<14:51:26,  1.95it/s]global step 484900, epoch: 3, batch: 92198, loss: 1.59449, acc: 0.95733, time: Mon Jan 24 07:30:15 2022
epoch 3 training:  47%|████▋     | 92297/196351 [13:06:11<14:40:18,  1.97it/s]global step 485000, epoch: 3, batch: 92298, loss: 1.23834, acc: 0.95734, time: Mon Jan 24 07:31:05 2022
epoch 3 training:  47%|████▋     | 92397/196351 [13:07:01<14:30:38,  1.99it/s]global step 485100, epoch: 3, batch: 92398, loss: 0.47134, acc: 0.95735, time: Mon Jan 24 07:31:56 2022
epoch 3 training:  47%|████▋     | 92497/196351 [13:07:53<15:47:40,  1.83it/s]global step 485200, epoch: 3, batch: 92498, loss: 1.02232, acc: 0.95736, time: Mon Jan 24 07:32:47 2022
epoch 3 training:  47%|████▋     | 92597/196351 [13:08:43<14:41:47,  1.96it/s]global step 485300, epoch: 3, batch: 92598, loss: 0.32970, acc: 0.95737, time: Mon Jan 24 07:33:38 2022
epoch 3 training:  47%|████▋     | 92697/196351 [13:09:34<14:34:47,  1.97it/s]global step 485400, epoch: 3, batch: 92698, loss: 2.22655, acc: 0.95737, time: Mon Jan 24 07:34:29 2022
epoch 3 training:  47%|████▋     | 92797/196351 [13:10:25<14:25:51,  1.99it/s]global step 485500, epoch: 3, batch: 92798, loss: 0.66424, acc: 0.95739, time: Mon Jan 24 07:35:20 2022
epoch 3 training:  47%|████▋     | 92897/196351 [13:11:15<14:34:47,  1.97it/s]global step 485600, epoch: 3, batch: 92898, loss: 1.69677, acc: 0.95741, time: Mon Jan 24 07:36:10 2022
epoch 3 training:  47%|████▋     | 92997/196351 [13:12:06<14:44:50,  1.95it/s]global step 485700, epoch: 3, batch: 92998, loss: 0.27192, acc: 0.95740, time: Mon Jan 24 07:37:01 2022
epoch 3 training:  47%|████▋     | 93097/196351 [13:12:57<14:41:40,  1.95it/s]global step 485800, epoch: 3, batch: 93098, loss: 1.31167, acc: 0.95742, time: Mon Jan 24 07:37:52 2022
epoch 3 training:  47%|████▋     | 93197/196351 [13:13:48<14:37:08,  1.96it/s]global step 485900, epoch: 3, batch: 93198, loss: 0.52737, acc: 0.95741, time: Mon Jan 24 07:38:43 2022
epoch 3 training:  48%|████▊     | 93297/196351 [13:14:38<14:36:10,  1.96it/s]global step 486000, epoch: 3, batch: 93298, loss: 0.22667, acc: 0.95741, time: Mon Jan 24 07:39:33 2022
epoch 3 training:  48%|████▊     | 93397/196351 [13:15:29<14:31:15,  1.97it/s]global step 486100, epoch: 3, batch: 93398, loss: 1.93158, acc: 0.95741, time: Mon Jan 24 07:40:24 2022
epoch 3 training:  48%|████▊     | 93497/196351 [13:16:20<14:39:31,  1.95it/s]global step 486200, epoch: 3, batch: 93498, loss: 0.15929, acc: 0.95743, time: Mon Jan 24 07:41:15 2022
epoch 3 training:  48%|████▊     | 93597/196351 [13:17:11<14:37:32,  1.95it/s]global step 486300, epoch: 3, batch: 93598, loss: 0.28611, acc: 0.95743, time: Mon Jan 24 07:42:06 2022
epoch 3 training:  48%|████▊     | 93697/196351 [13:18:02<14:39:25,  1.95it/s]global step 486400, epoch: 3, batch: 93698, loss: 0.22497, acc: 0.95743, time: Mon Jan 24 07:42:57 2022
epoch 3 training:  48%|████▊     | 93797/196351 [13:18:53<14:34:55,  1.95it/s]global step 486500, epoch: 3, batch: 93798, loss: 0.25373, acc: 0.95745, time: Mon Jan 24 07:43:48 2022
epoch 3 training:  48%|████▊     | 93897/196351 [13:19:44<14:25:01,  1.97it/s]global step 486600, epoch: 3, batch: 93898, loss: 0.11333, acc: 0.95745, time: Mon Jan 24 07:44:39 2022
epoch 3 training:  48%|████▊     | 93997/196351 [13:20:35<14:42:53,  1.93it/s]global step 486700, epoch: 3, batch: 93998, loss: 0.71532, acc: 0.95747, time: Mon Jan 24 07:45:30 2022
epoch 3 training:  48%|████▊     | 94097/196351 [13:21:26<14:33:24,  1.95it/s]global step 486800, epoch: 3, batch: 94098, loss: 0.65155, acc: 0.95746, time: Mon Jan 24 07:46:21 2022
epoch 3 training:  48%|████▊     | 94197/196351 [13:22:17<14:14:27,  1.99it/s]global step 486900, epoch: 3, batch: 94198, loss: 0.86329, acc: 0.95742, time: Mon Jan 24 07:47:12 2022
epoch 3 training:  48%|████▊     | 94297/196351 [13:23:08<14:18:19,  1.98it/s]global step 487000, epoch: 3, batch: 94298, loss: 1.57219, acc: 0.95743, time: Mon Jan 24 07:48:03 2022
epoch 3 training:  48%|████▊     | 94397/196351 [13:23:59<14:37:09,  1.94it/s]global step 487100, epoch: 3, batch: 94398, loss: 0.60116, acc: 0.95744, time: Mon Jan 24 07:48:54 2022
epoch 3 training:  48%|████▊     | 94497/196351 [13:24:50<14:20:47,  1.97it/s]global step 487200, epoch: 3, batch: 94498, loss: 2.13844, acc: 0.95744, time: Mon Jan 24 07:49:44 2022
epoch 3 training:  48%|████▊     | 94597/196351 [13:25:40<14:24:25,  1.96it/s]global step 487300, epoch: 3, batch: 94598, loss: 0.58411, acc: 0.95740, time: Mon Jan 24 07:50:35 2022
epoch 3 training:  48%|████▊     | 94697/196351 [13:26:31<14:17:23,  1.98it/s]global step 487400, epoch: 3, batch: 94698, loss: 1.48468, acc: 0.95742, time: Mon Jan 24 07:51:26 2022
epoch 3 training:  48%|████▊     | 94797/196351 [13:27:22<14:22:03,  1.96it/s]global step 487500, epoch: 3, batch: 94798, loss: 0.33487, acc: 0.95743, time: Mon Jan 24 07:52:17 2022
epoch 3 training:  48%|████▊     | 94897/196351 [13:28:12<14:24:32,  1.96it/s]global step 487600, epoch: 3, batch: 94898, loss: 0.68017, acc: 0.95741, time: Mon Jan 24 07:53:07 2022
epoch 3 training:  48%|████▊     | 94997/196351 [13:29:03<14:20:58,  1.96it/s]global step 487700, epoch: 3, batch: 94998, loss: 0.51218, acc: 0.95741, time: Mon Jan 24 07:53:58 2022
epoch 3 training:  48%|████▊     | 95097/196351 [13:29:54<14:17:04,  1.97it/s]global step 487800, epoch: 3, batch: 95098, loss: 0.22295, acc: 0.95741, time: Mon Jan 24 07:54:49 2022
epoch 3 training:  48%|████▊     | 95197/196351 [13:30:45<14:18:44,  1.96it/s]global step 487900, epoch: 3, batch: 95198, loss: 2.06149, acc: 0.95741, time: Mon Jan 24 07:55:40 2022
epoch 3 training:  49%|████▊     | 95297/196351 [13:31:36<14:06:16,  1.99it/s]global step 488000, epoch: 3, batch: 95298, loss: 0.40706, acc: 0.95740, time: Mon Jan 24 07:56:31 2022
epoch 3 training:  49%|████▊     | 95397/196351 [13:32:28<14:13:23,  1.97it/s]global step 488100, epoch: 3, batch: 95398, loss: 0.23384, acc: 0.95740, time: Mon Jan 24 07:57:23 2022
epoch 3 training:  49%|████▊     | 95497/196351 [13:33:19<14:22:27,  1.95it/s]global step 488200, epoch: 3, batch: 95498, loss: 3.85195, acc: 0.95739, time: Mon Jan 24 07:58:14 2022
epoch 3 training:  49%|████▊     | 95597/196351 [13:34:10<14:16:41,  1.96it/s]global step 488300, epoch: 3, batch: 95598, loss: 2.91899, acc: 0.95737, time: Mon Jan 24 07:59:05 2022
epoch 3 training:  49%|████▊     | 95697/196351 [13:35:01<14:13:37,  1.97it/s]global step 488400, epoch: 3, batch: 95698, loss: 0.62814, acc: 0.95736, time: Mon Jan 24 07:59:56 2022
epoch 3 training:  49%|████▉     | 95797/196351 [13:35:52<13:59:53,  2.00it/s]global step 488500, epoch: 3, batch: 95798, loss: 2.56590, acc: 0.95735, time: Mon Jan 24 08:00:47 2022
epoch 3 training:  49%|████▉     | 95897/196351 [13:36:43<14:12:36,  1.96it/s]global step 488600, epoch: 3, batch: 95898, loss: 1.50920, acc: 0.95735, time: Mon Jan 24 08:01:38 2022
epoch 3 training:  49%|████▉     | 95997/196351 [13:37:33<14:01:57,  1.99it/s]global step 488700, epoch: 3, batch: 95998, loss: 2.45251, acc: 0.95735, time: Mon Jan 24 08:02:28 2022
epoch 3 training:  49%|████▉     | 96097/196351 [13:38:24<14:05:37,  1.98it/s]global step 488800, epoch: 3, batch: 96098, loss: 1.62730, acc: 0.95736, time: Mon Jan 24 08:03:19 2022
epoch 3 training:  49%|████▉     | 96197/196351 [13:39:15<14:10:24,  1.96it/s]global step 488900, epoch: 3, batch: 96198, loss: 1.00565, acc: 0.95738, time: Mon Jan 24 08:04:09 2022
epoch 3 training:  49%|████▉     | 96297/196351 [13:40:06<14:14:38,  1.95it/s]global step 489000, epoch: 3, batch: 96298, loss: 0.26859, acc: 0.95739, time: Mon Jan 24 08:05:01 2022
epoch 3 training:  49%|████▉     | 96397/196351 [13:40:57<14:02:59,  1.98it/s]global step 489100, epoch: 3, batch: 96398, loss: 1.36087, acc: 0.95740, time: Mon Jan 24 08:05:52 2022
epoch 3 training:  49%|████▉     | 96497/196351 [13:41:48<14:23:28,  1.93it/s]global step 489200, epoch: 3, batch: 96498, loss: 0.45586, acc: 0.95740, time: Mon Jan 24 08:06:43 2022
epoch 3 training:  49%|████▉     | 96597/196351 [13:42:39<14:10:32,  1.95it/s]global step 489300, epoch: 3, batch: 96598, loss: 1.01655, acc: 0.95740, time: Mon Jan 24 08:07:34 2022
epoch 3 training:  49%|████▉     | 96697/196351 [13:43:30<14:05:51,  1.96it/s]global step 489400, epoch: 3, batch: 96698, loss: 1.01798, acc: 0.95742, time: Mon Jan 24 08:08:25 2022
epoch 3 training:  49%|████▉     | 96797/196351 [13:44:21<14:06:55,  1.96it/s]global step 489500, epoch: 3, batch: 96798, loss: 1.78917, acc: 0.95740, time: Mon Jan 24 08:09:16 2022
epoch 3 training:  49%|████▉     | 96897/196351 [13:45:12<14:15:17,  1.94it/s]global step 489600, epoch: 3, batch: 96898, loss: 1.97489, acc: 0.95742, time: Mon Jan 24 08:10:07 2022
epoch 3 training:  49%|████▉     | 96997/196351 [13:46:03<14:05:07,  1.96it/s]global step 489700, epoch: 3, batch: 96998, loss: 2.21909, acc: 0.95740, time: Mon Jan 24 08:10:58 2022
epoch 3 training:  49%|████▉     | 97097/196351 [13:46:54<14:02:56,  1.96it/s]global step 489800, epoch: 3, batch: 97098, loss: 2.09985, acc: 0.95743, time: Mon Jan 24 08:11:49 2022
epoch 3 training:  50%|████▉     | 97197/196351 [13:47:45<14:21:55,  1.92it/s]global step 489900, epoch: 3, batch: 97198, loss: 0.70401, acc: 0.95743, time: Mon Jan 24 08:12:40 2022
epoch 3 training:  50%|████▉     | 97297/196351 [13:48:35<13:47:12,  2.00it/s]global step 490000, epoch: 3, batch: 97298, loss: 2.83194, acc: 0.95743, time: Mon Jan 24 08:13:30 2022
epoch 3 training:  50%|████▉     | 97397/196351 [13:49:27<14:04:27,  1.95it/s]global step 490100, epoch: 3, batch: 97398, loss: 2.73585, acc: 0.95743, time: Mon Jan 24 08:14:21 2022
epoch 3 training:  50%|████▉     | 97497/196351 [13:50:18<14:11:43,  1.93it/s]global step 490200, epoch: 3, batch: 97498, loss: 0.87673, acc: 0.95746, time: Mon Jan 24 08:15:12 2022
epoch 3 training:  50%|████▉     | 97597/196351 [13:51:09<14:09:08,  1.94it/s]global step 490300, epoch: 3, batch: 97598, loss: 3.64239, acc: 0.95747, time: Mon Jan 24 08:16:03 2022
epoch 3 training:  50%|████▉     | 97697/196351 [13:52:00<14:05:46,  1.94it/s]global step 490400, epoch: 3, batch: 97698, loss: 2.16505, acc: 0.95745, time: Mon Jan 24 08:16:55 2022
epoch 3 training:  50%|████▉     | 97797/196351 [13:52:51<14:06:58,  1.94it/s]global step 490500, epoch: 3, batch: 97798, loss: 2.52193, acc: 0.95746, time: Mon Jan 24 08:17:46 2022
epoch 3 training:  50%|████▉     | 97897/196351 [13:53:42<13:56:04,  1.96it/s]global step 490600, epoch: 3, batch: 97898, loss: 0.41009, acc: 0.95748, time: Mon Jan 24 08:18:37 2022
epoch 3 training:  50%|████▉     | 97997/196351 [13:54:33<13:54:48,  1.96it/s]global step 490700, epoch: 3, batch: 97998, loss: 1.56437, acc: 0.95746, time: Mon Jan 24 08:19:28 2022
epoch 3 training:  50%|████▉     | 98097/196351 [13:55:23<13:49:45,  1.97it/s]global step 490800, epoch: 3, batch: 98098, loss: 0.44871, acc: 0.95745, time: Mon Jan 24 08:20:18 2022
epoch 3 training:  50%|█████     | 98197/196351 [13:56:14<13:43:54,  1.99it/s]global step 490900, epoch: 3, batch: 98198, loss: 0.34200, acc: 0.95746, time: Mon Jan 24 08:21:09 2022
epoch 3 training:  50%|█████     | 98297/196351 [13:57:05<13:41:16,  1.99it/s]global step 491000, epoch: 3, batch: 98298, loss: 1.13970, acc: 0.95746, time: Mon Jan 24 08:22:00 2022
epoch 3 training:  50%|█████     | 98397/196351 [13:57:56<13:56:26,  1.95it/s]global step 491100, epoch: 3, batch: 98398, loss: 1.04933, acc: 0.95746, time: Mon Jan 24 08:22:50 2022
epoch 3 training:  50%|█████     | 98497/196351 [13:58:47<13:48:18,  1.97it/s]global step 491200, epoch: 3, batch: 98498, loss: 0.81393, acc: 0.95748, time: Mon Jan 24 08:23:41 2022
epoch 3 training:  50%|█████     | 98597/196351 [13:59:37<13:56:36,  1.95it/s]global step 491300, epoch: 3, batch: 98598, loss: 0.86754, acc: 0.95747, time: Mon Jan 24 08:24:32 2022
epoch 3 training:  50%|█████     | 98697/196351 [14:00:28<13:50:46,  1.96it/s]global step 491400, epoch: 3, batch: 98698, loss: 0.34879, acc: 0.95745, time: Mon Jan 24 08:25:23 2022
epoch 3 training:  50%|█████     | 98797/196351 [14:01:19<13:55:31,  1.95it/s]global step 491500, epoch: 3, batch: 98798, loss: 1.15763, acc: 0.95746, time: Mon Jan 24 08:26:14 2022
epoch 3 training:  50%|█████     | 98897/196351 [14:02:09<13:54:30,  1.95it/s]global step 491600, epoch: 3, batch: 98898, loss: 1.29754, acc: 0.95746, time: Mon Jan 24 08:27:04 2022
epoch 3 training:  50%|█████     | 98997/196351 [14:03:00<14:00:56,  1.93it/s]global step 491700, epoch: 3, batch: 98998, loss: 0.12706, acc: 0.95747, time: Mon Jan 24 08:27:55 2022
epoch 3 training:  50%|█████     | 99097/196351 [14:03:51<13:38:04,  1.98it/s]global step 491800, epoch: 3, batch: 99098, loss: 0.53246, acc: 0.95749, time: Mon Jan 24 08:28:46 2022
epoch 3 training:  51%|█████     | 99197/196351 [14:04:42<13:48:47,  1.95it/s]global step 491900, epoch: 3, batch: 99198, loss: 0.39288, acc: 0.95747, time: Mon Jan 24 08:29:37 2022
epoch 3 training:  51%|█████     | 99297/196351 [14:05:33<13:49:39,  1.95it/s]global step 492000, epoch: 3, batch: 99298, loss: 2.41379, acc: 0.95748, time: Mon Jan 24 08:30:28 2022
epoch 3 training:  51%|█████     | 99397/196351 [14:06:24<13:44:11,  1.96it/s]global step 492100, epoch: 3, batch: 99398, loss: 0.42568, acc: 0.95749, time: Mon Jan 24 08:31:19 2022
epoch 3 training:  51%|█████     | 99497/196351 [14:07:15<13:47:55,  1.95it/s]global step 492200, epoch: 3, batch: 99498, loss: 0.20128, acc: 0.95748, time: Mon Jan 24 08:32:10 2022
epoch 3 training:  51%|█████     | 99597/196351 [14:08:06<13:40:34,  1.97it/s]global step 492300, epoch: 3, batch: 99598, loss: 2.65954, acc: 0.95748, time: Mon Jan 24 08:33:01 2022
epoch 3 training:  51%|█████     | 99697/196351 [14:08:57<13:43:48,  1.96it/s]global step 492400, epoch: 3, batch: 99698, loss: 1.94834, acc: 0.95748, time: Mon Jan 24 08:33:52 2022
epoch 3 training:  51%|█████     | 99797/196351 [14:09:48<13:40:52,  1.96it/s]global step 492500, epoch: 3, batch: 99798, loss: 0.60410, acc: 0.95749, time: Mon Jan 24 08:34:43 2022
epoch 3 training:  51%|█████     | 99897/196351 [14:10:39<13:38:56,  1.96it/s]global step 492600, epoch: 3, batch: 99898, loss: 1.60956, acc: 0.95747, time: Mon Jan 24 08:35:34 2022
epoch 3 training:  51%|█████     | 99997/196351 [14:11:30<13:37:16,  1.96it/s]global step 492700, epoch: 3, batch: 99998, loss: 0.44771, acc: 0.95749, time: Mon Jan 24 08:36:25 2022
epoch 3 training:  51%|█████     | 100097/196351 [14:12:21<13:30:03,  1.98it/s]global step 492800, epoch: 3, batch: 100098, loss: 3.40602, acc: 0.95747, time: Mon Jan 24 08:37:16 2022
epoch 3 training:  51%|█████     | 100197/196351 [14:13:12<13:34:27,  1.97it/s]global step 492900, epoch: 3, batch: 100198, loss: 1.65414, acc: 0.95748, time: Mon Jan 24 08:38:07 2022
epoch 3 training:  51%|█████     | 100297/196351 [14:14:03<13:35:26,  1.96it/s]global step 493000, epoch: 3, batch: 100298, loss: 1.76439, acc: 0.95749, time: Mon Jan 24 08:38:58 2022
epoch 3 training:  51%|█████     | 100397/196351 [14:14:54<13:32:38,  1.97it/s]global step 493100, epoch: 3, batch: 100398, loss: 3.54602, acc: 0.95747, time: Mon Jan 24 08:39:49 2022
epoch 3 training:  51%|█████     | 100497/196351 [14:15:45<13:27:04,  1.98it/s]global step 493200, epoch: 3, batch: 100498, loss: 0.27749, acc: 0.95747, time: Mon Jan 24 08:40:40 2022
epoch 3 training:  51%|█████     | 100597/196351 [14:16:36<13:26:28,  1.98it/s]global step 493300, epoch: 3, batch: 100598, loss: 0.76056, acc: 0.95748, time: Mon Jan 24 08:41:31 2022
epoch 3 training:  51%|█████▏    | 100697/196351 [14:17:27<13:29:18,  1.97it/s]global step 493400, epoch: 3, batch: 100698, loss: 0.68110, acc: 0.95746, time: Mon Jan 24 08:42:22 2022
epoch 3 training:  51%|█████▏    | 100797/196351 [14:18:18<13:22:05,  1.99it/s]global step 493500, epoch: 3, batch: 100798, loss: 1.21130, acc: 0.95748, time: Mon Jan 24 08:43:13 2022
epoch 3 training:  51%|█████▏    | 100897/196351 [14:19:09<13:25:13,  1.98it/s]global step 493600, epoch: 3, batch: 100898, loss: 0.13937, acc: 0.95747, time: Mon Jan 24 08:44:04 2022
epoch 3 training:  51%|█████▏    | 100997/196351 [14:20:00<13:36:04,  1.95it/s]global step 493700, epoch: 3, batch: 100998, loss: 0.95198, acc: 0.95749, time: Mon Jan 24 08:44:55 2022
epoch 3 training:  51%|█████▏    | 101097/196351 [14:20:51<13:42:05,  1.93it/s]global step 493800, epoch: 3, batch: 101098, loss: 0.89051, acc: 0.95749, time: Mon Jan 24 08:45:46 2022
epoch 3 training:  52%|█████▏    | 101197/196351 [14:21:42<13:18:51,  1.99it/s]global step 493900, epoch: 3, batch: 101198, loss: 2.35322, acc: 0.95749, time: Mon Jan 24 08:46:37 2022
epoch 3 training:  52%|█████▏    | 101297/196351 [14:22:34<13:47:55,  1.91it/s]global step 494000, epoch: 3, batch: 101298, loss: 1.56431, acc: 0.95750, time: Mon Jan 24 08:47:29 2022
epoch 3 training:  52%|█████▏    | 101397/196351 [14:23:25<13:27:32,  1.96it/s]global step 494100, epoch: 3, batch: 101398, loss: 1.66260, acc: 0.95748, time: Mon Jan 24 08:48:20 2022
epoch 3 training:  52%|█████▏    | 101497/196351 [14:24:16<13:25:01,  1.96it/s]global step 494200, epoch: 3, batch: 101498, loss: 0.71122, acc: 0.95750, time: Mon Jan 24 08:49:11 2022
epoch 3 training:  52%|█████▏    | 101597/196351 [14:25:08<13:29:09,  1.95it/s]global step 494300, epoch: 3, batch: 101598, loss: 1.07845, acc: 0.95750, time: Mon Jan 24 08:50:02 2022
epoch 3 training:  52%|█████▏    | 101697/196351 [14:25:58<13:10:11,  2.00it/s]global step 494400, epoch: 3, batch: 101698, loss: 1.14988, acc: 0.95750, time: Mon Jan 24 08:50:53 2022
epoch 3 training:  52%|█████▏    | 101797/196351 [14:26:49<13:11:13,  1.99it/s]global step 494500, epoch: 3, batch: 101798, loss: 0.63911, acc: 0.95751, time: Mon Jan 24 08:51:44 2022
epoch 3 training:  52%|█████▏    | 101897/196351 [14:27:39<13:32:13,  1.94it/s]global step 494600, epoch: 3, batch: 101898, loss: 0.34643, acc: 0.95750, time: Mon Jan 24 08:52:34 2022
epoch 3 training:  52%|█████▏    | 101997/196351 [14:28:30<13:27:03,  1.95it/s]global step 494700, epoch: 3, batch: 101998, loss: 0.43636, acc: 0.95749, time: Mon Jan 24 08:53:25 2022
epoch 3 training:  52%|█████▏    | 102097/196351 [14:29:21<13:18:59,  1.97it/s]global step 494800, epoch: 3, batch: 102098, loss: 1.26163, acc: 0.95749, time: Mon Jan 24 08:54:16 2022
epoch 3 training:  52%|█████▏    | 102197/196351 [14:30:12<13:13:58,  1.98it/s]global step 494900, epoch: 3, batch: 102198, loss: 0.87597, acc: 0.95748, time: Mon Jan 24 08:55:07 2022
epoch 3 training:  52%|█████▏    | 102297/196351 [14:31:03<13:01:57,  2.00it/s]global step 495000, epoch: 3, batch: 102298, loss: 0.53732, acc: 0.95747, time: Mon Jan 24 08:55:58 2022
epoch 3 training:  52%|█████▏    | 102397/196351 [14:31:54<13:20:38,  1.96it/s]global step 495100, epoch: 3, batch: 102398, loss: 2.93795, acc: 0.95747, time: Mon Jan 24 08:56:49 2022
epoch 3 training:  52%|█████▏    | 102497/196351 [14:32:45<13:06:04,  1.99it/s]global step 495200, epoch: 3, batch: 102498, loss: 0.43986, acc: 0.95748, time: Mon Jan 24 08:57:40 2022
epoch 3 training:  52%|█████▏    | 102597/196351 [14:33:36<13:09:44,  1.98it/s]global step 495300, epoch: 3, batch: 102598, loss: 1.29071, acc: 0.95747, time: Mon Jan 24 08:58:30 2022
epoch 3 training:  52%|█████▏    | 102697/196351 [14:34:27<13:26:07,  1.94it/s]global step 495400, epoch: 3, batch: 102698, loss: 3.84626, acc: 0.95746, time: Mon Jan 24 08:59:22 2022
epoch 3 training:  52%|█████▏    | 102797/196351 [14:35:18<13:24:36,  1.94it/s]global step 495500, epoch: 3, batch: 102798, loss: 2.16610, acc: 0.95742, time: Mon Jan 24 09:00:13 2022
epoch 3 training:  52%|█████▏    | 102897/196351 [14:36:09<13:08:04,  1.98it/s]global step 495600, epoch: 3, batch: 102898, loss: 2.20429, acc: 0.95738, time: Mon Jan 24 09:01:04 2022
epoch 3 training:  52%|█████▏    | 102997/196351 [14:37:00<13:03:36,  1.99it/s]global step 495700, epoch: 3, batch: 102998, loss: 1.10232, acc: 0.95738, time: Mon Jan 24 09:01:55 2022
epoch 3 training:  53%|█████▎    | 103097/196351 [14:37:51<13:08:54,  1.97it/s]global step 495800, epoch: 3, batch: 103098, loss: 0.67970, acc: 0.95737, time: Mon Jan 24 09:02:46 2022
epoch 3 training:  53%|█████▎    | 103197/196351 [14:38:42<13:03:01,  1.98it/s]global step 495900, epoch: 3, batch: 103198, loss: 0.49442, acc: 0.95738, time: Mon Jan 24 09:03:37 2022
epoch 3 training:  53%|█████▎    | 103297/196351 [14:39:33<13:13:56,  1.95it/s]global step 496000, epoch: 3, batch: 103298, loss: 2.35519, acc: 0.95739, time: Mon Jan 24 09:04:28 2022
epoch 3 training:  53%|█████▎    | 103397/196351 [14:40:24<13:00:34,  1.98it/s]global step 496100, epoch: 3, batch: 103398, loss: 0.35792, acc: 0.95740, time: Mon Jan 24 09:05:19 2022
epoch 3 training:  53%|█████▎    | 103497/196351 [14:41:15<13:13:47,  1.95it/s]global step 496200, epoch: 3, batch: 103498, loss: 0.43739, acc: 0.95740, time: Mon Jan 24 09:06:10 2022
epoch 3 training:  53%|█████▎    | 103597/196351 [14:42:06<13:03:13,  1.97it/s]global step 496300, epoch: 3, batch: 103598, loss: 1.11650, acc: 0.95739, time: Mon Jan 24 09:07:01 2022
epoch 3 training:  53%|█████▎    | 103697/196351 [14:42:57<13:03:03,  1.97it/s]global step 496400, epoch: 3, batch: 103698, loss: 2.29916, acc: 0.95741, time: Mon Jan 24 09:07:51 2022
epoch 3 training:  53%|█████▎    | 103797/196351 [14:43:47<13:11:12,  1.95it/s]global step 496500, epoch: 3, batch: 103798, loss: 0.34998, acc: 0.95742, time: Mon Jan 24 09:08:42 2022
epoch 3 training:  53%|█████▎    | 103897/196351 [14:44:38<13:03:56,  1.97it/s]global step 496600, epoch: 3, batch: 103898, loss: 0.30874, acc: 0.95741, time: Mon Jan 24 09:09:33 2022
epoch 3 training:  53%|█████▎    | 103997/196351 [14:45:29<12:58:10,  1.98it/s]global step 496700, epoch: 3, batch: 103998, loss: 1.29540, acc: 0.95739, time: Mon Jan 24 09:10:24 2022
epoch 3 training:  53%|█████▎    | 104097/196351 [14:46:20<13:13:52,  1.94it/s]global step 496800, epoch: 3, batch: 104098, loss: 0.72898, acc: 0.95738, time: Mon Jan 24 09:11:15 2022
epoch 3 training:  53%|█████▎    | 104197/196351 [14:47:11<12:48:45,  2.00it/s]global step 496900, epoch: 3, batch: 104198, loss: 1.73608, acc: 0.95734, time: Mon Jan 24 09:12:05 2022
epoch 3 training:  53%|█████▎    | 104297/196351 [14:48:01<12:51:09,  1.99it/s]global step 497000, epoch: 3, batch: 104298, loss: 0.33785, acc: 0.95731, time: Mon Jan 24 09:12:56 2022
epoch 3 training:  53%|█████▎    | 104397/196351 [14:48:52<12:46:55,  2.00it/s]global step 497100, epoch: 3, batch: 104398, loss: 2.45413, acc: 0.95732, time: Mon Jan 24 09:13:47 2022
epoch 3 training:  53%|█████▎    | 104497/196351 [14:49:42<13:05:53,  1.95it/s]global step 497200, epoch: 3, batch: 104498, loss: 0.14706, acc: 0.95733, time: Mon Jan 24 09:14:37 2022
epoch 3 training:  53%|█████▎    | 104597/196351 [14:50:33<12:55:32,  1.97it/s]global step 497300, epoch: 3, batch: 104598, loss: 0.45724, acc: 0.95732, time: Mon Jan 24 09:15:28 2022
epoch 3 training:  53%|█████▎    | 104697/196351 [14:51:24<12:53:25,  1.98it/s]global step 497400, epoch: 3, batch: 104698, loss: 0.65940, acc: 0.95733, time: Mon Jan 24 09:16:18 2022
epoch 3 training:  53%|█████▎    | 104797/196351 [14:52:14<13:08:09,  1.94it/s]global step 497500, epoch: 3, batch: 104798, loss: 0.39291, acc: 0.95733, time: Mon Jan 24 09:17:09 2022
epoch 3 training:  53%|█████▎    | 104897/196351 [14:53:05<12:58:24,  1.96it/s]global step 497600, epoch: 3, batch: 104898, loss: 2.48272, acc: 0.95733, time: Mon Jan 24 09:18:00 2022
epoch 3 training:  53%|█████▎    | 104997/196351 [14:53:55<12:48:30,  1.98it/s]global step 497700, epoch: 3, batch: 104998, loss: 1.04939, acc: 0.95734, time: Mon Jan 24 09:18:50 2022
epoch 3 training:  54%|█████▎    | 105097/196351 [14:54:46<12:51:05,  1.97it/s]global step 497800, epoch: 3, batch: 105098, loss: 0.13707, acc: 0.95733, time: Mon Jan 24 09:19:41 2022
epoch 3 training:  54%|█████▎    | 105197/196351 [14:55:37<12:46:06,  1.98it/s]global step 497900, epoch: 3, batch: 105198, loss: 1.33314, acc: 0.95733, time: Mon Jan 24 09:20:32 2022
epoch 3 training:  54%|█████▎    | 105297/196351 [14:56:28<12:58:04,  1.95it/s]global step 498000, epoch: 3, batch: 105298, loss: 2.09388, acc: 0.95731, time: Mon Jan 24 09:21:22 2022
epoch 3 training:  54%|█████▎    | 105397/196351 [14:57:18<12:51:53,  1.96it/s]global step 498100, epoch: 3, batch: 105398, loss: 0.45010, acc: 0.95730, time: Mon Jan 24 09:22:13 2022
epoch 3 training:  54%|█████▎    | 105497/196351 [14:58:09<12:50:20,  1.97it/s]global step 498200, epoch: 3, batch: 105498, loss: 0.28269, acc: 0.95729, time: Mon Jan 24 09:23:04 2022
epoch 3 training:  54%|█████▍    | 105597/196351 [14:58:59<12:50:24,  1.96it/s]global step 498300, epoch: 3, batch: 105598, loss: 0.86668, acc: 0.95726, time: Mon Jan 24 09:23:53 2022
epoch 3 training:  54%|█████▍    | 105697/196351 [14:59:49<12:43:41,  1.98it/s]global step 498400, epoch: 3, batch: 105698, loss: 0.31406, acc: 0.95727, time: Mon Jan 24 09:24:44 2022
epoch 3 training:  54%|█████▍    | 105797/196351 [15:00:40<12:37:49,  1.99it/s]global step 498500, epoch: 3, batch: 105798, loss: 0.64985, acc: 0.95728, time: Mon Jan 24 09:25:35 2022
epoch 3 training:  54%|█████▍    | 105897/196351 [15:01:30<12:49:01,  1.96it/s]global step 498600, epoch: 3, batch: 105898, loss: 0.92458, acc: 0.95727, time: Mon Jan 24 09:26:25 2022
epoch 3 training:  54%|█████▍    | 105997/196351 [15:02:21<12:48:40,  1.96it/s]global step 498700, epoch: 3, batch: 105998, loss: 3.28272, acc: 0.95725, time: Mon Jan 24 09:27:16 2022
epoch 3 training:  54%|█████▍    | 106097/196351 [15:03:12<12:33:25,  2.00it/s]global step 498800, epoch: 3, batch: 106098, loss: 0.69504, acc: 0.95726, time: Mon Jan 24 09:28:07 2022
epoch 3 training:  54%|█████▍    | 106197/196351 [15:04:02<12:49:22,  1.95it/s]global step 498900, epoch: 3, batch: 106198, loss: 2.28688, acc: 0.95726, time: Mon Jan 24 09:28:57 2022
epoch 3 training:  54%|█████▍    | 106297/196351 [15:04:53<12:43:37,  1.97it/s]global step 499000, epoch: 3, batch: 106298, loss: 0.30879, acc: 0.95728, time: Mon Jan 24 09:29:48 2022
epoch 3 training:  54%|█████▍    | 106397/196351 [15:05:44<12:37:27,  1.98it/s]global step 499100, epoch: 3, batch: 106398, loss: 0.56759, acc: 0.95727, time: Mon Jan 24 09:30:39 2022
epoch 3 training:  54%|█████▍    | 106497/196351 [15:06:35<12:42:42,  1.96it/s]global step 499200, epoch: 3, batch: 106498, loss: 0.34566, acc: 0.95725, time: Mon Jan 24 09:31:29 2022
epoch 3 training:  54%|█████▍    | 106597/196351 [15:07:26<12:46:58,  1.95it/s]global step 499300, epoch: 3, batch: 106598, loss: 2.56320, acc: 0.95727, time: Mon Jan 24 09:32:20 2022
epoch 3 training:  54%|█████▍    | 106697/196351 [15:08:16<12:35:47,  1.98it/s]global step 499400, epoch: 3, batch: 106698, loss: 0.83100, acc: 0.95726, time: Mon Jan 24 09:33:11 2022
epoch 3 training:  54%|█████▍    | 106797/196351 [15:09:07<12:32:09,  1.98it/s]global step 499500, epoch: 3, batch: 106798, loss: 1.78215, acc: 0.95721, time: Mon Jan 24 09:34:02 2022
epoch 3 training:  54%|█████▍    | 106897/196351 [15:09:58<13:32:38,  1.83it/s]global step 499600, epoch: 3, batch: 106898, loss: 0.99357, acc: 0.95723, time: Mon Jan 24 09:34:53 2022
epoch 3 training:  54%|█████▍    | 106997/196351 [15:10:49<12:35:28,  1.97it/s]global step 499700, epoch: 3, batch: 106998, loss: 1.38964, acc: 0.95723, time: Mon Jan 24 09:35:44 2022
epoch 3 training:  55%|█████▍    | 107097/196351 [15:11:40<12:47:45,  1.94it/s]global step 499800, epoch: 3, batch: 107098, loss: 0.45010, acc: 0.95724, time: Mon Jan 24 09:36:35 2022
epoch 3 training:  55%|█████▍    | 107197/196351 [15:12:32<12:48:39,  1.93it/s]global step 499900, epoch: 3, batch: 107198, loss: 1.05700, acc: 0.95722, time: Mon Jan 24 09:37:27 2022
epoch 3 training:  55%|█████▍    | 107297/196351 [15:13:23<12:38:45,  1.96it/s]global step 500000, epoch: 3, batch: 107298, loss: 4.38668, acc: 0.95719, time: Mon Jan 24 09:38:17 2022
epoch 3 training:  55%|█████▍    | 107397/196351 [15:14:14<12:42:14,  1.94it/s]global step 500100, epoch: 3, batch: 107398, loss: 2.18416, acc: 0.95718, time: Mon Jan 24 09:39:09 2022
epoch 3 training:  55%|█████▍    | 107497/196351 [15:15:05<12:30:33,  1.97it/s]global step 500200, epoch: 3, batch: 107498, loss: 0.45638, acc: 0.95718, time: Mon Jan 24 09:40:00 2022
epoch 3 training:  55%|█████▍    | 107597/196351 [15:15:58<12:43:09,  1.94it/s]global step 500300, epoch: 3, batch: 107598, loss: 1.51088, acc: 0.95717, time: Mon Jan 24 09:40:53 2022
epoch 3 training:  55%|█████▍    | 107697/196351 [15:16:49<12:20:34,  2.00it/s]global step 500400, epoch: 3, batch: 107698, loss: 0.47719, acc: 0.95714, time: Mon Jan 24 09:41:44 2022
epoch 3 training:  55%|█████▍    | 107797/196351 [15:17:40<12:37:08,  1.95it/s]global step 500500, epoch: 3, batch: 107798, loss: 0.38707, acc: 0.95717, time: Mon Jan 24 09:42:35 2022
epoch 3 training:  55%|█████▍    | 107897/196351 [15:18:31<12:28:57,  1.97it/s]global step 500600, epoch: 3, batch: 107898, loss: 2.46372, acc: 0.95718, time: Mon Jan 24 09:43:26 2022
epoch 3 training:  55%|█████▌    | 107997/196351 [15:19:23<12:31:21,  1.96it/s]global step 500700, epoch: 3, batch: 107998, loss: 2.12290, acc: 0.95719, time: Mon Jan 24 09:44:18 2022
epoch 3 training:  55%|█████▌    | 108097/196351 [15:20:15<12:47:43,  1.92it/s]global step 500800, epoch: 3, batch: 108098, loss: 0.63119, acc: 0.95720, time: Mon Jan 24 09:45:09 2022
epoch 3 training:  55%|█████▌    | 108197/196351 [15:21:06<12:38:30,  1.94it/s]global step 500900, epoch: 3, batch: 108198, loss: 2.26391, acc: 0.95717, time: Mon Jan 24 09:46:01 2022
epoch 3 training:  55%|█████▌    | 108297/196351 [15:21:57<13:20:10,  1.83it/s]global step 501000, epoch: 3, batch: 108298, loss: 2.15913, acc: 0.95716, time: Mon Jan 24 09:46:52 2022
epoch 3 training:  55%|█████▌    | 108397/196351 [15:22:49<12:43:09,  1.92it/s]global step 501100, epoch: 3, batch: 108398, loss: 0.45824, acc: 0.95714, time: Mon Jan 24 09:47:44 2022
epoch 3 training:  55%|█████▌    | 108497/196351 [15:23:40<12:46:45,  1.91it/s]global step 501200, epoch: 3, batch: 108498, loss: 0.81922, acc: 0.95713, time: Mon Jan 24 09:48:35 2022
epoch 3 training:  55%|█████▌    | 108597/196351 [15:24:32<12:25:51,  1.96it/s]global step 501300, epoch: 3, batch: 108598, loss: 0.80638, acc: 0.95713, time: Mon Jan 24 09:49:27 2022
epoch 3 training:  55%|█████▌    | 108697/196351 [15:25:24<12:31:49,  1.94it/s]global step 501400, epoch: 3, batch: 108698, loss: 0.27035, acc: 0.95713, time: Mon Jan 24 09:50:19 2022
epoch 3 training:  55%|█████▌    | 108797/196351 [15:26:16<13:06:01,  1.86it/s]global step 501500, epoch: 3, batch: 108798, loss: 2.02322, acc: 0.95713, time: Mon Jan 24 09:51:11 2022
epoch 3 training:  55%|█████▌    | 108897/196351 [15:27:08<12:54:22,  1.88it/s]global step 501600, epoch: 3, batch: 108898, loss: 1.07387, acc: 0.95713, time: Mon Jan 24 09:52:03 2022
epoch 3 training:  56%|█████▌    | 108997/196351 [15:28:00<12:24:38,  1.96it/s]global step 501700, epoch: 3, batch: 108998, loss: 0.40438, acc: 0.95713, time: Mon Jan 24 09:52:55 2022
epoch 3 training:  56%|█████▌    | 109097/196351 [15:28:53<12:22:26,  1.96it/s]global step 501800, epoch: 3, batch: 109098, loss: 2.13031, acc: 0.95712, time: Mon Jan 24 09:53:47 2022
epoch 3 training:  56%|█████▌    | 109197/196351 [15:29:44<12:26:20,  1.95it/s]global step 501900, epoch: 3, batch: 109198, loss: 1.01490, acc: 0.95712, time: Mon Jan 24 09:54:39 2022
epoch 3 training:  56%|█████▌    | 109297/196351 [15:30:37<12:35:43,  1.92it/s]global step 502000, epoch: 3, batch: 109298, loss: 0.44884, acc: 0.95713, time: Mon Jan 24 09:55:32 2022
epoch 3 training:  56%|█████▌    | 109397/196351 [15:31:29<12:35:35,  1.92it/s]global step 502100, epoch: 3, batch: 109398, loss: 0.44624, acc: 0.95715, time: Mon Jan 24 09:56:24 2022
epoch 3 training:  56%|█████▌    | 109497/196351 [15:32:20<12:17:38,  1.96it/s]global step 502200, epoch: 3, batch: 109498, loss: 0.47729, acc: 0.95715, time: Mon Jan 24 09:57:15 2022
epoch 3 training:  56%|█████▌    | 109597/196351 [15:33:12<12:33:49,  1.92it/s]global step 502300, epoch: 3, batch: 109598, loss: 0.41965, acc: 0.95716, time: Mon Jan 24 09:58:07 2022
epoch 3 training:  56%|█████▌    | 109697/196351 [15:34:03<12:27:49,  1.93it/s]global step 502400, epoch: 3, batch: 109698, loss: 0.30091, acc: 0.95716, time: Mon Jan 24 09:58:58 2022
epoch 3 training:  56%|█████▌    | 109797/196351 [15:34:55<12:29:22,  1.93it/s]global step 502500, epoch: 3, batch: 109798, loss: 0.49148, acc: 0.95718, time: Mon Jan 24 09:59:50 2022
epoch 3 training:  56%|█████▌    | 109897/196351 [15:35:47<12:28:13,  1.93it/s]global step 502600, epoch: 3, batch: 109898, loss: 0.54203, acc: 0.95719, time: Mon Jan 24 10:00:41 2022
epoch 3 training:  56%|█████▌    | 109997/196351 [15:36:38<12:21:39,  1.94it/s]global step 502700, epoch: 3, batch: 109998, loss: 1.30158, acc: 0.95720, time: Mon Jan 24 10:01:33 2022
epoch 3 training:  56%|█████▌    | 110060/196351 [15:37:11<12:14:47,  1.96it/s]
Traceback (most recent call last):
  File "C:\Users\woait\AppData\Roaming\JetBrains\IntelliJIdea2021.3\plugins\python\helpers\pydev\pydevd.py", line 1483, in _exec
    pydev_imports.execfile(file, globals, locals)  # execute the script
  File "C:\Users\woait\AppData\Roaming\JetBrains\IntelliJIdea2021.3\plugins\python\helpers\pydev\_pydev_imps\_pydev_execfile.py", line 18, in execfile
    exec(compile(contents+"\n", file, 'exec'), glob, loc)
  File "C:/JYN/workspace/DeBERTa_Decoding-enhanced_BERT_with_Disentangled_Attention/tools/test_torch.py", line 168, in <module>
    loss.backward()
  File "C:\JYN\sd\anaconda\anaconda\envs\paddle-py37\lib\site-packages\torch\_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "C:\JYN\sd\anaconda\anaconda\envs\paddle-py37\lib\site-packages\torch\autograd\__init__.py", line 156, in backward
    allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag
  File "C:\JYN\sd\anaconda\anaconda\envs\paddle-py37\lib\site-packages\torch\autograd\function.py", line 199, in apply
    return user_fn(self, *args)
  File "C:\JYN\sd\anaconda\anaconda\envs\paddle-py37\lib\site-packages\transformers\models\deberta_v2\modeling_deberta_v2.py", line 181, in backward
    return grad_output.masked_fill(mask, 0) * ctx.scale, None
RuntimeError: The size of tensor a (0) must match the size of tensor b (1536) at non-singleton dimension 1

Process finished with exit code -1073741819 (0xC0000005)
